{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load config and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import spacy, pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "# Custom impport\n",
    "from common.common_classes import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'prepare-word-embedding-nlp.ipynb', 'tokenization.ipynb', 'test-batching-padding.ipynb', 'train.tsv', 'test-batching-padding-ok.ipynb', 'sampleSubmission.csv', 'save_data', '.ipynb_checkpoints', '__init__.py', 'README.md', '.gitignore', '.git', 'common', 'simple-GRU-implement.ipynb']\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "save_data_path = path + 'save_data/'\n",
    "large_save_data_path = '/notebooks/large-storage/'\n",
    "saved_models_path = '/notebooks/large-storage/saved-models/'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(save_data_path + 'pre-processed-data.pkl', 'rb'))\n",
    "loaded_kaggle_test = pickle.load(open(save_data_path + 'pre-processed-kaggle-test.pkl', 'rb'))\n",
    "loaded_vocab = pickle.load(open(save_data_path + 'genereated-vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, a, series, xxeos]</td>\n",
       "      <td>[2, 10, 341, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, a, xxeos]</td>\n",
       "      <td>[2, 10, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, series, xxeos]</td>\n",
       "      <td>[2, 341, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  Phrase_length  \\\n",
       "0          1            188   \n",
       "1          2             77   \n",
       "2          2              8   \n",
       "3          2              1   \n",
       "4          2              6   \n",
       "\n",
       "                                    Tokenized_phrase  \\\n",
       "0  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "1  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "2                          [xxbos, a, series, xxeos]   \n",
       "3                                  [xxbos, a, xxeos]   \n",
       "4                             [xxbos, series, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "1  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "2                                    [2, 10, 341, 3]  \n",
       "3                                         [2, 10, 3]  \n",
       "4                                        [2, 341, 3]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "MAX_LABEL = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoding and prepraing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(large_save_data_path + 'process-spacy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[BOS].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab.get_vector('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890280, 308)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "PHRASE_ID = data.Field(use_vocab = False)\n",
    "TEXT = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)\n",
    "LABEL = data.LabelField(use_vocab = False, dtype=torch.long)\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "ID_TEST = data.Field(use_vocab = False)\n",
    "PHRASE_ID_TEST = data.Field(use_vocab = False)\n",
    "TEXT_TEST = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "fields = [('id', PHRASE_ID), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "fields_test = [('id', ID_TEST), ('phrase_id', PHRASE_ID_TEST), ('text', TEXT_TEST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_kaggle_test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f1f7c720208>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c720240>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c720278>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c7202b0>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c7202e8>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c720320>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c720358>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c720390>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c7203c8>,\n",
       " <torchtext.data.example.Example at 0x7f1f7c720400>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Train dataset\n",
    "examples = []\n",
    "length = len(loaded_data['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_data['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_data['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_data['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples.append(data.Example.fromlist([ [loaded_data['PhraseId'][i]], embedded, loaded_data['Sentiment'][i]], fields))\n",
    "    \n",
    "examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f1f251e6d68>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6dd8>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6e48>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6e80>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6eb8>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6ef0>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6f28>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6f60>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6f98>,\n",
       " <torchtext.data.example.Example at 0x7f1f251e6fd0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Test dataset\n",
    "examples_test = []\n",
    "length = len(loaded_kaggle_test['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_kaggle_test['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples_test.append(data.Example.fromlist([ [i], [loaded_kaggle_test['PhraseId'][i]], embedded ], fields_test))\n",
    "    \n",
    "examples_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "        -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "        -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "        -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "        -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "         3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "         3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "        -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "         3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "        -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "        -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "         3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "        -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "        -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "         1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "        -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "        -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "         6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "        -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "         4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "        -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "        -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "         2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "         2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "         1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "         1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "        -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "        -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "        -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "        -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "        -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "         4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "        -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "        -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "         1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "        -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "        -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "         4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "        -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "         1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "         1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "         7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "        -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "         7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "        -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "        -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "        -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "        -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "         8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "        -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "        -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "         3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "        -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "        -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "         3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "        -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "         4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "        -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "        -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "         8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.7793e-01, -6.2470e-02,  1.9596e-01, -4.3340e-02, -1.4570e-01,\n",
       "         6.7455e-02, -7.9048e-01, -4.7327e-01, -1.2694e-02,  9.4032e-01,\n",
       "         9.7596e-01,  3.6356e-01, -4.2334e-01, -4.7300e-01,  4.3313e-02,\n",
       "        -1.7159e-02,  7.7286e-02,  5.1450e-01, -1.3013e-01, -3.8677e-01,\n",
       "         8.5337e-02, -1.1537e-01, -2.9981e-01,  1.0327e-01,  7.4443e-02,\n",
       "         3.8978e-02, -1.0280e-01, -2.7481e-01, -2.9409e-01, -5.2790e-01,\n",
       "        -5.4200e-01, -4.1610e-01, -7.6345e-01, -2.1057e-01, -6.0277e-01,\n",
       "        -3.7840e-01, -5.1185e-01,  5.4210e-02, -8.2146e-02,  5.3257e-01,\n",
       "         3.3352e-01,  1.6082e-01, -1.6897e-02,  3.3638e-01, -1.3660e-01,\n",
       "         4.2157e-01, -2.5176e-01, -3.2271e-01, -3.9742e-01, -3.2712e-01,\n",
       "         3.5235e-01,  5.4523e-01, -5.8145e-01, -2.5093e-01, -4.2526e-01,\n",
       "         4.7148e-04,  5.4579e-01, -4.9980e-01,  2.3473e-02, -1.0801e-01,\n",
       "        -2.5952e-01, -7.5854e-03, -2.0041e-01, -3.7743e-01, -3.0881e-01,\n",
       "        -7.4719e-02,  3.9462e-03, -3.6002e-01, -7.1041e-02,  2.8597e-01,\n",
       "         1.3777e-01,  1.6269e-01, -1.0977e-01, -3.6319e-01,  8.5886e-02,\n",
       "         2.0098e-01,  1.9972e-01,  3.2699e-01, -1.1095e-01, -2.4142e-01,\n",
       "         1.1838e-01, -2.1962e-01, -6.1894e-01, -1.7774e-01, -1.5444e-01,\n",
       "         7.2033e-02,  7.5290e-01,  1.7973e-01,  3.2366e-01,  6.2210e-01,\n",
       "        -1.1795e-01, -2.4438e-01,  2.0690e-01,  4.0958e-01, -3.5867e-02,\n",
       "        -2.7432e-01,  3.3013e-01,  1.8814e-01,  2.6837e-01, -1.8496e-01,\n",
       "         6.3260e-02, -3.3311e-03, -5.4875e-02,  2.3964e-01, -1.4294e-01,\n",
       "        -1.9737e+00, -1.7595e-01, -7.4150e-02,  3.7287e-01,  1.6554e-01,\n",
       "         2.3770e-01, -1.4556e-01,  4.5459e-01, -8.1550e-02, -3.3170e-01,\n",
       "        -8.1371e-01, -1.7414e-01,  1.3691e-01, -6.0061e-01,  4.0306e-01,\n",
       "        -4.3174e-01, -1.7259e-01,  5.2948e-02, -2.4818e-01,  1.5827e-01,\n",
       "        -1.1220e-01,  3.2860e-02,  5.5025e-01,  2.0649e-01,  1.0403e-01,\n",
       "         4.3973e-01, -1.1043e-01, -2.3429e-01,  4.2473e-01, -2.0461e-01,\n",
       "        -2.3912e-01,  2.7573e-01,  6.4947e-01, -3.4708e-01, -1.4163e-01,\n",
       "        -1.7743e-01,  1.7144e-01, -1.7009e-01, -1.2667e-01, -5.6574e-02,\n",
       "         6.9985e-01,  9.3176e-02, -1.1523e-01,  7.9450e-02,  5.3647e-02,\n",
       "        -4.2493e-01,  7.1772e-01,  2.3705e-01,  2.1213e-01, -1.9776e-02,\n",
       "        -1.9379e-02,  2.9703e-01, -2.7536e-01,  7.3583e-02, -5.1954e-01,\n",
       "        -1.5928e-01, -3.5374e-01,  4.5916e-01,  2.0435e-01,  9.7278e-04,\n",
       "         4.6904e-01, -6.2419e-01,  3.2797e-01, -1.5744e-01, -8.8403e-02,\n",
       "         3.5625e-02, -1.2245e-01,  3.6698e-01, -8.8524e-02, -5.5171e-02,\n",
       "         6.0885e-01,  5.3027e-01, -3.3558e-01,  1.5178e-01, -1.6619e-01,\n",
       "         5.0028e-01, -1.4728e-02, -1.4555e-01,  5.6033e-01,  2.4656e-01,\n",
       "        -3.6279e-01, -7.8195e-04,  9.5365e-02,  5.8955e-02, -4.8528e-01,\n",
       "         5.4453e-01, -1.1293e-02, -1.5423e-01,  3.7746e-01, -4.1018e-01,\n",
       "        -7.5968e-02,  3.2843e-01, -1.0195e-01, -6.5580e-01, -5.7961e-01,\n",
       "        -4.3063e-01,  5.0521e-01,  1.4637e-02,  1.8917e-01,  5.0427e-02,\n",
       "        -1.2479e-01, -1.7724e-01, -1.0494e-01,  3.2128e-01,  5.9077e-02,\n",
       "         2.3804e-01, -6.3107e-02,  3.3216e-02, -2.0066e-01, -2.7485e-01,\n",
       "        -2.8084e-01,  4.9871e-01,  2.0036e-01,  2.9303e-01, -3.4742e-01,\n",
       "         4.9861e-01,  2.3172e-01,  1.4505e-01, -7.2846e-02, -8.4557e-02,\n",
       "        -1.5358e-01,  5.1501e-01, -1.7232e-01,  2.7167e-01, -2.3813e-01,\n",
       "         1.3970e-01,  2.5681e-01,  4.1433e-01,  3.3110e-01, -3.7062e-02,\n",
       "        -6.8761e-02,  6.5520e-02, -3.1337e-01, -3.6793e-02, -1.3847e-01,\n",
       "         2.2251e-01,  3.2041e-01,  3.1190e-01,  2.7103e-01, -4.0910e-01,\n",
       "         1.0762e-01,  1.6541e-01,  3.7305e-01,  2.1920e-01, -4.5466e-01,\n",
       "        -2.6570e-01, -4.3871e-01,  2.7044e-01,  2.0953e-01,  3.1834e-01,\n",
       "        -4.3875e-01,  2.4935e-01,  4.6443e-02, -3.2949e-01,  2.1793e-01,\n",
       "         7.3772e-01,  3.9756e-02, -5.0169e-02, -1.8881e-01,  7.7245e-02,\n",
       "         1.4199e-01,  3.9659e-02,  1.2333e-01, -9.6613e-02, -2.2687e-01,\n",
       "         4.8893e-01, -1.1373e-01,  5.1246e-02,  7.5477e-02, -1.2750e-01,\n",
       "        -1.0697e-01, -4.8071e-01, -2.6223e-01,  4.5893e-01,  3.6267e-01,\n",
       "         2.9100e-01,  1.7118e-02, -2.0185e-01, -7.4050e-02, -2.3640e-01,\n",
       "         2.8046e-01,  1.8657e-01,  8.7665e-02,  3.1472e-01, -7.0119e-02,\n",
       "        -1.8172e-01, -3.9074e-02,  2.8063e-02, -4.9746e-02,  1.0355e-01,\n",
       "        -1.5258e-01,  3.4939e-01, -1.6108e-01, -6.0705e-02, -9.0380e-03,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-0.2712   ,  0.14702  , -0.19126  , -0.24424  ,  0.16476  ,\n",
       "        -0.32537  ,  0.36027  ,  0.29804  , -0.49571  ,  1.2244   ,\n",
       "         0.097696 ,  0.30406  , -0.53059  , -0.25869  ,  0.16519  ,\n",
       "        -0.41392  , -0.2314   ,  0.86309  ,  0.35309  ,  0.41004  ,\n",
       "        -0.35114  ,  0.21699  ,  0.30512  , -0.30977  ,  0.065359 ,\n",
       "         0.13193  ,  0.59324  ,  0.089948 , -0.14227  , -0.31061  ,\n",
       "        -0.08432  ,  0.21609  , -0.57332  , -0.076418 , -0.0041399,\n",
       "        -0.48229  , -0.11635  , -0.0089557, -0.0064539, -0.031811 ,\n",
       "         0.16104  , -0.11247  , -0.042646 , -0.026045 ,  0.097288 ,\n",
       "        -0.030434 , -0.45127  , -0.028487 , -0.14567  , -0.10633  ,\n",
       "        -0.17795  , -0.097141 , -0.15773  , -0.42128  ,  0.25669  ,\n",
       "         0.53326  , -0.027647 ,  0.24529  , -0.46835  ,  0.090866 ,\n",
       "         0.12265  , -0.63001  , -0.25682  ,  0.0060041, -0.3454   ,\n",
       "         0.0072695,  0.17178  , -0.39181  , -0.060693 , -0.031147 ,\n",
       "         0.20888  ,  0.30827  ,  0.035736 , -0.23439  ,  0.28267  ,\n",
       "         0.08706  , -0.057023 , -0.30681  , -0.38813  , -0.039169 ,\n",
       "        -0.059306 , -0.36394  ,  0.26486  ,  0.29665  ,  0.12684  ,\n",
       "        -0.059682 ,  0.18076  ,  1.0117   , -0.020038 , -0.16224  ,\n",
       "        -0.069924 ,  0.29389  ,  0.023328 ,  0.67408  ,  0.13449  ,\n",
       "         0.021691 ,  0.21811  , -0.40404  , -0.13521  ,  0.11014  ,\n",
       "         0.19624  ,  0.05419  ,  0.16292  , -0.17251  ,  0.27033  ,\n",
       "        -1.616    ,  0.60791  ,  0.0033667, -0.29159  ,  0.14195  ,\n",
       "        -0.075748 , -0.19854  ,  0.27378  ,  0.087632 , -0.023092 ,\n",
       "        -0.24036  , -0.20799  , -0.062753 ,  0.040184 ,  0.032635 ,\n",
       "        -0.2035   ,  0.15727  , -0.67543  ,  0.67401  , -0.005282 ,\n",
       "         0.067059 ,  0.20846  , -0.65743  ,  0.17864  , -0.27987  ,\n",
       "        -0.022082 , -0.20144  ,  0.13538  , -0.12531  , -0.39399  ,\n",
       "         0.3964   , -0.11625  , -0.34535  ,  0.25273  ,  0.24191  ,\n",
       "        -1.6655   ,  0.61688  , -0.0035128,  0.37053  , -0.32058  ,\n",
       "        -0.48533  ,  0.1568   , -0.086594 ,  0.13272  , -0.050194 ,\n",
       "         0.12197  ,  0.0621   , -0.052821 ,  0.13192  ,  0.26901  ,\n",
       "         0.26882  , -0.47454  , -0.28357  ,  0.16668  , -0.51742  ,\n",
       "        -0.2906   ,  0.25908  ,  0.71942  ,  0.50009  , -0.092692 ,\n",
       "        -0.36577  , -0.034634 ,  0.056318 , -0.21117  ,  0.37486  ,\n",
       "         0.032773 , -0.14359  ,  0.6884   , -0.026806 ,  0.42897  ,\n",
       "         0.053205 , -0.42011  , -0.25344  , -0.21894  ,  0.14449  ,\n",
       "         0.045221 , -0.19444  ,  0.40097  ,  0.42644  , -0.39018  ,\n",
       "        -0.12735  ,  0.11559  ,  0.019183 , -0.027092 , -0.44417  ,\n",
       "         0.045983 ,  0.30359  ,  0.22169  ,  0.060414 , -0.0059974,\n",
       "        -0.66994  , -0.058462 ,  0.16644  , -0.1518   ,  0.0072898,\n",
       "        -0.21948  ,  0.45436  ,  0.05823  , -0.31103  , -0.23067  ,\n",
       "         0.0078952, -0.30799  ,  0.10996  ,  0.28958  , -0.033313 ,\n",
       "        -0.46981  ,  0.17656  , -0.077724 , -0.44687  , -0.13734  ,\n",
       "         0.44273  ,  0.43806  , -0.32096  ,  0.65241  , -0.29144  ,\n",
       "         0.35948  , -0.39277  ,  0.078458 ,  0.6577   ,  0.45683  ,\n",
       "         0.093801 , -0.25023  ,  0.046448 ,  0.04619  , -0.14565  ,\n",
       "         0.17054  ,  0.072038 , -0.86523  ,  0.20702  ,  0.45185  ,\n",
       "         0.20958  ,  0.043511 , -0.17528  , -0.15821  ,  0.011212 ,\n",
       "         0.41577  , -0.31444  , -0.17748  ,  0.089159 , -0.34662  ,\n",
       "        -0.097137 , -0.28784  , -0.38902  ,  0.21508  ,  0.31001  ,\n",
       "         0.030091 ,  0.30658  ,  0.23107  ,  0.048666 ,  0.13822  ,\n",
       "         0.4905   ,  0.66801  , -0.023158 ,  0.38607  ,  0.1293   ,\n",
       "        -0.027527 , -0.31134  ,  0.31469  ,  0.26038  ,  0.3263   ,\n",
       "         0.087236 , -0.014318 , -0.080271 , -0.14507  ,  0.34931  ,\n",
       "        -0.69845  , -0.25372  , -0.28831  , -0.038035 , -0.59529  ,\n",
       "         0.17554  ,  0.78562  , -0.066821 , -0.062887 ,  0.28903  ,\n",
       "         0.82979  ,  0.33097  ,  0.17223  , -0.27315  , -0.5014   ,\n",
       "        -0.29307  ,  0.43277  ,  0.59082  ,  0.059657 ,  0.19892  ,\n",
       "        -0.39105  , -0.020178 , -0.23956  ,  0.26956  ,  0.15734  ,\n",
       "         0.049013 ,  0.2598   , -0.15101  ,  0.069548 ,  0.058311 ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ], dtype=float32),\n",
       " array([-1.6890e-02,  1.7402e-01, -3.0247e-01, -3.0063e-01,  2.1415e-01,\n",
       "         6.3863e-02,  1.0107e-01, -2.4155e-01, -9.5228e-02,  2.9253e+00,\n",
       "        -5.6759e-03,  1.1752e-01,  1.6121e-01,  2.0813e-02, -8.3593e-02,\n",
       "        -1.4048e-01, -4.0069e-02,  7.5133e-01, -5.6699e-02, -9.6198e-02,\n",
       "        -7.2134e-02, -3.1287e-02,  1.8146e-01, -2.4846e-01, -6.5068e-02,\n",
       "         6.6374e-02, -1.2173e-01, -1.3479e-01,  2.6163e-01, -2.1599e-01,\n",
       "        -2.4221e-01,  9.1074e-02, -4.3504e-02, -1.8047e-01, -1.8158e-01,\n",
       "        -6.6229e-02,  2.6480e-02, -2.5439e-01, -7.8805e-02, -2.1853e-01,\n",
       "        -3.0239e-02,  1.3049e-01,  1.0992e-01,  3.5563e-02,  3.2769e-01,\n",
       "        -2.7750e-02, -1.8852e-01, -1.8579e-01, -8.5064e-02, -2.4057e-02,\n",
       "        -1.4141e-01,  1.2708e-01, -7.9024e-02, -1.3320e-01,  3.7619e-02,\n",
       "        -1.1080e-01, -2.2742e-01, -2.7703e-01, -8.1760e-02,  4.7761e-02,\n",
       "        -1.7936e-01, -3.1907e-01,  1.3931e-01,  3.6374e-01, -2.1399e-01,\n",
       "        -2.7044e-01, -2.0847e-01,  1.1192e-02,  1.6017e-01,  2.4218e-01,\n",
       "         2.5058e-01,  3.2767e-01,  1.3340e-01,  6.4510e-03,  1.7994e-02,\n",
       "         1.1242e-01,  9.0136e-02, -7.2882e-02, -1.6506e-01,  4.8300e-01,\n",
       "         4.6465e-03, -4.8611e-02,  2.7421e-02,  9.7357e-02,  1.0873e-01,\n",
       "        -3.1722e-01,  2.6092e-01, -5.9396e-01,  2.6522e-01,  7.2456e-02,\n",
       "        -1.9246e-01,  3.4932e-02, -1.5662e-01,  3.0779e-01,  3.3744e-01,\n",
       "        -4.8664e-03,  1.5165e-01, -1.9861e-02, -1.4789e-01, -1.7031e-02,\n",
       "        -1.7279e-01, -3.8278e-02, -3.2560e-01, -1.6450e-01,  2.2257e-01,\n",
       "        -1.2442e+00, -1.4105e-01,  7.4932e-02, -1.8879e-01, -1.0341e-01,\n",
       "        -3.6646e-03, -2.1667e-01,  3.5087e-02, -1.9168e-01, -1.0044e-01,\n",
       "        -2.8554e-03,  5.6079e-02, -7.4098e-02, -8.3292e-03, -2.3693e-01,\n",
       "         4.1375e-02,  6.9771e-02,  2.0383e-01, -1.8666e-01, -4.2077e-02,\n",
       "         1.3305e-01, -4.5272e-02, -2.9631e-01,  1.6454e-01, -1.1591e-01,\n",
       "         2.4631e-02, -6.1717e-02, -6.2958e-02,  1.3487e-01,  2.0565e-01,\n",
       "         1.4678e-02,  1.9056e-01, -3.2541e-01,  1.0896e-01, -9.1117e-02,\n",
       "        -1.8126e+00,  3.5567e-01,  5.4440e-01, -3.5205e-02, -2.7339e-02,\n",
       "        -1.0750e-01, -3.0940e-01,  1.2539e-01,  8.7296e-02,  8.2909e-02,\n",
       "        -7.9026e-02, -1.8092e-02,  2.6749e-01,  1.1947e-02, -1.3090e-01,\n",
       "        -3.3304e-01, -2.0343e-01, -2.8020e-01, -1.0974e-01, -2.9613e-01,\n",
       "        -1.3973e-01,  1.8934e-01,  3.5228e-02, -2.5916e-01, -2.6859e-01,\n",
       "        -2.2678e-01,  1.3141e-01,  5.1456e-02,  2.6723e-01, -1.8335e-01,\n",
       "        -7.5100e-02, -1.6394e-02, -9.9408e-02, -1.8685e-01, -1.4718e-01,\n",
       "         6.4933e-02, -1.3858e-01, -1.9941e-01, -8.3017e-02, -1.7141e-01,\n",
       "         1.4692e-02, -2.4256e-01, -2.7010e-01, -8.5571e-02, -1.7614e-01,\n",
       "        -1.7786e-01, -1.3418e-02, -1.4634e-01,  3.0529e-01,  6.0719e-02,\n",
       "         2.8651e-01,  1.4169e-01, -1.9963e-01,  1.3507e-01,  7.0590e-02,\n",
       "        -4.2919e-02, -7.0586e-02, -3.5384e-01,  1.3877e-01,  2.4022e-01,\n",
       "        -3.0725e-02,  4.1167e-02, -2.2029e-01, -1.1796e-01,  1.1195e-01,\n",
       "         2.0739e-01, -1.6588e-01, -1.4708e-01,  9.7583e-02,  2.0831e-01,\n",
       "        -1.2357e-01, -7.9109e-02, -7.7406e-02, -3.4475e-01,  2.9539e-01,\n",
       "         1.4243e-02,  7.5820e-02, -1.9915e-02, -1.6280e-01,  1.9031e-02,\n",
       "         2.5743e-01,  1.2232e-01, -8.2438e-02,  6.1687e-02,  2.1967e-01,\n",
       "         2.2370e-01, -9.4008e-02,  1.5609e-01,  1.6797e-03,  1.8566e-01,\n",
       "        -2.6615e-01, -1.0925e-01,  3.6341e-01,  1.0227e-01,  1.6318e-01,\n",
       "        -1.5906e-01,  2.7914e-02, -9.7719e-02,  2.1670e-02,  1.3730e-01,\n",
       "         3.1447e-01,  1.6585e-02, -3.2731e-01,  4.1633e-01,  1.8380e-01,\n",
       "        -2.2986e-01, -1.2125e-01, -1.5239e-01, -1.3330e-02,  1.5810e-01,\n",
       "         3.2326e-01, -7.6780e-02, -1.2008e-01, -1.1215e-01, -5.0701e-02,\n",
       "         1.1711e-01,  1.4279e-01, -1.2161e-01, -1.5066e-02, -7.6299e-02,\n",
       "         2.2693e-01,  1.7511e-01,  2.3596e-02,  1.4218e-01,  2.1250e-01,\n",
       "         2.1344e-01, -6.8560e-02,  6.3065e-02,  5.5161e-01,  2.6941e-01,\n",
       "         1.7143e-01, -1.9773e-01,  3.0509e-02, -3.7473e-01, -2.3374e-01,\n",
       "         1.9453e-01,  2.3319e-03,  5.2309e-03,  1.6025e-01,  5.0800e-01,\n",
       "         3.0588e-01,  2.6994e-02,  1.1541e-01, -7.9795e-02, -9.9845e-02,\n",
       "        -3.1527e-02,  9.3683e-02,  1.0850e-02,  3.8706e-01, -2.6888e-01,\n",
       "        -4.0783e-01, -1.6802e-03,  1.5798e-02,  3.5886e-02,  1.1664e-01,\n",
       "         2.3909e-02, -5.3473e-02, -3.2640e-01,  1.7957e-01,  1.1095e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.1395e-01, -1.8600e-01, -3.1139e-01,  6.9232e-02, -2.2679e-02,\n",
       "         3.5245e-01, -1.7451e-01, -3.4762e-02,  3.6499e-02,  2.4352e+00,\n",
       "         1.5621e-01, -4.1787e-02, -7.2947e-02, -3.1921e-02,  8.9066e-02,\n",
       "         1.9200e-01,  1.6327e-02,  8.0790e-01, -2.3115e-02, -5.3932e-02,\n",
       "        -2.7845e-01,  2.0944e-02, -2.3572e-01,  8.7822e-02,  1.7885e-01,\n",
       "        -1.2684e-01,  1.1533e-01, -1.9993e-01,  1.7179e-01, -2.9116e-01,\n",
       "        -1.3022e-01,  1.4925e-02, -3.4743e-01, -4.2228e-01, -4.9583e-01,\n",
       "         2.5687e-01, -6.4040e-02,  1.4373e-01, -8.7055e-02,  2.3477e-02,\n",
       "         1.1067e-01,  3.4200e-01,  1.0643e-01, -5.5534e-03,  2.1959e-01,\n",
       "         3.2662e-01,  4.2147e-01, -1.1966e-01,  1.5098e-01,  5.5619e-02,\n",
       "        -3.8991e-01,  1.5516e-01, -8.2267e-02, -2.4025e-01, -3.4519e-01,\n",
       "        -1.9722e-01, -3.1244e-01, -5.8754e-01, -2.0353e-01, -7.6791e-03,\n",
       "        -9.1359e-02, -1.7571e-01, -1.7574e-01,  3.2297e-01, -2.6038e-01,\n",
       "        -2.8268e-01, -6.8955e-02,  2.6774e-02, -1.0006e-01,  3.6755e-01,\n",
       "        -1.4354e-01,  3.6205e-01, -1.1899e-01,  1.9502e-02,  6.3868e-01,\n",
       "        -1.4751e-02, -1.6021e-01,  4.8338e-01,  5.8345e-03,  3.2956e-01,\n",
       "         1.2950e-01, -2.2254e-01, -1.9471e-01, -2.5058e-01,  1.0409e-01,\n",
       "        -1.5040e-01, -3.0280e-01,  2.3811e-01,  2.9955e-01, -1.9751e-02,\n",
       "         1.8583e-03, -3.4478e-01,  1.4361e-01, -1.2800e-03,  4.9823e-01,\n",
       "        -6.4337e-02,  4.9903e-01,  5.1760e-01, -1.5414e-01, -3.1006e-01,\n",
       "        -1.6568e-01,  2.8804e-02, -2.9711e-01,  3.8346e-02,  2.3626e-01,\n",
       "        -1.5317e+00,  4.8050e-03, -4.0435e-01,  2.8585e-02,  1.5562e-01,\n",
       "         8.9044e-02, -8.2613e-01, -3.3854e-01, -2.1842e-01, -4.1949e-01,\n",
       "         1.4582e-01, -7.1290e-02, -1.7822e-01, -4.6116e-01,  8.1208e-02,\n",
       "         1.0228e-01, -2.2000e-02,  1.5704e-01, -4.6219e-01,  4.4219e-01,\n",
       "        -1.2331e-02,  6.0651e-01, -4.6436e-02,  7.1817e-02, -2.8741e-01,\n",
       "         8.0092e-02,  9.5995e-02,  2.1868e-01,  9.2623e-02,  5.9085e-02,\n",
       "        -1.5068e-01,  7.5694e-02, -4.2070e-01,  7.7992e-02,  2.3851e-01,\n",
       "        -1.4185e+00, -6.6634e-02,  1.6350e-01,  1.4849e-01, -1.9227e-01,\n",
       "         1.2907e-02, -3.4447e-01, -4.6486e-01,  1.4224e-01,  1.3265e-01,\n",
       "         8.8162e-02,  1.6292e-01,  1.3023e-01, -3.1362e-01, -3.9454e-01,\n",
       "        -1.4404e-01, -2.1622e-01, -2.0923e-01, -3.4271e-01, -1.6063e-01,\n",
       "         7.1870e-02, -1.9801e-01, -5.0524e-02,  7.4052e-05, -4.3107e-01,\n",
       "        -8.2001e-02, -3.8524e-01, -4.9691e-02,  1.0136e-01, -4.7003e-01,\n",
       "         1.1299e-01,  2.6024e-01, -1.6097e-01,  8.5316e-02, -2.4209e-01,\n",
       "         4.3188e-02, -2.8629e-01, -2.3318e-01,  1.9134e-01, -3.7870e-01,\n",
       "         1.9383e-01, -1.3177e-01, -4.2849e-01, -2.7833e-01, -6.3169e-02,\n",
       "        -2.3149e-01,  3.5011e-02, -2.3458e-01,  5.1698e-01,  8.0902e-02,\n",
       "         5.3812e-01, -1.0787e-01, -2.1465e-01, -3.5011e-01, -1.1482e-01,\n",
       "        -3.7470e-01, -1.4581e-01, -1.5781e-01, -8.8941e-02, -2.5458e-01,\n",
       "        -2.2646e-01, -1.4723e-01,  1.9318e-01,  1.5410e-01,  3.5199e-02,\n",
       "        -1.0698e-01, -4.4188e-02, -4.5023e-01,  2.6755e-01,  3.9788e-02,\n",
       "         1.6353e-01, -4.0591e-02,  1.4291e-01, -2.7215e-01,  2.4453e-01,\n",
       "         1.0331e-02,  2.1538e-01, -1.1237e-01, -3.7174e-02, -1.1926e-01,\n",
       "         2.9896e-01,  5.2738e-02, -8.8533e-02,  4.3619e-02, -1.0675e-01,\n",
       "         1.6294e-01,  2.5775e-01, -3.6536e-02,  9.5472e-02,  2.0618e-01,\n",
       "        -1.4716e-01, -2.3276e-01,  2.8959e-01,  3.0868e-02,  2.7450e-01,\n",
       "        -2.0971e-02,  2.8721e-01, -2.1334e-01,  4.2814e-01,  3.2721e-01,\n",
       "        -1.8220e-01,  2.6856e-01, -2.6014e-01,  2.3042e-01, -3.0705e-01,\n",
       "         2.9832e-01, -4.2529e-01, -3.5242e-01,  2.3699e-01, -5.1793e-03,\n",
       "         1.9576e-01,  1.3232e-01,  1.8231e-01,  9.8129e-02,  2.3562e-01,\n",
       "        -6.8167e-02,  1.2068e-01,  1.9259e-01, -1.3113e-01,  1.4483e-01,\n",
       "         3.3028e-01,  1.0524e-01, -1.6241e-01,  1.8550e-01, -5.3597e-02,\n",
       "         1.2893e-01,  1.0969e-01, -3.3200e-01,  1.4515e-01,  2.8574e-01,\n",
       "        -2.3060e-01, -2.6947e-01, -4.2454e-01, -9.0470e-01, -9.2193e-02,\n",
       "        -1.4784e-01,  5.1422e-02, -2.2871e-01,  1.0521e-01,  4.7294e-01,\n",
       "         2.0239e-01, -1.6172e-02, -2.8370e-02, -1.0857e-01,  2.3019e-01,\n",
       "         1.2928e-01, -3.9928e-01, -5.9975e-02, -1.4594e-01, -5.7901e-02,\n",
       "        -7.0202e-01, -1.8404e-01, -8.4864e-02,  4.5815e-02, -2.0621e-01,\n",
       "         4.5970e-01, -9.3192e-02, -6.1150e-01, -7.5240e-02, -1.2648e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7386e-02,  2.1880e-01,  1.7469e-01,  2.7509e-01, -2.1631e-01,\n",
       "         3.5347e-01, -2.2180e-02, -2.8373e-01,  2.5881e-01,  1.9462e+00,\n",
       "        -4.0591e-02, -1.7405e-01,  1.3791e-01,  5.1707e-01, -1.1379e-01,\n",
       "         4.3605e-01,  4.4465e-01,  1.5117e+00, -3.5965e-01,  4.3858e-02,\n",
       "        -5.1923e-02,  2.9570e-01, -7.3748e-01, -5.4741e-03, -1.7881e-01,\n",
       "        -2.2322e-01,  6.4444e-01, -5.1164e-01,  9.3286e-02, -3.9014e-01,\n",
       "        -2.3993e-01, -3.4526e-01,  6.7683e-03, -4.2794e-01,  1.0783e-01,\n",
       "         4.2653e-01,  3.0891e-01, -1.0818e-01,  1.8644e-01, -2.1826e-01,\n",
       "        -4.1482e-01,  1.0898e-02, -1.7486e-01,  3.9860e-01, -2.0510e-01,\n",
       "         4.8380e-01, -5.0375e-02,  8.0554e-01, -9.0117e-02,  9.7864e-02,\n",
       "         1.5842e-01, -3.6625e-01,  3.8516e-01, -3.7493e-02, -3.4814e-01,\n",
       "         9.3430e-02,  9.1870e-01, -2.3090e-01,  3.5374e-01,  6.5830e-02,\n",
       "         1.1644e-01,  5.0977e-01, -1.5380e-01, -1.8577e-01,  4.4437e-01,\n",
       "         2.7601e-01,  2.2735e-01,  5.3475e-01, -1.8252e-01, -7.4210e-02,\n",
       "         2.1165e-01, -5.0073e-01,  1.2662e-01, -1.2315e-01,  3.8191e-01,\n",
       "        -1.6075e-01,  5.3988e-02, -1.9419e-01,  1.7041e-02, -2.8735e-01,\n",
       "         4.7740e-01, -6.2051e-01, -3.0525e-01, -3.2444e-01, -8.1936e-02,\n",
       "         2.4708e-01,  1.6119e-01,  1.4163e-01, -2.6534e-02,  6.8003e-01,\n",
       "         3.8574e-01,  7.9060e-01,  1.7182e-01,  1.9055e-01, -4.0842e-01,\n",
       "        -7.1388e-01, -2.4055e-01,  3.0572e-01,  3.2042e-01,  9.4508e-02,\n",
       "         1.9716e-01, -3.2531e-01, -4.6567e-02,  4.9304e-03,  2.3895e-01,\n",
       "        -1.5985e+00,  4.4618e-01,  6.7059e-02,  4.2562e-02, -9.4264e-02,\n",
       "         2.6359e-01,  5.9031e-02,  2.1923e-01,  1.1912e-01, -4.8703e-01,\n",
       "        -1.6320e-01,  1.3074e-01,  1.3469e-01, -8.6061e-04,  6.5107e-02,\n",
       "         2.6348e-01, -6.8230e-02, -3.4943e-01,  1.4124e-03, -3.2127e-01,\n",
       "        -3.2119e-01,  2.4275e-01,  1.4489e-01,  5.2155e-01,  1.8236e-02,\n",
       "         3.8936e-01,  1.7392e-01, -1.8137e-01,  2.3953e-01,  7.9878e-02,\n",
       "         2.3554e-01,  3.4491e-01,  1.3673e-01, -2.4935e-02,  1.2661e-01,\n",
       "        -9.4117e-01, -2.7312e-01, -3.9914e-02, -2.1242e-02, -2.4627e-01,\n",
       "         1.8920e-01, -3.7878e-02, -7.5084e-01,  3.1216e-01, -8.1300e-02,\n",
       "        -1.4081e-01,  1.9424e-01,  5.0618e-01,  5.5668e-02, -2.2838e-01,\n",
       "        -7.9835e-02,  3.3727e-01, -3.9557e-01, -3.1844e-01, -6.1797e-01,\n",
       "        -4.3276e-01, -1.0177e-02, -1.6727e-01, -1.4701e-01, -1.3138e-03,\n",
       "        -3.6874e-01,  8.8321e-03,  1.0844e-01,  2.2943e-01, -5.6433e-01,\n",
       "         3.1256e-01, -1.9476e-01, -7.2129e-02, -8.9461e-01, -5.7574e-01,\n",
       "        -9.0824e-02, -6.1415e-01,  1.7660e-01, -2.6355e-01,  6.2693e-01,\n",
       "        -4.5756e-01, -8.6851e-01, -2.4981e-01,  5.5472e-01, -8.5896e-02,\n",
       "         1.1540e-01,  3.1408e-01,  3.9062e-01, -3.4876e-01,  5.4840e-02,\n",
       "        -2.2244e-01,  5.9099e-02,  9.9086e-02, -5.1477e-01, -1.7693e-01,\n",
       "         1.3470e-01, -2.5853e-01, -4.6366e-01,  2.2667e-01,  2.6040e-01,\n",
       "        -3.4442e-01,  2.5386e-01,  3.2574e-02,  3.5130e-01, -6.8617e-01,\n",
       "        -1.3479e-01, -1.5026e-01,  1.6556e-01,  3.2562e-01,  1.3296e-01,\n",
       "         2.2568e-01,  2.6165e-01,  9.7049e-03,  4.1276e-02, -3.9056e-01,\n",
       "        -2.5348e-01,  1.1446e-01, -2.1078e-01,  3.7831e-02, -4.9510e-02,\n",
       "         3.3036e-01,  2.0437e-01, -5.9281e-01,  1.0520e-01, -4.3911e-01,\n",
       "         5.1661e-01, -1.9186e-01,  6.2483e-01,  2.6081e-02, -1.2373e-01,\n",
       "        -2.3343e-01,  7.1194e-01,  3.1010e-01,  1.5132e-01, -2.8571e-01,\n",
       "         2.9020e-02, -6.9205e-02, -2.8972e-01,  1.1664e-03,  9.4657e-03,\n",
       "        -3.1325e-01, -3.6959e-01,  6.3799e-01,  4.5772e-03, -4.4835e-02,\n",
       "        -1.0021e-01,  2.0841e-01, -8.3103e-02,  3.5557e-02,  5.7080e-01,\n",
       "         1.3201e-02,  6.1434e-02,  7.9083e-02, -5.3205e-02,  1.2540e-01,\n",
       "         1.2251e-01,  2.2707e-01, -6.0252e-02, -1.3078e-01,  1.8732e-01,\n",
       "        -2.4053e-01,  3.9825e-01,  9.7576e-02, -6.0194e-01, -9.7147e-02,\n",
       "         5.0851e-01,  9.4998e-02,  2.9620e-01,  6.9577e-01,  2.7756e-03,\n",
       "         6.0900e-02, -2.9739e-01, -2.0041e-01, -3.3111e-01,  3.2232e-01,\n",
       "         2.9970e-01,  7.8138e-02, -2.1120e-01,  2.4587e-01,  7.2880e-01,\n",
       "        -5.8055e-02, -3.1365e-02,  5.0065e-01, -4.5560e-03, -2.1692e-01,\n",
       "        -1.8249e-01, -3.7010e-01,  1.9931e-01,  3.1825e-02, -1.2789e-01,\n",
       "         5.8350e-01,  4.7768e-01,  3.3556e-01,  1.1028e-01,  1.5622e-01,\n",
       "        -7.6022e-02,  2.5118e-01,  5.3027e-02, -7.8588e-02, -4.7783e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7675e-01, -1.1799e-01,  1.7866e-01,  3.2482e-02, -4.9160e-01,\n",
       "         4.4509e-02, -3.8697e-01,  3.4402e-02, -4.0194e-02,  3.1124e+00,\n",
       "        -2.2361e-01,  1.1824e-01,  6.9428e-02, -3.2642e-02,  1.4955e-01,\n",
       "         1.5286e-01,  3.2991e-02,  6.3527e-01, -2.3576e-01,  2.4908e-01,\n",
       "         2.0384e-01,  1.4182e-01,  2.0118e-01, -2.0678e-01, -1.6844e-01,\n",
       "         1.3097e-01,  4.8774e-01, -3.5005e-01, -7.5147e-02, -1.5022e-01,\n",
       "         1.6138e-01, -5.5018e-01, -1.9398e-01, -2.8315e-02,  1.6917e-01,\n",
       "        -4.0531e-01,  6.5695e-02, -1.3529e-01,  1.7965e-01,  7.5198e-02,\n",
       "         1.2148e-01,  6.1736e-01,  3.6658e-02, -5.6777e-01,  3.2880e-01,\n",
       "        -2.4745e-01,  1.3974e-01,  7.4644e-01, -1.9929e-01,  3.6678e-01,\n",
       "         2.9422e-01, -8.5806e-02,  1.2210e-01,  1.6887e-01, -2.1271e-01,\n",
       "         7.3281e-03, -7.6509e-02,  1.7360e-01,  3.0140e-01, -6.2679e-02,\n",
       "        -2.4980e-01, -6.2602e-02, -2.8363e-01,  1.1710e-01,  1.4115e-01,\n",
       "        -5.0121e-01,  1.2329e-01, -1.8731e-01,  3.9721e-01, -2.5896e-02,\n",
       "         2.9596e-01, -3.2547e-02,  1.7333e-01, -1.1771e-01,  1.2088e-01,\n",
       "        -3.5890e-01, -1.3854e-01, -8.9703e-02,  3.1391e-01, -9.1165e-02,\n",
       "        -1.3064e-01, -2.2216e-02,  1.5877e-02,  2.6226e-01, -2.7216e-01,\n",
       "        -1.5718e-01, -8.3953e-01, -1.4901e-01,  1.2313e-01,  3.2840e-01,\n",
       "        -1.1891e-01,  5.5510e-02,  1.8767e-01,  5.3449e-01, -2.0262e-01,\n",
       "         1.6928e-01, -1.9827e-01, -2.4103e-01, -1.8439e-02, -9.7009e-02,\n",
       "        -7.1952e-02, -3.6047e-01, -9.6811e-02, -2.9746e-01, -2.7191e-02,\n",
       "        -1.4366e+00,  4.3149e-01,  1.3098e-01, -7.2661e-02,  1.7736e-01,\n",
       "         1.6811e-01,  1.7400e-02,  1.7484e-01, -2.2591e-01,  4.0063e-01,\n",
       "         1.2184e-01,  6.9323e-02, -1.1818e-01,  3.1484e-01, -1.0699e-01,\n",
       "         4.0806e-01, -1.2300e-01, -3.1155e-01,  2.1126e-01, -5.7120e-02,\n",
       "         8.6237e-02, -4.4460e-01,  2.2277e-04,  2.4001e-02,  3.0837e-01,\n",
       "        -1.6179e-01,  2.5340e-01, -4.2660e-02,  2.5280e-03,  9.9567e-02,\n",
       "        -5.1727e-02,  3.5747e-02, -1.7966e-01,  3.3886e-01, -1.6398e-01,\n",
       "        -7.8192e-01,  6.0500e-01,  2.8252e-01, -1.2994e-01, -1.6583e-02,\n",
       "        -3.4698e-01, -7.2527e-01,  8.0799e-02,  9.4704e-02,  3.6744e-01,\n",
       "        -8.9103e-02,  6.9915e-02,  1.6852e-01, -1.2281e-01, -2.8876e-02,\n",
       "        -7.7054e-02, -1.8588e-01, -1.8323e-01, -1.9980e-01,  4.5056e-02,\n",
       "        -7.6702e-02,  3.8135e-01, -3.8801e-01, -2.1650e-01, -3.0306e-02,\n",
       "         1.6275e-01, -1.0535e-02, -1.6363e-01,  7.2000e-04, -2.2225e-02,\n",
       "        -7.1306e-02, -2.7842e-02,  2.6646e-01, -2.7049e-01, -2.1350e-02,\n",
       "        -4.4271e-02, -1.2112e-01,  1.1946e-01, -1.1000e-02,  3.2661e-01,\n",
       "        -1.8398e-01,  7.4138e-02, -4.2220e-02,  9.7410e-02, -1.4307e-01,\n",
       "         1.0406e-02,  2.1065e-02,  1.9372e-01, -1.3614e-01,  1.1002e-01,\n",
       "         1.5361e-01, -5.0540e-02,  8.6166e-02,  2.4223e-01,  1.4647e-01,\n",
       "        -1.3835e-01, -6.9210e-02, -1.4987e-01,  1.8680e-01,  3.9816e-01,\n",
       "         5.7365e-01,  1.5033e-02,  4.5188e-02,  1.6298e-01,  9.3265e-02,\n",
       "        -7.3806e-02, -3.6836e-01, -1.0767e-01,  4.6960e-01, -1.3725e-01,\n",
       "         1.1711e-01,  2.3232e-01,  2.1569e-02,  8.2387e-03,  2.0852e-01,\n",
       "         1.8448e-01,  2.6981e-01, -7.7155e-02, -1.5074e-01, -1.4044e-01,\n",
       "         1.5819e-01, -3.1127e-01, -2.3062e-01,  1.6080e-01,  1.3612e-01,\n",
       "        -6.8917e-02,  2.7529e-01, -2.0376e-01, -4.3880e-02,  3.3157e-01,\n",
       "         1.8735e-01,  5.5636e-02,  1.2360e-01, -1.1264e-01,  1.3654e-01,\n",
       "         3.6212e-01, -2.0679e-01,  2.2256e-02,  8.0054e-02, -5.1004e-02,\n",
       "         1.5415e-01, -4.8844e-01, -1.7730e-01,  4.1286e-01, -2.8209e-01,\n",
       "        -2.8011e-01, -1.5195e-01, -3.5349e-01,  3.4257e-01,  1.2766e-01,\n",
       "        -4.7474e-02,  3.2728e-01, -6.9187e-01,  1.7493e-01,  8.9532e-02,\n",
       "        -6.1105e-02, -6.9829e-02, -1.5193e-01,  5.7987e-02,  3.0111e-02,\n",
       "        -3.9252e-02,  5.3934e-02,  4.5133e-01,  4.2870e-02, -2.2657e-01,\n",
       "        -3.5830e-01, -5.6488e-02,  1.0440e-01,  4.9360e-01,  2.7961e-01,\n",
       "         5.4662e-01, -1.3974e-01, -2.1185e-01, -4.5541e-01, -6.9248e-02,\n",
       "        -3.5238e-01,  2.2941e-01, -2.4661e-01,  1.6707e-01,  2.9528e-01,\n",
       "        -1.2137e-01, -2.0418e-01,  2.4620e-01,  3.4143e-01, -2.9176e-01,\n",
       "        -5.7014e-01, -2.6190e-01, -3.2669e-01,  3.1928e-01, -3.0172e-01,\n",
       "        -1.3678e-01,  4.1218e-01,  1.1811e-01,  2.8056e-01,  4.2607e-02,\n",
       "        -4.6627e-01,  2.9769e-01, -4.3038e-02, -5.6063e-01,  2.5515e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_test[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "       -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "       -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "       -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "       -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "        3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "        3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "       -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "        3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "       -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "       -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "        3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "       -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "       -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "        1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "       -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "       -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "        6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "       -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "        4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "       -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "       -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "        2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "        2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "        1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "        1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "       -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "       -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "       -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "       -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "       -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "        4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "       -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "       -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "        1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "       -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "       -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "        4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "       -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "        1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "        1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "        7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "       -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "        7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "       -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "       -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "       -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "       -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "        8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "       -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "       -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "        3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "       -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "       -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "        3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "       -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "        4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "       -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "       -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "        8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.Dataset(examples, fields)\n",
    "data_set_test = data.Dataset(examples_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.sort_key = lambda x: len(x.text)\n",
    "data_set_test.sort_key = lambda x: len(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data_set.split([0.8, 0.1, 0.1], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124848"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(train_data.sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set_test.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(data_set_test.sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)\n",
    "\n",
    "kaggle_test_iterator = data.BucketIterator(\n",
    "    data_set_test, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_iterator = iter(train_iterator)\n",
    "batch = next(raw_train_iterator)\n",
    "\n",
    "raw_kaggle_test_iterator = iter(kaggle_test_iterator)\n",
    "batch_test = next(raw_kaggle_test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = batch.text\n",
    "\n",
    "c, d = batch_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 308])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 1, 3, 3, 1, 3, 3, 2, 2, 2, 2, 2, 0, 2, 1, 4, 3, 2, 3, 2,\n",
       "        2, 2, 1, 3, 1, 2, 2, 1, 3, 1, 2, 2, 1, 4, 2, 3, 2, 3, 2, 2, 1, 4, 2, 2,\n",
       "        2, 3, 2, 3, 1, 2, 2, 3, 4, 2, 2, 4, 2, 2, 1, 4], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133918,  57783,  94049,  86671,  84086,  74128, 112683, 126943,  12599,\n",
       "         138419,  47937,  62635, 104584,  86587, 113162,  81036,  62472, 147974,\n",
       "          32209, 133654,  95241, 137072,  66415,  75555,  53039, 130485,  79837,\n",
       "         152257,  39951, 136680, 121126, 102615, 101662, 119728, 108929, 145776,\n",
       "          36689,  66928, 109732, 109843, 130506,  33026, 123268, 155495, 114095,\n",
       "          27453, 122599,  82520,  21535, 104973,  91096,  97182,  84542, 119081,\n",
       "         104435,  65401,  30863, 141808,  39997, 145932,  46149, 125167, 132902,\n",
       "         117864]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data['Sentiment'][117864-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10.], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 308])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.phrase_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId                                                       193462\n",
       "SentenceId                                                      10289\n",
       "Phrase                         you 're not into the Pokemon franchise\n",
       "Phrase_length                                                      72\n",
       "Tokenized_phrase    [xxbos, you, 're, not, into, the, xxmaj, pokem...\n",
       "Indexed_phrase               [2, 33, 157, 41, 61, 8, 7, 2893, 978, 3]\n",
       "Name: 37401, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.iloc[37401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "        -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "         1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "        -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "         6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "        -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "        -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "         8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "        -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "         1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "         5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "        -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "         1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "        -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "         2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "        -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "         3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "        -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "        -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "         1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "        -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "        -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "        -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "         7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "         6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "        -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "        -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "        -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "        -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "        -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "         4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "         3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "        -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "        -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "         7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "        -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "         8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "         8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "         3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "         1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "         1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "        -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "         2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "         4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "        -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "        -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "        -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "         2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "         2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "         1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "        -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "         1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "         3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "         1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "        -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "        -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "         1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "        -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "        -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "         1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[63][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "       -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "        1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "       -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "        6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "       -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "       -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "        8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "       -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "        1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "        5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "       -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "        1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "       -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "        2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "       -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "        3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "       -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "       -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "        1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "       -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "       -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "       -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "        7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "        6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "       -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "       -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "       -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "       -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "       -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "        4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "        3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "       -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "       -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "        7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "       -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "        8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "        8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "        3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "        1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "        1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "       -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "        2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "        4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "       -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "       -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "       -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "        2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "        2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "        1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "       -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "        1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "        3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "        1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "       -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "       -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "        1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "       -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "       -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "        1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector(\"'re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU( embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(text, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        #if self.bidirectional:\n",
    "        #    hidden = [batch size, hid dim * num directions]\n",
    "        #else:\n",
    "        #    hidden = [batch size, hid dim]\n",
    "        \n",
    "        # with RELU\n",
    "        #return self.fc(self.relu(hidden))\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 308\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "# Regularization hyperparameter\n",
    "DROPOUT = 0\n",
    "L2_LAMBDA = 0.001\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "N_EPOCHS = 240\n",
    "\n",
    "MODEL_SAVE_FILE = 'simple-GRU_origin.pt'\n",
    "model = GRU(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,054,661 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define epoch time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01501 | Train Acc: 60.44%\n",
      "\t Val. Loss: 0.01448 |  Val. Acc: 61.48%\n",
      "Epoch: 02 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01418 | Train Acc: 62.25%\n",
      "\t Val. Loss: 0.01415 |  Val. Acc: 62.29%\n",
      "Epoch: 03 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01403 | Train Acc: 62.68%\n",
      "\t Val. Loss: 0.01409 |  Val. Acc: 62.57%\n",
      "Epoch: 04 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01394 | Train Acc: 62.87%\n",
      "\t Val. Loss: 0.01404 |  Val. Acc: 62.66%\n",
      "Epoch: 05 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01384 | Train Acc: 63.03%\n",
      "\t Val. Loss: 0.01391 |  Val. Acc: 62.82%\n",
      "Epoch: 06 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01378 | Train Acc: 63.19%\n",
      "\t Val. Loss: 0.01397 |  Val. Acc: 62.86%\n",
      "Epoch: 07 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01369 | Train Acc: 63.37%\n",
      "\t Val. Loss: 0.01380 |  Val. Acc: 62.91%\n",
      "Epoch: 08 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01363 | Train Acc: 63.50%\n",
      "\t Val. Loss: 0.01375 |  Val. Acc: 63.32%\n",
      "Epoch: 09 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01354 | Train Acc: 63.84%\n",
      "\t Val. Loss: 0.01365 |  Val. Acc: 63.65%\n",
      "Epoch: 10 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01346 | Train Acc: 64.07%\n",
      "\t Val. Loss: 0.01358 |  Val. Acc: 63.76%\n",
      "Epoch: 11 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01342 | Train Acc: 64.11%\n",
      "\t Val. Loss: 0.01366 |  Val. Acc: 63.79%\n",
      "Epoch: 12 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01335 | Train Acc: 64.29%\n",
      "\t Val. Loss: 0.01349 |  Val. Acc: 64.07%\n",
      "Epoch: 13 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01329 | Train Acc: 64.47%\n",
      "\t Val. Loss: 0.01345 |  Val. Acc: 64.48%\n",
      "Epoch: 14 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01324 | Train Acc: 64.62%\n",
      "\t Val. Loss: 0.01338 |  Val. Acc: 64.42%\n",
      "Epoch: 15 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01320 | Train Acc: 64.72%\n",
      "\t Val. Loss: 0.01338 |  Val. Acc: 64.65%\n",
      "Epoch: 16 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01317 | Train Acc: 64.83%\n",
      "\t Val. Loss: 0.01334 |  Val. Acc: 64.73%\n",
      "Epoch: 17 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01312 | Train Acc: 64.95%\n",
      "\t Val. Loss: 0.01343 |  Val. Acc: 64.17%\n",
      "Epoch: 18 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01308 | Train Acc: 65.10%\n",
      "\t Val. Loss: 0.01340 |  Val. Acc: 64.66%\n",
      "Epoch: 19 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01305 | Train Acc: 65.12%\n",
      "\t Val. Loss: 0.01325 |  Val. Acc: 65.06%\n",
      "Epoch: 20 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01302 | Train Acc: 65.33%\n",
      "\t Val. Loss: 0.01329 |  Val. Acc: 64.74%\n",
      "Epoch: 21 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01298 | Train Acc: 65.29%\n",
      "\t Val. Loss: 0.01321 |  Val. Acc: 64.89%\n",
      "Epoch: 22 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01295 | Train Acc: 65.41%\n",
      "\t Val. Loss: 0.01322 |  Val. Acc: 64.92%\n",
      "Epoch: 23 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01291 | Train Acc: 65.61%\n",
      "\t Val. Loss: 0.01323 |  Val. Acc: 65.15%\n",
      "Epoch: 24 | Epoch Time: 1m 5s\n",
      "\tTrain Loss: 0.01290 | Train Acc: 65.58%\n",
      "\t Val. Loss: 0.01323 |  Val. Acc: 65.10%\n",
      "Epoch: 25 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01285 | Train Acc: 65.78%\n",
      "\t Val. Loss: 0.01311 |  Val. Acc: 65.58%\n",
      "Epoch: 26 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01284 | Train Acc: 65.76%\n",
      "\t Val. Loss: 0.01316 |  Val. Acc: 65.19%\n",
      "Epoch: 27 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01281 | Train Acc: 65.82%\n",
      "\t Val. Loss: 0.01309 |  Val. Acc: 65.17%\n",
      "Epoch: 28 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01277 | Train Acc: 65.99%\n",
      "\t Val. Loss: 0.01308 |  Val. Acc: 65.09%\n",
      "Epoch: 29 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01275 | Train Acc: 66.03%\n",
      "\t Val. Loss: 0.01304 |  Val. Acc: 65.46%\n",
      "Epoch: 30 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01271 | Train Acc: 66.13%\n",
      "\t Val. Loss: 0.01310 |  Val. Acc: 65.09%\n",
      "Epoch: 31 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01272 | Train Acc: 66.16%\n",
      "\t Val. Loss: 0.01305 |  Val. Acc: 65.49%\n",
      "Epoch: 32 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01266 | Train Acc: 66.14%\n",
      "\t Val. Loss: 0.01307 |  Val. Acc: 65.10%\n",
      "Epoch: 33 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01263 | Train Acc: 66.36%\n",
      "\t Val. Loss: 0.01301 |  Val. Acc: 65.27%\n",
      "Epoch: 34 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01262 | Train Acc: 66.33%\n",
      "\t Val. Loss: 0.01292 |  Val. Acc: 65.75%\n",
      "Epoch: 35 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01258 | Train Acc: 66.51%\n",
      "\t Val. Loss: 0.01292 |  Val. Acc: 65.70%\n",
      "Epoch: 36 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01255 | Train Acc: 66.52%\n",
      "\t Val. Loss: 0.01296 |  Val. Acc: 65.80%\n",
      "Epoch: 37 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01253 | Train Acc: 66.50%\n",
      "\t Val. Loss: 0.01291 |  Val. Acc: 65.59%\n",
      "Epoch: 38 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01250 | Train Acc: 66.64%\n",
      "\t Val. Loss: 0.01288 |  Val. Acc: 65.98%\n",
      "Epoch: 39 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01247 | Train Acc: 66.72%\n",
      "\t Val. Loss: 0.01290 |  Val. Acc: 65.85%\n",
      "Epoch: 40 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01247 | Train Acc: 66.71%\n",
      "\t Val. Loss: 0.01282 |  Val. Acc: 66.19%\n",
      "Epoch: 41 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01242 | Train Acc: 66.88%\n",
      "\t Val. Loss: 0.01288 |  Val. Acc: 65.77%\n",
      "Epoch: 42 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01240 | Train Acc: 66.98%\n",
      "\t Val. Loss: 0.01282 |  Val. Acc: 66.05%\n",
      "Epoch: 43 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01239 | Train Acc: 66.93%\n",
      "\t Val. Loss: 0.01276 |  Val. Acc: 66.56%\n",
      "Epoch: 44 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01235 | Train Acc: 67.08%\n",
      "\t Val. Loss: 0.01278 |  Val. Acc: 66.20%\n",
      "Epoch: 45 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01235 | Train Acc: 67.11%\n",
      "\t Val. Loss: 0.01282 |  Val. Acc: 65.96%\n",
      "Epoch: 46 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01232 | Train Acc: 67.16%\n",
      "\t Val. Loss: 0.01282 |  Val. Acc: 66.38%\n",
      "Epoch: 47 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01228 | Train Acc: 67.26%\n",
      "\t Val. Loss: 0.01279 |  Val. Acc: 66.29%\n",
      "Epoch: 48 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01229 | Train Acc: 67.23%\n",
      "\t Val. Loss: 0.01272 |  Val. Acc: 66.17%\n",
      "Epoch: 49 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01224 | Train Acc: 67.28%\n",
      "\t Val. Loss: 0.01287 |  Val. Acc: 65.88%\n",
      "Epoch: 50 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01222 | Train Acc: 67.50%\n",
      "\t Val. Loss: 0.01282 |  Val. Acc: 66.06%\n",
      "Epoch: 51 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01219 | Train Acc: 67.49%\n",
      "\t Val. Loss: 0.01278 |  Val. Acc: 65.99%\n",
      "Epoch: 52 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01217 | Train Acc: 67.49%\n",
      "\t Val. Loss: 0.01264 |  Val. Acc: 66.55%\n",
      "Epoch: 53 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01216 | Train Acc: 67.59%\n",
      "\t Val. Loss: 0.01266 |  Val. Acc: 66.48%\n",
      "Epoch: 54 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01213 | Train Acc: 67.66%\n",
      "\t Val. Loss: 0.01276 |  Val. Acc: 66.33%\n",
      "Epoch: 55 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01211 | Train Acc: 67.64%\n",
      "\t Val. Loss: 0.01263 |  Val. Acc: 66.63%\n",
      "Epoch: 56 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01211 | Train Acc: 67.69%\n",
      "\t Val. Loss: 0.01261 |  Val. Acc: 66.69%\n",
      "Epoch: 57 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01208 | Train Acc: 67.77%\n",
      "\t Val. Loss: 0.01275 |  Val. Acc: 66.19%\n",
      "Epoch: 58 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01207 | Train Acc: 67.85%\n",
      "\t Val. Loss: 0.01264 |  Val. Acc: 66.31%\n",
      "Epoch: 59 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01205 | Train Acc: 67.94%\n",
      "\t Val. Loss: 0.01262 |  Val. Acc: 66.72%\n",
      "Epoch: 60 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01203 | Train Acc: 68.00%\n",
      "\t Val. Loss: 0.01260 |  Val. Acc: 66.74%\n",
      "Epoch: 61 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01200 | Train Acc: 68.08%\n",
      "\t Val. Loss: 0.01263 |  Val. Acc: 66.71%\n",
      "Epoch: 62 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01198 | Train Acc: 68.17%\n",
      "\t Val. Loss: 0.01266 |  Val. Acc: 66.51%\n",
      "Epoch: 63 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01197 | Train Acc: 67.99%\n",
      "\t Val. Loss: 0.01255 |  Val. Acc: 66.81%\n",
      "Epoch: 64 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01196 | Train Acc: 68.09%\n",
      "\t Val. Loss: 0.01258 |  Val. Acc: 66.74%\n",
      "Epoch: 65 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01193 | Train Acc: 68.20%\n",
      "\t Val. Loss: 0.01256 |  Val. Acc: 66.91%\n",
      "Epoch: 66 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01191 | Train Acc: 68.26%\n",
      "\t Val. Loss: 0.01250 |  Val. Acc: 67.20%\n",
      "Epoch: 67 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01191 | Train Acc: 68.29%\n",
      "\t Val. Loss: 0.01267 |  Val. Acc: 66.61%\n",
      "Epoch: 68 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01191 | Train Acc: 68.29%\n",
      "\t Val. Loss: 0.01254 |  Val. Acc: 66.92%\n",
      "Epoch: 69 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01186 | Train Acc: 68.41%\n",
      "\t Val. Loss: 0.01254 |  Val. Acc: 67.06%\n",
      "Epoch: 70 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01186 | Train Acc: 68.29%\n",
      "\t Val. Loss: 0.01255 |  Val. Acc: 66.76%\n",
      "Epoch: 71 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01183 | Train Acc: 68.59%\n",
      "\t Val. Loss: 0.01253 |  Val. Acc: 67.17%\n",
      "Epoch: 72 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01185 | Train Acc: 68.32%\n",
      "\t Val. Loss: 0.01260 |  Val. Acc: 66.52%\n",
      "Epoch: 73 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01182 | Train Acc: 68.53%\n",
      "\t Val. Loss: 0.01270 |  Val. Acc: 66.38%\n",
      "Epoch: 74 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01179 | Train Acc: 68.63%\n",
      "\t Val. Loss: 0.01252 |  Val. Acc: 67.01%\n",
      "Epoch: 75 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01177 | Train Acc: 68.54%\n",
      "\t Val. Loss: 0.01279 |  Val. Acc: 66.30%\n",
      "Epoch: 76 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01178 | Train Acc: 68.53%\n",
      "\t Val. Loss: 0.01310 |  Val. Acc: 65.31%\n",
      "Epoch: 77 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01175 | Train Acc: 68.63%\n",
      "\t Val. Loss: 0.01265 |  Val. Acc: 66.78%\n",
      "Epoch: 78 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01174 | Train Acc: 68.72%\n",
      "\t Val. Loss: 0.01243 |  Val. Acc: 67.18%\n",
      "Epoch: 79 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01171 | Train Acc: 68.71%\n",
      "\t Val. Loss: 0.01245 |  Val. Acc: 67.14%\n",
      "Epoch: 80 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01169 | Train Acc: 68.76%\n",
      "\t Val. Loss: 0.01258 |  Val. Acc: 66.87%\n",
      "Epoch: 81 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01170 | Train Acc: 68.78%\n",
      "\t Val. Loss: 0.01250 |  Val. Acc: 67.10%\n",
      "Epoch: 82 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01169 | Train Acc: 68.77%\n",
      "\t Val. Loss: 0.01316 |  Val. Acc: 65.44%\n",
      "Epoch: 83 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01169 | Train Acc: 68.82%\n",
      "\t Val. Loss: 0.01252 |  Val. Acc: 67.15%\n",
      "Epoch: 84 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01165 | Train Acc: 68.95%\n",
      "\t Val. Loss: 0.01244 |  Val. Acc: 67.34%\n",
      "Epoch: 85 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01164 | Train Acc: 68.97%\n",
      "\t Val. Loss: 0.01241 |  Val. Acc: 67.08%\n",
      "Epoch: 86 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01164 | Train Acc: 68.97%\n",
      "\t Val. Loss: 0.01239 |  Val. Acc: 67.44%\n",
      "Epoch: 87 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01164 | Train Acc: 69.03%\n",
      "\t Val. Loss: 0.01314 |  Val. Acc: 65.35%\n",
      "Epoch: 88 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01161 | Train Acc: 69.01%\n",
      "\t Val. Loss: 0.01248 |  Val. Acc: 67.13%\n",
      "Epoch: 89 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01161 | Train Acc: 69.05%\n",
      "\t Val. Loss: 0.01243 |  Val. Acc: 67.22%\n",
      "Epoch: 90 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01156 | Train Acc: 69.17%\n",
      "\t Val. Loss: 0.01249 |  Val. Acc: 67.16%\n",
      "Epoch: 91 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01157 | Train Acc: 69.13%\n",
      "\t Val. Loss: 0.01240 |  Val. Acc: 67.45%\n",
      "Epoch: 92 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01156 | Train Acc: 69.20%\n",
      "\t Val. Loss: 0.01235 |  Val. Acc: 67.54%\n",
      "Epoch: 93 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01155 | Train Acc: 69.21%\n",
      "\t Val. Loss: 0.01236 |  Val. Acc: 67.52%\n",
      "Epoch: 94 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01152 | Train Acc: 69.29%\n",
      "\t Val. Loss: 0.01246 |  Val. Acc: 66.99%\n",
      "Epoch: 95 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01152 | Train Acc: 69.29%\n",
      "\t Val. Loss: 0.01241 |  Val. Acc: 67.08%\n",
      "Epoch: 96 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01154 | Train Acc: 69.29%\n",
      "\t Val. Loss: 0.01265 |  Val. Acc: 66.56%\n",
      "Epoch: 97 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01150 | Train Acc: 69.33%\n",
      "\t Val. Loss: 0.01240 |  Val. Acc: 67.33%\n",
      "Epoch: 98 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01149 | Train Acc: 69.32%\n",
      "\t Val. Loss: 0.01238 |  Val. Acc: 67.59%\n",
      "Epoch: 99 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01149 | Train Acc: 69.38%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.59%\n",
      "Epoch: 100 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01148 | Train Acc: 69.39%\n",
      "\t Val. Loss: 0.01245 |  Val. Acc: 67.35%\n",
      "Epoch: 101 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01147 | Train Acc: 69.45%\n",
      "\t Val. Loss: 0.01251 |  Val. Acc: 67.26%\n",
      "Epoch: 102 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01144 | Train Acc: 69.47%\n",
      "\t Val. Loss: 0.01244 |  Val. Acc: 67.23%\n",
      "Epoch: 103 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01142 | Train Acc: 69.56%\n",
      "\t Val. Loss: 0.01237 |  Val. Acc: 67.36%\n",
      "Epoch: 104 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01145 | Train Acc: 69.49%\n",
      "\t Val. Loss: 0.01234 |  Val. Acc: 67.40%\n",
      "Epoch: 105 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01143 | Train Acc: 69.52%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.63%\n",
      "Epoch: 106 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01140 | Train Acc: 69.62%\n",
      "\t Val. Loss: 0.01232 |  Val. Acc: 67.51%\n",
      "Epoch: 107 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01140 | Train Acc: 69.60%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.77%\n",
      "Epoch: 108 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01139 | Train Acc: 69.62%\n",
      "\t Val. Loss: 0.01230 |  Val. Acc: 67.71%\n",
      "Epoch: 109 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01139 | Train Acc: 69.66%\n",
      "\t Val. Loss: 0.01232 |  Val. Acc: 67.58%\n",
      "Epoch: 110 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01137 | Train Acc: 69.73%\n",
      "\t Val. Loss: 0.01229 |  Val. Acc: 67.74%\n",
      "Epoch: 111 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01134 | Train Acc: 69.84%\n",
      "\t Val. Loss: 0.01245 |  Val. Acc: 67.10%\n",
      "Epoch: 112 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01133 | Train Acc: 69.80%\n",
      "\t Val. Loss: 0.01242 |  Val. Acc: 67.54%\n",
      "Epoch: 113 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01134 | Train Acc: 69.69%\n",
      "\t Val. Loss: 0.01240 |  Val. Acc: 67.37%\n",
      "Epoch: 114 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01131 | Train Acc: 69.89%\n",
      "\t Val. Loss: 0.01234 |  Val. Acc: 67.34%\n",
      "Epoch: 115 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01132 | Train Acc: 69.86%\n",
      "\t Val. Loss: 0.01230 |  Val. Acc: 67.54%\n",
      "Epoch: 116 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01130 | Train Acc: 69.86%\n",
      "\t Val. Loss: 0.01254 |  Val. Acc: 66.94%\n",
      "Epoch: 117 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01132 | Train Acc: 69.87%\n",
      "\t Val. Loss: 0.01262 |  Val. Acc: 66.69%\n",
      "Epoch: 118 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01129 | Train Acc: 69.86%\n",
      "\t Val. Loss: 0.01227 |  Val. Acc: 67.60%\n",
      "Epoch: 119 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01129 | Train Acc: 69.93%\n",
      "\t Val. Loss: 0.01231 |  Val. Acc: 67.67%\n",
      "Epoch: 120 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01128 | Train Acc: 69.87%\n",
      "\t Val. Loss: 0.01221 |  Val. Acc: 67.93%\n",
      "Epoch: 121 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01127 | Train Acc: 69.87%\n",
      "\t Val. Loss: 0.01222 |  Val. Acc: 67.82%\n",
      "Epoch: 122 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01125 | Train Acc: 69.98%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.19%\n",
      "Epoch: 123 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01127 | Train Acc: 69.92%\n",
      "\t Val. Loss: 0.01228 |  Val. Acc: 67.84%\n",
      "Epoch: 124 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01121 | Train Acc: 70.19%\n",
      "\t Val. Loss: 0.01236 |  Val. Acc: 67.54%\n",
      "Epoch: 125 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01124 | Train Acc: 70.07%\n",
      "\t Val. Loss: 0.01244 |  Val. Acc: 67.20%\n",
      "Epoch: 126 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01123 | Train Acc: 70.05%\n",
      "\t Val. Loss: 0.01228 |  Val. Acc: 67.99%\n",
      "Epoch: 127 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01120 | Train Acc: 70.12%\n",
      "\t Val. Loss: 0.01225 |  Val. Acc: 68.01%\n",
      "Epoch: 128 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01120 | Train Acc: 70.10%\n",
      "\t Val. Loss: 0.01228 |  Val. Acc: 67.64%\n",
      "Epoch: 129 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01120 | Train Acc: 70.26%\n",
      "\t Val. Loss: 0.01226 |  Val. Acc: 67.58%\n",
      "Epoch: 130 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01120 | Train Acc: 70.11%\n",
      "\t Val. Loss: 0.01221 |  Val. Acc: 68.03%\n",
      "Epoch: 131 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01114 | Train Acc: 70.29%\n",
      "\t Val. Loss: 0.01297 |  Val. Acc: 66.43%\n",
      "Epoch: 132 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01116 | Train Acc: 70.31%\n",
      "\t Val. Loss: 0.01241 |  Val. Acc: 67.40%\n",
      "Epoch: 133 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01114 | Train Acc: 70.30%\n",
      "\t Val. Loss: 0.01238 |  Val. Acc: 67.62%\n",
      "Epoch: 134 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01117 | Train Acc: 70.17%\n",
      "\t Val. Loss: 0.01229 |  Val. Acc: 68.00%\n",
      "Epoch: 135 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01113 | Train Acc: 70.41%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 67.99%\n",
      "Epoch: 136 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01115 | Train Acc: 70.27%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.40%\n",
      "Epoch: 137 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01112 | Train Acc: 70.40%\n",
      "\t Val. Loss: 0.01250 |  Val. Acc: 67.13%\n",
      "Epoch: 138 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01107 | Train Acc: 70.40%\n",
      "\t Val. Loss: 0.01219 |  Val. Acc: 67.86%\n",
      "Epoch: 139 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01112 | Train Acc: 70.31%\n",
      "\t Val. Loss: 0.01227 |  Val. Acc: 67.72%\n",
      "Epoch: 140 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01107 | Train Acc: 70.42%\n",
      "\t Val. Loss: 0.01262 |  Val. Acc: 66.82%\n",
      "Epoch: 141 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01108 | Train Acc: 70.49%\n",
      "\t Val. Loss: 0.01228 |  Val. Acc: 67.79%\n",
      "Epoch: 142 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01106 | Train Acc: 70.54%\n",
      "\t Val. Loss: 0.01244 |  Val. Acc: 67.48%\n",
      "Epoch: 143 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01106 | Train Acc: 70.46%\n",
      "\t Val. Loss: 0.01227 |  Val. Acc: 67.74%\n",
      "Epoch: 144 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01105 | Train Acc: 70.58%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 68.04%\n",
      "Epoch: 145 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01107 | Train Acc: 70.45%\n",
      "\t Val. Loss: 0.01226 |  Val. Acc: 67.97%\n",
      "Epoch: 146 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01103 | Train Acc: 70.62%\n",
      "\t Val. Loss: 0.01247 |  Val. Acc: 67.06%\n",
      "Epoch: 147 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01102 | Train Acc: 70.57%\n",
      "\t Val. Loss: 0.01221 |  Val. Acc: 68.11%\n",
      "Epoch: 148 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01103 | Train Acc: 70.58%\n",
      "\t Val. Loss: 0.01226 |  Val. Acc: 67.86%\n",
      "Epoch: 149 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01101 | Train Acc: 70.49%\n",
      "\t Val. Loss: 0.01226 |  Val. Acc: 68.01%\n",
      "Epoch: 150 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01103 | Train Acc: 70.63%\n",
      "\t Val. Loss: 0.01387 |  Val. Acc: 63.65%\n",
      "Epoch: 151 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01099 | Train Acc: 70.70%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 67.93%\n",
      "Epoch: 152 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01099 | Train Acc: 70.70%\n",
      "\t Val. Loss: 0.01226 |  Val. Acc: 68.15%\n",
      "Epoch: 153 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01098 | Train Acc: 70.78%\n",
      "\t Val. Loss: 0.01257 |  Val. Acc: 66.94%\n",
      "Epoch: 154 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01101 | Train Acc: 70.71%\n",
      "\t Val. Loss: 0.01215 |  Val. Acc: 68.13%\n",
      "Epoch: 155 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01098 | Train Acc: 70.77%\n",
      "\t Val. Loss: 0.01245 |  Val. Acc: 67.58%\n",
      "Epoch: 156 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01095 | Train Acc: 70.70%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.79%\n",
      "Epoch: 157 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01097 | Train Acc: 70.76%\n",
      "\t Val. Loss: 0.01234 |  Val. Acc: 67.49%\n",
      "Epoch: 158 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01094 | Train Acc: 70.87%\n",
      "\t Val. Loss: 0.01246 |  Val. Acc: 67.49%\n",
      "Epoch: 159 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01095 | Train Acc: 70.82%\n",
      "\t Val. Loss: 0.01223 |  Val. Acc: 68.04%\n",
      "Epoch: 160 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01094 | Train Acc: 70.77%\n",
      "\t Val. Loss: 0.01253 |  Val. Acc: 66.91%\n",
      "Epoch: 161 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01092 | Train Acc: 70.87%\n",
      "\t Val. Loss: 0.01328 |  Val. Acc: 65.76%\n",
      "Epoch: 162 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01089 | Train Acc: 70.95%\n",
      "\t Val. Loss: 0.01222 |  Val. Acc: 68.04%\n",
      "Epoch: 163 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01091 | Train Acc: 70.77%\n",
      "\t Val. Loss: 0.01218 |  Val. Acc: 68.27%\n",
      "Epoch: 164 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01089 | Train Acc: 71.05%\n",
      "\t Val. Loss: 0.01213 |  Val. Acc: 68.06%\n",
      "Epoch: 165 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01088 | Train Acc: 70.90%\n",
      "\t Val. Loss: 0.01223 |  Val. Acc: 67.86%\n",
      "Epoch: 166 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01090 | Train Acc: 70.86%\n",
      "\t Val. Loss: 0.01219 |  Val. Acc: 67.89%\n",
      "Epoch: 167 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01088 | Train Acc: 70.98%\n",
      "\t Val. Loss: 0.01214 |  Val. Acc: 68.17%\n",
      "Epoch: 168 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01088 | Train Acc: 71.02%\n",
      "\t Val. Loss: 0.01235 |  Val. Acc: 67.88%\n",
      "Epoch: 169 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01089 | Train Acc: 70.96%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.87%\n",
      "Epoch: 170 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01084 | Train Acc: 71.10%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 68.36%\n",
      "Epoch: 171 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01084 | Train Acc: 71.06%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 68.13%\n",
      "Epoch: 172 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01083 | Train Acc: 71.04%\n",
      "\t Val. Loss: 0.01235 |  Val. Acc: 67.92%\n",
      "Epoch: 173 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01086 | Train Acc: 71.07%\n",
      "\t Val. Loss: 0.01244 |  Val. Acc: 67.33%\n",
      "Epoch: 174 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01083 | Train Acc: 71.19%\n",
      "\t Val. Loss: 0.01254 |  Val. Acc: 67.33%\n",
      "Epoch: 175 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01081 | Train Acc: 71.10%\n",
      "\t Val. Loss: 0.01208 |  Val. Acc: 68.50%\n",
      "Epoch: 176 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01081 | Train Acc: 71.16%\n",
      "\t Val. Loss: 0.01233 |  Val. Acc: 67.57%\n",
      "Epoch: 177 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01081 | Train Acc: 71.07%\n",
      "\t Val. Loss: 0.01226 |  Val. Acc: 68.01%\n",
      "Epoch: 178 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01080 | Train Acc: 71.13%\n",
      "\t Val. Loss: 0.01211 |  Val. Acc: 68.01%\n",
      "Epoch: 179 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01079 | Train Acc: 71.21%\n",
      "\t Val. Loss: 0.01240 |  Val. Acc: 67.41%\n",
      "Epoch: 180 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01077 | Train Acc: 71.19%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 68.31%\n",
      "Epoch: 181 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01078 | Train Acc: 71.27%\n",
      "\t Val. Loss: 0.01218 |  Val. Acc: 68.35%\n",
      "Epoch: 182 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01078 | Train Acc: 71.19%\n",
      "\t Val. Loss: 0.01234 |  Val. Acc: 67.54%\n",
      "Epoch: 183 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01076 | Train Acc: 71.28%\n",
      "\t Val. Loss: 0.01213 |  Val. Acc: 68.28%\n",
      "Epoch: 184 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01079 | Train Acc: 71.28%\n",
      "\t Val. Loss: 0.01215 |  Val. Acc: 68.36%\n",
      "Epoch: 185 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01078 | Train Acc: 71.17%\n",
      "\t Val. Loss: 0.01219 |  Val. Acc: 67.72%\n",
      "Epoch: 186 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01075 | Train Acc: 71.33%\n",
      "\t Val. Loss: 0.01218 |  Val. Acc: 68.08%\n",
      "Epoch: 187 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01077 | Train Acc: 71.39%\n",
      "\t Val. Loss: 0.01214 |  Val. Acc: 68.40%\n",
      "Epoch: 188 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01072 | Train Acc: 71.51%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 67.90%\n",
      "Epoch: 189 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01073 | Train Acc: 71.36%\n",
      "\t Val. Loss: 0.01232 |  Val. Acc: 68.15%\n",
      "Epoch: 190 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01074 | Train Acc: 71.28%\n",
      "\t Val. Loss: 0.01241 |  Val. Acc: 67.33%\n",
      "Epoch: 191 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01074 | Train Acc: 71.39%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 67.80%\n",
      "Epoch: 192 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01071 | Train Acc: 71.43%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 67.97%\n",
      "Epoch: 193 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01068 | Train Acc: 71.45%\n",
      "\t Val. Loss: 0.01216 |  Val. Acc: 68.13%\n",
      "Epoch: 194 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01069 | Train Acc: 71.56%\n",
      "\t Val. Loss: 0.01222 |  Val. Acc: 68.03%\n",
      "Epoch: 195 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01072 | Train Acc: 71.44%\n",
      "\t Val. Loss: 0.01247 |  Val. Acc: 66.92%\n",
      "Epoch: 196 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01068 | Train Acc: 71.48%\n",
      "\t Val. Loss: 0.01223 |  Val. Acc: 68.11%\n",
      "Epoch: 197 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01068 | Train Acc: 71.47%\n",
      "\t Val. Loss: 0.01261 |  Val. Acc: 67.26%\n",
      "Epoch: 198 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01066 | Train Acc: 71.49%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 68.09%\n",
      "Epoch: 199 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01068 | Train Acc: 71.51%\n",
      "\t Val. Loss: 0.01243 |  Val. Acc: 67.83%\n",
      "Epoch: 200 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01068 | Train Acc: 71.48%\n",
      "\t Val. Loss: 0.01258 |  Val. Acc: 66.91%\n",
      "Epoch: 201 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01065 | Train Acc: 71.63%\n",
      "\t Val. Loss: 0.01222 |  Val. Acc: 68.32%\n",
      "Epoch: 202 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01063 | Train Acc: 71.71%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 68.08%\n",
      "Epoch: 203 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01065 | Train Acc: 71.50%\n",
      "\t Val. Loss: 0.01222 |  Val. Acc: 68.02%\n",
      "Epoch: 204 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01065 | Train Acc: 71.53%\n",
      "\t Val. Loss: 0.01220 |  Val. Acc: 67.95%\n",
      "Epoch: 205 | Epoch Time: 0m 56s\n",
      "\tTrain Loss: 0.01063 | Train Acc: 71.67%\n",
      "\t Val. Loss: 0.01214 |  Val. Acc: 67.98%\n",
      "Epoch: 206 | Epoch Time: 0m 54s\n",
      "\tTrain Loss: 0.01063 | Train Acc: 71.65%\n",
      "\t Val. Loss: 0.01222 |  Val. Acc: 68.15%\n",
      "Epoch: 207 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01062 | Train Acc: 71.59%\n",
      "\t Val. Loss: 0.01213 |  Val. Acc: 68.35%\n",
      "Epoch: 208 | Epoch Time: 0m 55s\n",
      "\tTrain Loss: 0.01062 | Train Acc: 71.74%\n",
      "\t Val. Loss: 0.01217 |  Val. Acc: 68.03%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ecaec7ca698f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-6b206d8692c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, set_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# For splotting\n",
    "all_train_losses = []\n",
    "all_valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, len(train_data))\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), saved_models_path + MODEL_SAVE_FILE)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    all_train_losses.append(train_loss)\n",
    "    all_valid_losses.append(valid_loss)\n",
    "    \n",
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# plt.xticks(range(0, 10))\n",
    "plt.plot(all_train_losses)\n",
    "plt.plot(all_valid_losses)\n",
    "plt.axvline(x=best_epoch, color='green')\n",
    "\n",
    "plt.legend(['train loss', 'valid loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 175\n"
     ]
    }
   ],
   "source": [
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01215 | Test Acc: 67.74%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, len(test_data))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.5f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reevaluate on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01208 | Test Acc: 68.50%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "\n",
    "print(f'Test Loss: {valid_loss:.5f} | Test Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Kaggle Dataset and create submittion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model target file\n",
    "MODEL_SAVE_FILE_TARGET = 'simple-GRU-train-71.10-valid-68.50.pt'\n",
    "\n",
    "def predict_kaggle_test(model, iterator):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            predictions = predictions.argmax(1)\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                result.append([batch.id[0][i].item(), [batch.phrase_id[0][i].item(), predictions[i].item()]] )\n",
    "    \n",
    "    result.sort(key = lambda val: val[0])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [156061, 2]],\n",
       " [1, [156062, 2]],\n",
       " [2, [156063, 2]],\n",
       " [3, [156064, 2]],\n",
       " [4, [156065, 2]],\n",
       " [5, [156066, 3]],\n",
       " [6, [156067, 3]],\n",
       " [7, [156068, 2]],\n",
       " [8, [156069, 3]],\n",
       " [9, [156070, 2]]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE_TARGET))\n",
    "\n",
    "kaggle_result_list = predict_kaggle_test(model, kaggle_test_iterator)\n",
    "\n",
    "kaggle_result_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66282, [222343, 2]],\n",
       " [66283, [222344, 2]],\n",
       " [66284, [222345, 2]],\n",
       " [66285, [222346, 2]],\n",
       " [66286, [222347, 2]],\n",
       " [66287, [222348, 1]],\n",
       " [66288, [222349, 1]],\n",
       " [66289, [222350, 1]],\n",
       " [66290, [222351, 1]],\n",
       " [66291, [222352, 1]]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_result_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156066</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but</td>\n",
       "      <td>68</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, xxeos]</td>\n",
       "      <td>[2, 2606, 1723, 30, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>156067</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing</td>\n",
       "      <td>2</td>\n",
       "      <td>[xxbos, intermittently, pleasing, xxeos]</td>\n",
       "      <td>[2, 2606, 1723, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>156068</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently</td>\n",
       "      <td>65</td>\n",
       "      <td>[xxbos, intermittently, xxeos]</td>\n",
       "      <td>[2, 2606, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156069</td>\n",
       "      <td>8545</td>\n",
       "      <td>pleasing</td>\n",
       "      <td>9</td>\n",
       "      <td>[xxbos, pleasing, xxeos]</td>\n",
       "      <td>[2, 1723, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156070</td>\n",
       "      <td>8545</td>\n",
       "      <td>but</td>\n",
       "      <td>55</td>\n",
       "      <td>[xxbos, but, xxeos]</td>\n",
       "      <td>[2, 30, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "5    156066        8545                        intermittently pleasing but   \n",
       "6    156067        8545                            intermittently pleasing   \n",
       "7    156068        8545                                     intermittently   \n",
       "8    156069        8545                                           pleasing   \n",
       "9    156070        8545                                                but   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "5             68      [xxbos, intermittently, pleasing, but, xxeos]   \n",
       "6              2           [xxbos, intermittently, pleasing, xxeos]   \n",
       "7             65                     [xxbos, intermittently, xxeos]   \n",
       "8              9                           [xxbos, pleasing, xxeos]   \n",
       "9             55                                [xxbos, but, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  \n",
       "5                             [2, 2606, 1723, 30, 3]  \n",
       "6                                 [2, 2606, 1723, 3]  \n",
       "7                                       [2, 2606, 3]  \n",
       "8                                       [2, 1723, 3]  \n",
       "9                                         [2, 30, 3]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_EXTENSION = '.submit.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(saved_models_path + MODEL_SAVE_FILE_TARGET + CSV_EXTENSION, mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    csv_writer.writerow(['PhraseId', 'Sentiment'])\n",
    "    \n",
    "    for i in range(len(kaggle_result_list)):\n",
    "        csv_writer.writerow(kaggle_result_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
