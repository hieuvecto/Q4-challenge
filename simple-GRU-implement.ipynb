{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load config and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import spacy, pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "# Custom impport\n",
    "from common.common_classes import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'prepare-word-embedding-nlp.ipynb', 'tokenization.ipynb', 'test-batching-padding.ipynb', 'train.tsv', 'test-batching-padding-ok.ipynb', 'sampleSubmission.csv', 'save_data', '.ipynb_checkpoints', '__init__.py', 'README.md', '.gitignore', '.git', 'common', 'simple-GRU-implement.ipynb']\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "save_data_path = path + 'save_data/'\n",
    "large_save_data_path = '/notebooks/large-storage/'\n",
    "saved_models_path = '/notebooks/large-storage/saved-models/'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(save_data_path + 'pre-processed-data.pkl', 'rb'))\n",
    "loaded_kaggle_test = pickle.load(open(save_data_path + 'pre-processed-kaggle-test.pkl', 'rb'))\n",
    "loaded_vocab = pickle.load(open(save_data_path + 'genereated-vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, a, series, xxeos]</td>\n",
       "      <td>[2, 10, 341, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, a, xxeos]</td>\n",
       "      <td>[2, 10, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, series, xxeos]</td>\n",
       "      <td>[2, 341, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  Phrase_length  \\\n",
       "0          1            188   \n",
       "1          2             77   \n",
       "2          2              8   \n",
       "3          2              1   \n",
       "4          2              6   \n",
       "\n",
       "                                    Tokenized_phrase  \\\n",
       "0  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "1  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "2                          [xxbos, a, series, xxeos]   \n",
       "3                                  [xxbos, a, xxeos]   \n",
       "4                             [xxbos, series, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "1  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "2                                    [2, 10, 341, 3]  \n",
       "3                                         [2, 10, 3]  \n",
       "4                                        [2, 341, 3]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "MAX_LABEL = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoding and prepraing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(large_save_data_path + 'process-spacy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[BOS].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab.get_vector('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890280, 308)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "PHRASE_ID = data.Field(use_vocab = False)\n",
    "TEXT = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)\n",
    "LABEL = data.LabelField(use_vocab = False, dtype=torch.long)\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "ID_TEST = data.Field(use_vocab = False)\n",
    "PHRASE_ID_TEST = data.Field(use_vocab = False)\n",
    "TEXT_TEST = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "fields = [('id', PHRASE_ID), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "fields_test = [('id', ID_TEST), ('phrase_id', PHRASE_ID_TEST), ('text', TEXT_TEST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_kaggle_test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7fb22610f208>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f240>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f278>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f2b0>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f2e8>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f320>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f358>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f390>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f3c8>,\n",
       " <torchtext.data.example.Example at 0x7fb22610f400>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Train dataset\n",
    "examples = []\n",
    "length = len(loaded_data['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_data['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_data['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_data['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples.append(data.Example.fromlist([ [loaded_data['PhraseId'][i]], embedded, loaded_data['Sentiment'][i]], fields))\n",
    "    \n",
    "examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7fb1cebe5e10>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5e48>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5e80>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5eb8>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5ef0>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5f28>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5f60>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5f98>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebe5fd0>,\n",
       " <torchtext.data.example.Example at 0x7fb1cebf6048>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Test dataset\n",
    "examples_test = []\n",
    "length = len(loaded_kaggle_test['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_kaggle_test['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples_test.append(data.Example.fromlist([ [i], [loaded_kaggle_test['PhraseId'][i]], embedded ], fields_test))\n",
    "    \n",
    "examples_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "        -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "        -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "        -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "        -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "         3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "         3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "        -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "         3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "        -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "        -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "         3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "        -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "        -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "         1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "        -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "        -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "         6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "        -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "         4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "        -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "        -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "         2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "         2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "         1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "         1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "        -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "        -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "        -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "        -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "        -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "         4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "        -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "        -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "         1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "        -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "        -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "         4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "        -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "         1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "         1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "         7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "        -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "         7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "        -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "        -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "        -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "        -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "         8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "        -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "        -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "         3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "        -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "        -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "         3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "        -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "         4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "        -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "        -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "         8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.7793e-01, -6.2470e-02,  1.9596e-01, -4.3340e-02, -1.4570e-01,\n",
       "         6.7455e-02, -7.9048e-01, -4.7327e-01, -1.2694e-02,  9.4032e-01,\n",
       "         9.7596e-01,  3.6356e-01, -4.2334e-01, -4.7300e-01,  4.3313e-02,\n",
       "        -1.7159e-02,  7.7286e-02,  5.1450e-01, -1.3013e-01, -3.8677e-01,\n",
       "         8.5337e-02, -1.1537e-01, -2.9981e-01,  1.0327e-01,  7.4443e-02,\n",
       "         3.8978e-02, -1.0280e-01, -2.7481e-01, -2.9409e-01, -5.2790e-01,\n",
       "        -5.4200e-01, -4.1610e-01, -7.6345e-01, -2.1057e-01, -6.0277e-01,\n",
       "        -3.7840e-01, -5.1185e-01,  5.4210e-02, -8.2146e-02,  5.3257e-01,\n",
       "         3.3352e-01,  1.6082e-01, -1.6897e-02,  3.3638e-01, -1.3660e-01,\n",
       "         4.2157e-01, -2.5176e-01, -3.2271e-01, -3.9742e-01, -3.2712e-01,\n",
       "         3.5235e-01,  5.4523e-01, -5.8145e-01, -2.5093e-01, -4.2526e-01,\n",
       "         4.7148e-04,  5.4579e-01, -4.9980e-01,  2.3473e-02, -1.0801e-01,\n",
       "        -2.5952e-01, -7.5854e-03, -2.0041e-01, -3.7743e-01, -3.0881e-01,\n",
       "        -7.4719e-02,  3.9462e-03, -3.6002e-01, -7.1041e-02,  2.8597e-01,\n",
       "         1.3777e-01,  1.6269e-01, -1.0977e-01, -3.6319e-01,  8.5886e-02,\n",
       "         2.0098e-01,  1.9972e-01,  3.2699e-01, -1.1095e-01, -2.4142e-01,\n",
       "         1.1838e-01, -2.1962e-01, -6.1894e-01, -1.7774e-01, -1.5444e-01,\n",
       "         7.2033e-02,  7.5290e-01,  1.7973e-01,  3.2366e-01,  6.2210e-01,\n",
       "        -1.1795e-01, -2.4438e-01,  2.0690e-01,  4.0958e-01, -3.5867e-02,\n",
       "        -2.7432e-01,  3.3013e-01,  1.8814e-01,  2.6837e-01, -1.8496e-01,\n",
       "         6.3260e-02, -3.3311e-03, -5.4875e-02,  2.3964e-01, -1.4294e-01,\n",
       "        -1.9737e+00, -1.7595e-01, -7.4150e-02,  3.7287e-01,  1.6554e-01,\n",
       "         2.3770e-01, -1.4556e-01,  4.5459e-01, -8.1550e-02, -3.3170e-01,\n",
       "        -8.1371e-01, -1.7414e-01,  1.3691e-01, -6.0061e-01,  4.0306e-01,\n",
       "        -4.3174e-01, -1.7259e-01,  5.2948e-02, -2.4818e-01,  1.5827e-01,\n",
       "        -1.1220e-01,  3.2860e-02,  5.5025e-01,  2.0649e-01,  1.0403e-01,\n",
       "         4.3973e-01, -1.1043e-01, -2.3429e-01,  4.2473e-01, -2.0461e-01,\n",
       "        -2.3912e-01,  2.7573e-01,  6.4947e-01, -3.4708e-01, -1.4163e-01,\n",
       "        -1.7743e-01,  1.7144e-01, -1.7009e-01, -1.2667e-01, -5.6574e-02,\n",
       "         6.9985e-01,  9.3176e-02, -1.1523e-01,  7.9450e-02,  5.3647e-02,\n",
       "        -4.2493e-01,  7.1772e-01,  2.3705e-01,  2.1213e-01, -1.9776e-02,\n",
       "        -1.9379e-02,  2.9703e-01, -2.7536e-01,  7.3583e-02, -5.1954e-01,\n",
       "        -1.5928e-01, -3.5374e-01,  4.5916e-01,  2.0435e-01,  9.7278e-04,\n",
       "         4.6904e-01, -6.2419e-01,  3.2797e-01, -1.5744e-01, -8.8403e-02,\n",
       "         3.5625e-02, -1.2245e-01,  3.6698e-01, -8.8524e-02, -5.5171e-02,\n",
       "         6.0885e-01,  5.3027e-01, -3.3558e-01,  1.5178e-01, -1.6619e-01,\n",
       "         5.0028e-01, -1.4728e-02, -1.4555e-01,  5.6033e-01,  2.4656e-01,\n",
       "        -3.6279e-01, -7.8195e-04,  9.5365e-02,  5.8955e-02, -4.8528e-01,\n",
       "         5.4453e-01, -1.1293e-02, -1.5423e-01,  3.7746e-01, -4.1018e-01,\n",
       "        -7.5968e-02,  3.2843e-01, -1.0195e-01, -6.5580e-01, -5.7961e-01,\n",
       "        -4.3063e-01,  5.0521e-01,  1.4637e-02,  1.8917e-01,  5.0427e-02,\n",
       "        -1.2479e-01, -1.7724e-01, -1.0494e-01,  3.2128e-01,  5.9077e-02,\n",
       "         2.3804e-01, -6.3107e-02,  3.3216e-02, -2.0066e-01, -2.7485e-01,\n",
       "        -2.8084e-01,  4.9871e-01,  2.0036e-01,  2.9303e-01, -3.4742e-01,\n",
       "         4.9861e-01,  2.3172e-01,  1.4505e-01, -7.2846e-02, -8.4557e-02,\n",
       "        -1.5358e-01,  5.1501e-01, -1.7232e-01,  2.7167e-01, -2.3813e-01,\n",
       "         1.3970e-01,  2.5681e-01,  4.1433e-01,  3.3110e-01, -3.7062e-02,\n",
       "        -6.8761e-02,  6.5520e-02, -3.1337e-01, -3.6793e-02, -1.3847e-01,\n",
       "         2.2251e-01,  3.2041e-01,  3.1190e-01,  2.7103e-01, -4.0910e-01,\n",
       "         1.0762e-01,  1.6541e-01,  3.7305e-01,  2.1920e-01, -4.5466e-01,\n",
       "        -2.6570e-01, -4.3871e-01,  2.7044e-01,  2.0953e-01,  3.1834e-01,\n",
       "        -4.3875e-01,  2.4935e-01,  4.6443e-02, -3.2949e-01,  2.1793e-01,\n",
       "         7.3772e-01,  3.9756e-02, -5.0169e-02, -1.8881e-01,  7.7245e-02,\n",
       "         1.4199e-01,  3.9659e-02,  1.2333e-01, -9.6613e-02, -2.2687e-01,\n",
       "         4.8893e-01, -1.1373e-01,  5.1246e-02,  7.5477e-02, -1.2750e-01,\n",
       "        -1.0697e-01, -4.8071e-01, -2.6223e-01,  4.5893e-01,  3.6267e-01,\n",
       "         2.9100e-01,  1.7118e-02, -2.0185e-01, -7.4050e-02, -2.3640e-01,\n",
       "         2.8046e-01,  1.8657e-01,  8.7665e-02,  3.1472e-01, -7.0119e-02,\n",
       "        -1.8172e-01, -3.9074e-02,  2.8063e-02, -4.9746e-02,  1.0355e-01,\n",
       "        -1.5258e-01,  3.4939e-01, -1.6108e-01, -6.0705e-02, -9.0380e-03,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-0.2712   ,  0.14702  , -0.19126  , -0.24424  ,  0.16476  ,\n",
       "        -0.32537  ,  0.36027  ,  0.29804  , -0.49571  ,  1.2244   ,\n",
       "         0.097696 ,  0.30406  , -0.53059  , -0.25869  ,  0.16519  ,\n",
       "        -0.41392  , -0.2314   ,  0.86309  ,  0.35309  ,  0.41004  ,\n",
       "        -0.35114  ,  0.21699  ,  0.30512  , -0.30977  ,  0.065359 ,\n",
       "         0.13193  ,  0.59324  ,  0.089948 , -0.14227  , -0.31061  ,\n",
       "        -0.08432  ,  0.21609  , -0.57332  , -0.076418 , -0.0041399,\n",
       "        -0.48229  , -0.11635  , -0.0089557, -0.0064539, -0.031811 ,\n",
       "         0.16104  , -0.11247  , -0.042646 , -0.026045 ,  0.097288 ,\n",
       "        -0.030434 , -0.45127  , -0.028487 , -0.14567  , -0.10633  ,\n",
       "        -0.17795  , -0.097141 , -0.15773  , -0.42128  ,  0.25669  ,\n",
       "         0.53326  , -0.027647 ,  0.24529  , -0.46835  ,  0.090866 ,\n",
       "         0.12265  , -0.63001  , -0.25682  ,  0.0060041, -0.3454   ,\n",
       "         0.0072695,  0.17178  , -0.39181  , -0.060693 , -0.031147 ,\n",
       "         0.20888  ,  0.30827  ,  0.035736 , -0.23439  ,  0.28267  ,\n",
       "         0.08706  , -0.057023 , -0.30681  , -0.38813  , -0.039169 ,\n",
       "        -0.059306 , -0.36394  ,  0.26486  ,  0.29665  ,  0.12684  ,\n",
       "        -0.059682 ,  0.18076  ,  1.0117   , -0.020038 , -0.16224  ,\n",
       "        -0.069924 ,  0.29389  ,  0.023328 ,  0.67408  ,  0.13449  ,\n",
       "         0.021691 ,  0.21811  , -0.40404  , -0.13521  ,  0.11014  ,\n",
       "         0.19624  ,  0.05419  ,  0.16292  , -0.17251  ,  0.27033  ,\n",
       "        -1.616    ,  0.60791  ,  0.0033667, -0.29159  ,  0.14195  ,\n",
       "        -0.075748 , -0.19854  ,  0.27378  ,  0.087632 , -0.023092 ,\n",
       "        -0.24036  , -0.20799  , -0.062753 ,  0.040184 ,  0.032635 ,\n",
       "        -0.2035   ,  0.15727  , -0.67543  ,  0.67401  , -0.005282 ,\n",
       "         0.067059 ,  0.20846  , -0.65743  ,  0.17864  , -0.27987  ,\n",
       "        -0.022082 , -0.20144  ,  0.13538  , -0.12531  , -0.39399  ,\n",
       "         0.3964   , -0.11625  , -0.34535  ,  0.25273  ,  0.24191  ,\n",
       "        -1.6655   ,  0.61688  , -0.0035128,  0.37053  , -0.32058  ,\n",
       "        -0.48533  ,  0.1568   , -0.086594 ,  0.13272  , -0.050194 ,\n",
       "         0.12197  ,  0.0621   , -0.052821 ,  0.13192  ,  0.26901  ,\n",
       "         0.26882  , -0.47454  , -0.28357  ,  0.16668  , -0.51742  ,\n",
       "        -0.2906   ,  0.25908  ,  0.71942  ,  0.50009  , -0.092692 ,\n",
       "        -0.36577  , -0.034634 ,  0.056318 , -0.21117  ,  0.37486  ,\n",
       "         0.032773 , -0.14359  ,  0.6884   , -0.026806 ,  0.42897  ,\n",
       "         0.053205 , -0.42011  , -0.25344  , -0.21894  ,  0.14449  ,\n",
       "         0.045221 , -0.19444  ,  0.40097  ,  0.42644  , -0.39018  ,\n",
       "        -0.12735  ,  0.11559  ,  0.019183 , -0.027092 , -0.44417  ,\n",
       "         0.045983 ,  0.30359  ,  0.22169  ,  0.060414 , -0.0059974,\n",
       "        -0.66994  , -0.058462 ,  0.16644  , -0.1518   ,  0.0072898,\n",
       "        -0.21948  ,  0.45436  ,  0.05823  , -0.31103  , -0.23067  ,\n",
       "         0.0078952, -0.30799  ,  0.10996  ,  0.28958  , -0.033313 ,\n",
       "        -0.46981  ,  0.17656  , -0.077724 , -0.44687  , -0.13734  ,\n",
       "         0.44273  ,  0.43806  , -0.32096  ,  0.65241  , -0.29144  ,\n",
       "         0.35948  , -0.39277  ,  0.078458 ,  0.6577   ,  0.45683  ,\n",
       "         0.093801 , -0.25023  ,  0.046448 ,  0.04619  , -0.14565  ,\n",
       "         0.17054  ,  0.072038 , -0.86523  ,  0.20702  ,  0.45185  ,\n",
       "         0.20958  ,  0.043511 , -0.17528  , -0.15821  ,  0.011212 ,\n",
       "         0.41577  , -0.31444  , -0.17748  ,  0.089159 , -0.34662  ,\n",
       "        -0.097137 , -0.28784  , -0.38902  ,  0.21508  ,  0.31001  ,\n",
       "         0.030091 ,  0.30658  ,  0.23107  ,  0.048666 ,  0.13822  ,\n",
       "         0.4905   ,  0.66801  , -0.023158 ,  0.38607  ,  0.1293   ,\n",
       "        -0.027527 , -0.31134  ,  0.31469  ,  0.26038  ,  0.3263   ,\n",
       "         0.087236 , -0.014318 , -0.080271 , -0.14507  ,  0.34931  ,\n",
       "        -0.69845  , -0.25372  , -0.28831  , -0.038035 , -0.59529  ,\n",
       "         0.17554  ,  0.78562  , -0.066821 , -0.062887 ,  0.28903  ,\n",
       "         0.82979  ,  0.33097  ,  0.17223  , -0.27315  , -0.5014   ,\n",
       "        -0.29307  ,  0.43277  ,  0.59082  ,  0.059657 ,  0.19892  ,\n",
       "        -0.39105  , -0.020178 , -0.23956  ,  0.26956  ,  0.15734  ,\n",
       "         0.049013 ,  0.2598   , -0.15101  ,  0.069548 ,  0.058311 ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ], dtype=float32),\n",
       " array([-1.6890e-02,  1.7402e-01, -3.0247e-01, -3.0063e-01,  2.1415e-01,\n",
       "         6.3863e-02,  1.0107e-01, -2.4155e-01, -9.5228e-02,  2.9253e+00,\n",
       "        -5.6759e-03,  1.1752e-01,  1.6121e-01,  2.0813e-02, -8.3593e-02,\n",
       "        -1.4048e-01, -4.0069e-02,  7.5133e-01, -5.6699e-02, -9.6198e-02,\n",
       "        -7.2134e-02, -3.1287e-02,  1.8146e-01, -2.4846e-01, -6.5068e-02,\n",
       "         6.6374e-02, -1.2173e-01, -1.3479e-01,  2.6163e-01, -2.1599e-01,\n",
       "        -2.4221e-01,  9.1074e-02, -4.3504e-02, -1.8047e-01, -1.8158e-01,\n",
       "        -6.6229e-02,  2.6480e-02, -2.5439e-01, -7.8805e-02, -2.1853e-01,\n",
       "        -3.0239e-02,  1.3049e-01,  1.0992e-01,  3.5563e-02,  3.2769e-01,\n",
       "        -2.7750e-02, -1.8852e-01, -1.8579e-01, -8.5064e-02, -2.4057e-02,\n",
       "        -1.4141e-01,  1.2708e-01, -7.9024e-02, -1.3320e-01,  3.7619e-02,\n",
       "        -1.1080e-01, -2.2742e-01, -2.7703e-01, -8.1760e-02,  4.7761e-02,\n",
       "        -1.7936e-01, -3.1907e-01,  1.3931e-01,  3.6374e-01, -2.1399e-01,\n",
       "        -2.7044e-01, -2.0847e-01,  1.1192e-02,  1.6017e-01,  2.4218e-01,\n",
       "         2.5058e-01,  3.2767e-01,  1.3340e-01,  6.4510e-03,  1.7994e-02,\n",
       "         1.1242e-01,  9.0136e-02, -7.2882e-02, -1.6506e-01,  4.8300e-01,\n",
       "         4.6465e-03, -4.8611e-02,  2.7421e-02,  9.7357e-02,  1.0873e-01,\n",
       "        -3.1722e-01,  2.6092e-01, -5.9396e-01,  2.6522e-01,  7.2456e-02,\n",
       "        -1.9246e-01,  3.4932e-02, -1.5662e-01,  3.0779e-01,  3.3744e-01,\n",
       "        -4.8664e-03,  1.5165e-01, -1.9861e-02, -1.4789e-01, -1.7031e-02,\n",
       "        -1.7279e-01, -3.8278e-02, -3.2560e-01, -1.6450e-01,  2.2257e-01,\n",
       "        -1.2442e+00, -1.4105e-01,  7.4932e-02, -1.8879e-01, -1.0341e-01,\n",
       "        -3.6646e-03, -2.1667e-01,  3.5087e-02, -1.9168e-01, -1.0044e-01,\n",
       "        -2.8554e-03,  5.6079e-02, -7.4098e-02, -8.3292e-03, -2.3693e-01,\n",
       "         4.1375e-02,  6.9771e-02,  2.0383e-01, -1.8666e-01, -4.2077e-02,\n",
       "         1.3305e-01, -4.5272e-02, -2.9631e-01,  1.6454e-01, -1.1591e-01,\n",
       "         2.4631e-02, -6.1717e-02, -6.2958e-02,  1.3487e-01,  2.0565e-01,\n",
       "         1.4678e-02,  1.9056e-01, -3.2541e-01,  1.0896e-01, -9.1117e-02,\n",
       "        -1.8126e+00,  3.5567e-01,  5.4440e-01, -3.5205e-02, -2.7339e-02,\n",
       "        -1.0750e-01, -3.0940e-01,  1.2539e-01,  8.7296e-02,  8.2909e-02,\n",
       "        -7.9026e-02, -1.8092e-02,  2.6749e-01,  1.1947e-02, -1.3090e-01,\n",
       "        -3.3304e-01, -2.0343e-01, -2.8020e-01, -1.0974e-01, -2.9613e-01,\n",
       "        -1.3973e-01,  1.8934e-01,  3.5228e-02, -2.5916e-01, -2.6859e-01,\n",
       "        -2.2678e-01,  1.3141e-01,  5.1456e-02,  2.6723e-01, -1.8335e-01,\n",
       "        -7.5100e-02, -1.6394e-02, -9.9408e-02, -1.8685e-01, -1.4718e-01,\n",
       "         6.4933e-02, -1.3858e-01, -1.9941e-01, -8.3017e-02, -1.7141e-01,\n",
       "         1.4692e-02, -2.4256e-01, -2.7010e-01, -8.5571e-02, -1.7614e-01,\n",
       "        -1.7786e-01, -1.3418e-02, -1.4634e-01,  3.0529e-01,  6.0719e-02,\n",
       "         2.8651e-01,  1.4169e-01, -1.9963e-01,  1.3507e-01,  7.0590e-02,\n",
       "        -4.2919e-02, -7.0586e-02, -3.5384e-01,  1.3877e-01,  2.4022e-01,\n",
       "        -3.0725e-02,  4.1167e-02, -2.2029e-01, -1.1796e-01,  1.1195e-01,\n",
       "         2.0739e-01, -1.6588e-01, -1.4708e-01,  9.7583e-02,  2.0831e-01,\n",
       "        -1.2357e-01, -7.9109e-02, -7.7406e-02, -3.4475e-01,  2.9539e-01,\n",
       "         1.4243e-02,  7.5820e-02, -1.9915e-02, -1.6280e-01,  1.9031e-02,\n",
       "         2.5743e-01,  1.2232e-01, -8.2438e-02,  6.1687e-02,  2.1967e-01,\n",
       "         2.2370e-01, -9.4008e-02,  1.5609e-01,  1.6797e-03,  1.8566e-01,\n",
       "        -2.6615e-01, -1.0925e-01,  3.6341e-01,  1.0227e-01,  1.6318e-01,\n",
       "        -1.5906e-01,  2.7914e-02, -9.7719e-02,  2.1670e-02,  1.3730e-01,\n",
       "         3.1447e-01,  1.6585e-02, -3.2731e-01,  4.1633e-01,  1.8380e-01,\n",
       "        -2.2986e-01, -1.2125e-01, -1.5239e-01, -1.3330e-02,  1.5810e-01,\n",
       "         3.2326e-01, -7.6780e-02, -1.2008e-01, -1.1215e-01, -5.0701e-02,\n",
       "         1.1711e-01,  1.4279e-01, -1.2161e-01, -1.5066e-02, -7.6299e-02,\n",
       "         2.2693e-01,  1.7511e-01,  2.3596e-02,  1.4218e-01,  2.1250e-01,\n",
       "         2.1344e-01, -6.8560e-02,  6.3065e-02,  5.5161e-01,  2.6941e-01,\n",
       "         1.7143e-01, -1.9773e-01,  3.0509e-02, -3.7473e-01, -2.3374e-01,\n",
       "         1.9453e-01,  2.3319e-03,  5.2309e-03,  1.6025e-01,  5.0800e-01,\n",
       "         3.0588e-01,  2.6994e-02,  1.1541e-01, -7.9795e-02, -9.9845e-02,\n",
       "        -3.1527e-02,  9.3683e-02,  1.0850e-02,  3.8706e-01, -2.6888e-01,\n",
       "        -4.0783e-01, -1.6802e-03,  1.5798e-02,  3.5886e-02,  1.1664e-01,\n",
       "         2.3909e-02, -5.3473e-02, -3.2640e-01,  1.7957e-01,  1.1095e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.1395e-01, -1.8600e-01, -3.1139e-01,  6.9232e-02, -2.2679e-02,\n",
       "         3.5245e-01, -1.7451e-01, -3.4762e-02,  3.6499e-02,  2.4352e+00,\n",
       "         1.5621e-01, -4.1787e-02, -7.2947e-02, -3.1921e-02,  8.9066e-02,\n",
       "         1.9200e-01,  1.6327e-02,  8.0790e-01, -2.3115e-02, -5.3932e-02,\n",
       "        -2.7845e-01,  2.0944e-02, -2.3572e-01,  8.7822e-02,  1.7885e-01,\n",
       "        -1.2684e-01,  1.1533e-01, -1.9993e-01,  1.7179e-01, -2.9116e-01,\n",
       "        -1.3022e-01,  1.4925e-02, -3.4743e-01, -4.2228e-01, -4.9583e-01,\n",
       "         2.5687e-01, -6.4040e-02,  1.4373e-01, -8.7055e-02,  2.3477e-02,\n",
       "         1.1067e-01,  3.4200e-01,  1.0643e-01, -5.5534e-03,  2.1959e-01,\n",
       "         3.2662e-01,  4.2147e-01, -1.1966e-01,  1.5098e-01,  5.5619e-02,\n",
       "        -3.8991e-01,  1.5516e-01, -8.2267e-02, -2.4025e-01, -3.4519e-01,\n",
       "        -1.9722e-01, -3.1244e-01, -5.8754e-01, -2.0353e-01, -7.6791e-03,\n",
       "        -9.1359e-02, -1.7571e-01, -1.7574e-01,  3.2297e-01, -2.6038e-01,\n",
       "        -2.8268e-01, -6.8955e-02,  2.6774e-02, -1.0006e-01,  3.6755e-01,\n",
       "        -1.4354e-01,  3.6205e-01, -1.1899e-01,  1.9502e-02,  6.3868e-01,\n",
       "        -1.4751e-02, -1.6021e-01,  4.8338e-01,  5.8345e-03,  3.2956e-01,\n",
       "         1.2950e-01, -2.2254e-01, -1.9471e-01, -2.5058e-01,  1.0409e-01,\n",
       "        -1.5040e-01, -3.0280e-01,  2.3811e-01,  2.9955e-01, -1.9751e-02,\n",
       "         1.8583e-03, -3.4478e-01,  1.4361e-01, -1.2800e-03,  4.9823e-01,\n",
       "        -6.4337e-02,  4.9903e-01,  5.1760e-01, -1.5414e-01, -3.1006e-01,\n",
       "        -1.6568e-01,  2.8804e-02, -2.9711e-01,  3.8346e-02,  2.3626e-01,\n",
       "        -1.5317e+00,  4.8050e-03, -4.0435e-01,  2.8585e-02,  1.5562e-01,\n",
       "         8.9044e-02, -8.2613e-01, -3.3854e-01, -2.1842e-01, -4.1949e-01,\n",
       "         1.4582e-01, -7.1290e-02, -1.7822e-01, -4.6116e-01,  8.1208e-02,\n",
       "         1.0228e-01, -2.2000e-02,  1.5704e-01, -4.6219e-01,  4.4219e-01,\n",
       "        -1.2331e-02,  6.0651e-01, -4.6436e-02,  7.1817e-02, -2.8741e-01,\n",
       "         8.0092e-02,  9.5995e-02,  2.1868e-01,  9.2623e-02,  5.9085e-02,\n",
       "        -1.5068e-01,  7.5694e-02, -4.2070e-01,  7.7992e-02,  2.3851e-01,\n",
       "        -1.4185e+00, -6.6634e-02,  1.6350e-01,  1.4849e-01, -1.9227e-01,\n",
       "         1.2907e-02, -3.4447e-01, -4.6486e-01,  1.4224e-01,  1.3265e-01,\n",
       "         8.8162e-02,  1.6292e-01,  1.3023e-01, -3.1362e-01, -3.9454e-01,\n",
       "        -1.4404e-01, -2.1622e-01, -2.0923e-01, -3.4271e-01, -1.6063e-01,\n",
       "         7.1870e-02, -1.9801e-01, -5.0524e-02,  7.4052e-05, -4.3107e-01,\n",
       "        -8.2001e-02, -3.8524e-01, -4.9691e-02,  1.0136e-01, -4.7003e-01,\n",
       "         1.1299e-01,  2.6024e-01, -1.6097e-01,  8.5316e-02, -2.4209e-01,\n",
       "         4.3188e-02, -2.8629e-01, -2.3318e-01,  1.9134e-01, -3.7870e-01,\n",
       "         1.9383e-01, -1.3177e-01, -4.2849e-01, -2.7833e-01, -6.3169e-02,\n",
       "        -2.3149e-01,  3.5011e-02, -2.3458e-01,  5.1698e-01,  8.0902e-02,\n",
       "         5.3812e-01, -1.0787e-01, -2.1465e-01, -3.5011e-01, -1.1482e-01,\n",
       "        -3.7470e-01, -1.4581e-01, -1.5781e-01, -8.8941e-02, -2.5458e-01,\n",
       "        -2.2646e-01, -1.4723e-01,  1.9318e-01,  1.5410e-01,  3.5199e-02,\n",
       "        -1.0698e-01, -4.4188e-02, -4.5023e-01,  2.6755e-01,  3.9788e-02,\n",
       "         1.6353e-01, -4.0591e-02,  1.4291e-01, -2.7215e-01,  2.4453e-01,\n",
       "         1.0331e-02,  2.1538e-01, -1.1237e-01, -3.7174e-02, -1.1926e-01,\n",
       "         2.9896e-01,  5.2738e-02, -8.8533e-02,  4.3619e-02, -1.0675e-01,\n",
       "         1.6294e-01,  2.5775e-01, -3.6536e-02,  9.5472e-02,  2.0618e-01,\n",
       "        -1.4716e-01, -2.3276e-01,  2.8959e-01,  3.0868e-02,  2.7450e-01,\n",
       "        -2.0971e-02,  2.8721e-01, -2.1334e-01,  4.2814e-01,  3.2721e-01,\n",
       "        -1.8220e-01,  2.6856e-01, -2.6014e-01,  2.3042e-01, -3.0705e-01,\n",
       "         2.9832e-01, -4.2529e-01, -3.5242e-01,  2.3699e-01, -5.1793e-03,\n",
       "         1.9576e-01,  1.3232e-01,  1.8231e-01,  9.8129e-02,  2.3562e-01,\n",
       "        -6.8167e-02,  1.2068e-01,  1.9259e-01, -1.3113e-01,  1.4483e-01,\n",
       "         3.3028e-01,  1.0524e-01, -1.6241e-01,  1.8550e-01, -5.3597e-02,\n",
       "         1.2893e-01,  1.0969e-01, -3.3200e-01,  1.4515e-01,  2.8574e-01,\n",
       "        -2.3060e-01, -2.6947e-01, -4.2454e-01, -9.0470e-01, -9.2193e-02,\n",
       "        -1.4784e-01,  5.1422e-02, -2.2871e-01,  1.0521e-01,  4.7294e-01,\n",
       "         2.0239e-01, -1.6172e-02, -2.8370e-02, -1.0857e-01,  2.3019e-01,\n",
       "         1.2928e-01, -3.9928e-01, -5.9975e-02, -1.4594e-01, -5.7901e-02,\n",
       "        -7.0202e-01, -1.8404e-01, -8.4864e-02,  4.5815e-02, -2.0621e-01,\n",
       "         4.5970e-01, -9.3192e-02, -6.1150e-01, -7.5240e-02, -1.2648e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7386e-02,  2.1880e-01,  1.7469e-01,  2.7509e-01, -2.1631e-01,\n",
       "         3.5347e-01, -2.2180e-02, -2.8373e-01,  2.5881e-01,  1.9462e+00,\n",
       "        -4.0591e-02, -1.7405e-01,  1.3791e-01,  5.1707e-01, -1.1379e-01,\n",
       "         4.3605e-01,  4.4465e-01,  1.5117e+00, -3.5965e-01,  4.3858e-02,\n",
       "        -5.1923e-02,  2.9570e-01, -7.3748e-01, -5.4741e-03, -1.7881e-01,\n",
       "        -2.2322e-01,  6.4444e-01, -5.1164e-01,  9.3286e-02, -3.9014e-01,\n",
       "        -2.3993e-01, -3.4526e-01,  6.7683e-03, -4.2794e-01,  1.0783e-01,\n",
       "         4.2653e-01,  3.0891e-01, -1.0818e-01,  1.8644e-01, -2.1826e-01,\n",
       "        -4.1482e-01,  1.0898e-02, -1.7486e-01,  3.9860e-01, -2.0510e-01,\n",
       "         4.8380e-01, -5.0375e-02,  8.0554e-01, -9.0117e-02,  9.7864e-02,\n",
       "         1.5842e-01, -3.6625e-01,  3.8516e-01, -3.7493e-02, -3.4814e-01,\n",
       "         9.3430e-02,  9.1870e-01, -2.3090e-01,  3.5374e-01,  6.5830e-02,\n",
       "         1.1644e-01,  5.0977e-01, -1.5380e-01, -1.8577e-01,  4.4437e-01,\n",
       "         2.7601e-01,  2.2735e-01,  5.3475e-01, -1.8252e-01, -7.4210e-02,\n",
       "         2.1165e-01, -5.0073e-01,  1.2662e-01, -1.2315e-01,  3.8191e-01,\n",
       "        -1.6075e-01,  5.3988e-02, -1.9419e-01,  1.7041e-02, -2.8735e-01,\n",
       "         4.7740e-01, -6.2051e-01, -3.0525e-01, -3.2444e-01, -8.1936e-02,\n",
       "         2.4708e-01,  1.6119e-01,  1.4163e-01, -2.6534e-02,  6.8003e-01,\n",
       "         3.8574e-01,  7.9060e-01,  1.7182e-01,  1.9055e-01, -4.0842e-01,\n",
       "        -7.1388e-01, -2.4055e-01,  3.0572e-01,  3.2042e-01,  9.4508e-02,\n",
       "         1.9716e-01, -3.2531e-01, -4.6567e-02,  4.9304e-03,  2.3895e-01,\n",
       "        -1.5985e+00,  4.4618e-01,  6.7059e-02,  4.2562e-02, -9.4264e-02,\n",
       "         2.6359e-01,  5.9031e-02,  2.1923e-01,  1.1912e-01, -4.8703e-01,\n",
       "        -1.6320e-01,  1.3074e-01,  1.3469e-01, -8.6061e-04,  6.5107e-02,\n",
       "         2.6348e-01, -6.8230e-02, -3.4943e-01,  1.4124e-03, -3.2127e-01,\n",
       "        -3.2119e-01,  2.4275e-01,  1.4489e-01,  5.2155e-01,  1.8236e-02,\n",
       "         3.8936e-01,  1.7392e-01, -1.8137e-01,  2.3953e-01,  7.9878e-02,\n",
       "         2.3554e-01,  3.4491e-01,  1.3673e-01, -2.4935e-02,  1.2661e-01,\n",
       "        -9.4117e-01, -2.7312e-01, -3.9914e-02, -2.1242e-02, -2.4627e-01,\n",
       "         1.8920e-01, -3.7878e-02, -7.5084e-01,  3.1216e-01, -8.1300e-02,\n",
       "        -1.4081e-01,  1.9424e-01,  5.0618e-01,  5.5668e-02, -2.2838e-01,\n",
       "        -7.9835e-02,  3.3727e-01, -3.9557e-01, -3.1844e-01, -6.1797e-01,\n",
       "        -4.3276e-01, -1.0177e-02, -1.6727e-01, -1.4701e-01, -1.3138e-03,\n",
       "        -3.6874e-01,  8.8321e-03,  1.0844e-01,  2.2943e-01, -5.6433e-01,\n",
       "         3.1256e-01, -1.9476e-01, -7.2129e-02, -8.9461e-01, -5.7574e-01,\n",
       "        -9.0824e-02, -6.1415e-01,  1.7660e-01, -2.6355e-01,  6.2693e-01,\n",
       "        -4.5756e-01, -8.6851e-01, -2.4981e-01,  5.5472e-01, -8.5896e-02,\n",
       "         1.1540e-01,  3.1408e-01,  3.9062e-01, -3.4876e-01,  5.4840e-02,\n",
       "        -2.2244e-01,  5.9099e-02,  9.9086e-02, -5.1477e-01, -1.7693e-01,\n",
       "         1.3470e-01, -2.5853e-01, -4.6366e-01,  2.2667e-01,  2.6040e-01,\n",
       "        -3.4442e-01,  2.5386e-01,  3.2574e-02,  3.5130e-01, -6.8617e-01,\n",
       "        -1.3479e-01, -1.5026e-01,  1.6556e-01,  3.2562e-01,  1.3296e-01,\n",
       "         2.2568e-01,  2.6165e-01,  9.7049e-03,  4.1276e-02, -3.9056e-01,\n",
       "        -2.5348e-01,  1.1446e-01, -2.1078e-01,  3.7831e-02, -4.9510e-02,\n",
       "         3.3036e-01,  2.0437e-01, -5.9281e-01,  1.0520e-01, -4.3911e-01,\n",
       "         5.1661e-01, -1.9186e-01,  6.2483e-01,  2.6081e-02, -1.2373e-01,\n",
       "        -2.3343e-01,  7.1194e-01,  3.1010e-01,  1.5132e-01, -2.8571e-01,\n",
       "         2.9020e-02, -6.9205e-02, -2.8972e-01,  1.1664e-03,  9.4657e-03,\n",
       "        -3.1325e-01, -3.6959e-01,  6.3799e-01,  4.5772e-03, -4.4835e-02,\n",
       "        -1.0021e-01,  2.0841e-01, -8.3103e-02,  3.5557e-02,  5.7080e-01,\n",
       "         1.3201e-02,  6.1434e-02,  7.9083e-02, -5.3205e-02,  1.2540e-01,\n",
       "         1.2251e-01,  2.2707e-01, -6.0252e-02, -1.3078e-01,  1.8732e-01,\n",
       "        -2.4053e-01,  3.9825e-01,  9.7576e-02, -6.0194e-01, -9.7147e-02,\n",
       "         5.0851e-01,  9.4998e-02,  2.9620e-01,  6.9577e-01,  2.7756e-03,\n",
       "         6.0900e-02, -2.9739e-01, -2.0041e-01, -3.3111e-01,  3.2232e-01,\n",
       "         2.9970e-01,  7.8138e-02, -2.1120e-01,  2.4587e-01,  7.2880e-01,\n",
       "        -5.8055e-02, -3.1365e-02,  5.0065e-01, -4.5560e-03, -2.1692e-01,\n",
       "        -1.8249e-01, -3.7010e-01,  1.9931e-01,  3.1825e-02, -1.2789e-01,\n",
       "         5.8350e-01,  4.7768e-01,  3.3556e-01,  1.1028e-01,  1.5622e-01,\n",
       "        -7.6022e-02,  2.5118e-01,  5.3027e-02, -7.8588e-02, -4.7783e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7675e-01, -1.1799e-01,  1.7866e-01,  3.2482e-02, -4.9160e-01,\n",
       "         4.4509e-02, -3.8697e-01,  3.4402e-02, -4.0194e-02,  3.1124e+00,\n",
       "        -2.2361e-01,  1.1824e-01,  6.9428e-02, -3.2642e-02,  1.4955e-01,\n",
       "         1.5286e-01,  3.2991e-02,  6.3527e-01, -2.3576e-01,  2.4908e-01,\n",
       "         2.0384e-01,  1.4182e-01,  2.0118e-01, -2.0678e-01, -1.6844e-01,\n",
       "         1.3097e-01,  4.8774e-01, -3.5005e-01, -7.5147e-02, -1.5022e-01,\n",
       "         1.6138e-01, -5.5018e-01, -1.9398e-01, -2.8315e-02,  1.6917e-01,\n",
       "        -4.0531e-01,  6.5695e-02, -1.3529e-01,  1.7965e-01,  7.5198e-02,\n",
       "         1.2148e-01,  6.1736e-01,  3.6658e-02, -5.6777e-01,  3.2880e-01,\n",
       "        -2.4745e-01,  1.3974e-01,  7.4644e-01, -1.9929e-01,  3.6678e-01,\n",
       "         2.9422e-01, -8.5806e-02,  1.2210e-01,  1.6887e-01, -2.1271e-01,\n",
       "         7.3281e-03, -7.6509e-02,  1.7360e-01,  3.0140e-01, -6.2679e-02,\n",
       "        -2.4980e-01, -6.2602e-02, -2.8363e-01,  1.1710e-01,  1.4115e-01,\n",
       "        -5.0121e-01,  1.2329e-01, -1.8731e-01,  3.9721e-01, -2.5896e-02,\n",
       "         2.9596e-01, -3.2547e-02,  1.7333e-01, -1.1771e-01,  1.2088e-01,\n",
       "        -3.5890e-01, -1.3854e-01, -8.9703e-02,  3.1391e-01, -9.1165e-02,\n",
       "        -1.3064e-01, -2.2216e-02,  1.5877e-02,  2.6226e-01, -2.7216e-01,\n",
       "        -1.5718e-01, -8.3953e-01, -1.4901e-01,  1.2313e-01,  3.2840e-01,\n",
       "        -1.1891e-01,  5.5510e-02,  1.8767e-01,  5.3449e-01, -2.0262e-01,\n",
       "         1.6928e-01, -1.9827e-01, -2.4103e-01, -1.8439e-02, -9.7009e-02,\n",
       "        -7.1952e-02, -3.6047e-01, -9.6811e-02, -2.9746e-01, -2.7191e-02,\n",
       "        -1.4366e+00,  4.3149e-01,  1.3098e-01, -7.2661e-02,  1.7736e-01,\n",
       "         1.6811e-01,  1.7400e-02,  1.7484e-01, -2.2591e-01,  4.0063e-01,\n",
       "         1.2184e-01,  6.9323e-02, -1.1818e-01,  3.1484e-01, -1.0699e-01,\n",
       "         4.0806e-01, -1.2300e-01, -3.1155e-01,  2.1126e-01, -5.7120e-02,\n",
       "         8.6237e-02, -4.4460e-01,  2.2277e-04,  2.4001e-02,  3.0837e-01,\n",
       "        -1.6179e-01,  2.5340e-01, -4.2660e-02,  2.5280e-03,  9.9567e-02,\n",
       "        -5.1727e-02,  3.5747e-02, -1.7966e-01,  3.3886e-01, -1.6398e-01,\n",
       "        -7.8192e-01,  6.0500e-01,  2.8252e-01, -1.2994e-01, -1.6583e-02,\n",
       "        -3.4698e-01, -7.2527e-01,  8.0799e-02,  9.4704e-02,  3.6744e-01,\n",
       "        -8.9103e-02,  6.9915e-02,  1.6852e-01, -1.2281e-01, -2.8876e-02,\n",
       "        -7.7054e-02, -1.8588e-01, -1.8323e-01, -1.9980e-01,  4.5056e-02,\n",
       "        -7.6702e-02,  3.8135e-01, -3.8801e-01, -2.1650e-01, -3.0306e-02,\n",
       "         1.6275e-01, -1.0535e-02, -1.6363e-01,  7.2000e-04, -2.2225e-02,\n",
       "        -7.1306e-02, -2.7842e-02,  2.6646e-01, -2.7049e-01, -2.1350e-02,\n",
       "        -4.4271e-02, -1.2112e-01,  1.1946e-01, -1.1000e-02,  3.2661e-01,\n",
       "        -1.8398e-01,  7.4138e-02, -4.2220e-02,  9.7410e-02, -1.4307e-01,\n",
       "         1.0406e-02,  2.1065e-02,  1.9372e-01, -1.3614e-01,  1.1002e-01,\n",
       "         1.5361e-01, -5.0540e-02,  8.6166e-02,  2.4223e-01,  1.4647e-01,\n",
       "        -1.3835e-01, -6.9210e-02, -1.4987e-01,  1.8680e-01,  3.9816e-01,\n",
       "         5.7365e-01,  1.5033e-02,  4.5188e-02,  1.6298e-01,  9.3265e-02,\n",
       "        -7.3806e-02, -3.6836e-01, -1.0767e-01,  4.6960e-01, -1.3725e-01,\n",
       "         1.1711e-01,  2.3232e-01,  2.1569e-02,  8.2387e-03,  2.0852e-01,\n",
       "         1.8448e-01,  2.6981e-01, -7.7155e-02, -1.5074e-01, -1.4044e-01,\n",
       "         1.5819e-01, -3.1127e-01, -2.3062e-01,  1.6080e-01,  1.3612e-01,\n",
       "        -6.8917e-02,  2.7529e-01, -2.0376e-01, -4.3880e-02,  3.3157e-01,\n",
       "         1.8735e-01,  5.5636e-02,  1.2360e-01, -1.1264e-01,  1.3654e-01,\n",
       "         3.6212e-01, -2.0679e-01,  2.2256e-02,  8.0054e-02, -5.1004e-02,\n",
       "         1.5415e-01, -4.8844e-01, -1.7730e-01,  4.1286e-01, -2.8209e-01,\n",
       "        -2.8011e-01, -1.5195e-01, -3.5349e-01,  3.4257e-01,  1.2766e-01,\n",
       "        -4.7474e-02,  3.2728e-01, -6.9187e-01,  1.7493e-01,  8.9532e-02,\n",
       "        -6.1105e-02, -6.9829e-02, -1.5193e-01,  5.7987e-02,  3.0111e-02,\n",
       "        -3.9252e-02,  5.3934e-02,  4.5133e-01,  4.2870e-02, -2.2657e-01,\n",
       "        -3.5830e-01, -5.6488e-02,  1.0440e-01,  4.9360e-01,  2.7961e-01,\n",
       "         5.4662e-01, -1.3974e-01, -2.1185e-01, -4.5541e-01, -6.9248e-02,\n",
       "        -3.5238e-01,  2.2941e-01, -2.4661e-01,  1.6707e-01,  2.9528e-01,\n",
       "        -1.2137e-01, -2.0418e-01,  2.4620e-01,  3.4143e-01, -2.9176e-01,\n",
       "        -5.7014e-01, -2.6190e-01, -3.2669e-01,  3.1928e-01, -3.0172e-01,\n",
       "        -1.3678e-01,  4.1218e-01,  1.1811e-01,  2.8056e-01,  4.2607e-02,\n",
       "        -4.6627e-01,  2.9769e-01, -4.3038e-02, -5.6063e-01,  2.5515e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_test[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "       -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "       -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "       -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "       -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "        3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "        3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "       -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "        3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "       -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "       -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "        3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "       -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "       -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "        1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "       -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "       -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "        6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "       -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "        4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "       -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "       -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "        2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "        2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "        1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "        1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "       -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "       -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "       -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "       -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "       -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "        4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "       -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "       -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "        1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "       -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "       -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "        4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "       -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "        1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "        1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "        7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "       -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "        7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "       -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "       -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "       -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "       -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "        8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "       -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "       -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "        3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "       -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "       -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "        3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "       -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "        4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "       -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "       -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "        8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.Dataset(examples, fields)\n",
    "data_set_test = data.Dataset(examples_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.sort_key = lambda x: len(x.text)\n",
    "data_set_test.sort_key = lambda x: len(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data_set.split([0.8, 0.1, 0.1], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124848"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(train_data.sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set_test.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(data_set_test.sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)\n",
    "\n",
    "kaggle_test_iterator = data.BucketIterator(\n",
    "    data_set_test, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_iterator = iter(train_iterator)\n",
    "batch = next(raw_train_iterator)\n",
    "\n",
    "raw_kaggle_test_iterator = iter(kaggle_test_iterator)\n",
    "batch_test = next(raw_kaggle_test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = batch.text\n",
    "\n",
    "c, d = batch_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 308])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 1, 3, 3, 1, 3, 3, 2, 2, 2, 2, 2, 0, 2, 1, 4, 3, 2, 3, 2,\n",
       "        2, 2, 1, 3, 1, 2, 2, 1, 3, 1, 2, 2, 1, 4, 2, 3, 2, 3, 2, 2, 1, 4, 2, 2,\n",
       "        2, 3, 2, 3, 1, 2, 2, 3, 4, 2, 2, 4, 2, 2, 1, 4], device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133918,  57783,  94049,  86671,  84086,  74128, 112683, 126943,  12599,\n",
       "         138419,  47937,  62635, 104584,  86587, 113162,  81036,  62472, 147974,\n",
       "          32209, 133654,  95241, 137072,  66415,  75555,  53039, 130485,  79837,\n",
       "         152257,  39951, 136680, 121126, 102615, 101662, 119728, 108929, 145776,\n",
       "          36689,  66928, 109732, 109843, 130506,  33026, 123268, 155495, 114095,\n",
       "          27453, 122599,  82520,  21535, 104973,  91096,  97182,  84542, 119081,\n",
       "         104435,  65401,  30863, 141808,  39997, 145932,  46149, 125167, 132902,\n",
       "         117864]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data['Sentiment'][117864-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10.], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 308])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.phrase_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId                                                       193462\n",
       "SentenceId                                                      10289\n",
       "Phrase                         you 're not into the Pokemon franchise\n",
       "Phrase_length                                                      72\n",
       "Tokenized_phrase    [xxbos, you, 're, not, into, the, xxmaj, pokem...\n",
       "Indexed_phrase               [2, 33, 157, 41, 61, 8, 7, 2893, 978, 3]\n",
       "Name: 37401, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.iloc[37401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "        -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "         1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "        -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "         6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "        -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "        -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "         8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "        -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "         1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "         5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "        -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "         1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "        -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "         2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "        -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "         3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "        -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "        -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "         1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "        -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "        -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "        -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "         7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "         6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "        -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "        -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "        -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "        -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "        -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "         4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "         3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "        -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "        -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "         7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "        -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "         8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "         8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "         3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "         1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "         1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "        -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "         2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "         4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "        -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "        -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "        -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "         2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "         2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "         1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "        -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "         1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "         3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "         1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "        -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "        -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "         1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "        -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "        -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "         1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[63][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "       -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "        1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "       -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "        6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "       -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "       -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "        8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "       -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "        1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "        5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "       -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "        1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "       -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "        2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "       -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "        3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "       -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "       -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "        1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "       -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "       -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "       -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "        7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "        6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "       -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "       -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "       -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "       -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "       -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "        4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "        3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "       -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "       -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "        7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "       -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "        8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "        8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "        3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "        1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "        1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "       -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "        2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "        4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "       -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "       -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "       -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "        2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "        2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "        1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "       -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "        1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "        3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "        1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "       -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "       -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "        1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "       -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "       -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "        1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector(\"'re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU( embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(text, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        #if self.bidirectional:\n",
    "        #    hidden = [batch size, hid dim * num directions]\n",
    "        #else:\n",
    "        #    hidden = [batch size, hid dim]\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 308\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "N_EPOCHS = 60\n",
    "\n",
    "MODEL_SAVE_FILE = 'simple-GRU_origin.pt'\n",
    "model = GRU(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,054,661 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define epoch time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.01436 | Train Acc: 61.77%\n",
      "\t Val. Loss: 0.01366 |  Val. Acc: 63.67%\n",
      "Epoch: 02 | Epoch Time: 1m 11s\n",
      "\tTrain Loss: 0.01309 | Train Acc: 64.95%\n",
      "\t Val. Loss: 0.01271 |  Val. Acc: 66.22%\n",
      "Epoch: 03 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.01238 | Train Acc: 66.95%\n",
      "\t Val. Loss: 0.01230 |  Val. Acc: 67.12%\n",
      "Epoch: 04 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.01178 | Train Acc: 68.65%\n",
      "\t Val. Loss: 0.01212 |  Val. Acc: 67.92%\n",
      "Epoch: 05 | Epoch Time: 1m 10s\n",
      "\tTrain Loss: 0.01124 | Train Acc: 70.03%\n",
      "\t Val. Loss: 0.01187 |  Val. Acc: 68.60%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, len(train_data))\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), saved_models_path + MODEL_SAVE_FILE)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.012 | Test Acc: 68.49%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, len(test_data))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Kaggle Dataset and create submittion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model target file\n",
    "MODEL_SAVE_FILE_TARGET = 'simple-GRU-valid-69.59.pt'\n",
    "\n",
    "def predict_kaggle_test(model, iterator):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            predictions = predictions.argmax(1)\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                result.append([batch.id[0][i].item(), [batch.phrase_id[0][i].item(), predictions[i].item()]] )\n",
    "    \n",
    "    result.sort(key = lambda val: val[0])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [156061, 2]],\n",
       " [1, [156062, 2]],\n",
       " [2, [156063, 2]],\n",
       " [3, [156064, 2]],\n",
       " [4, [156065, 2]],\n",
       " [5, [156066, 2]],\n",
       " [6, [156067, 3]],\n",
       " [7, [156068, 2]],\n",
       " [8, [156069, 3]],\n",
       " [9, [156070, 2]]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE_TARGET))\n",
    "\n",
    "kaggle_result_list = predict_kaggle_test(model, kaggle_test_iterator)\n",
    "\n",
    "kaggle_result_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66282, [222343, 2]],\n",
       " [66283, [222344, 2]],\n",
       " [66284, [222345, 2]],\n",
       " [66285, [222346, 2]],\n",
       " [66286, [222347, 2]],\n",
       " [66287, [222348, 0]],\n",
       " [66288, [222349, 0]],\n",
       " [66289, [222350, 1]],\n",
       " [66290, [222351, 1]],\n",
       " [66291, [222352, 1]]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_result_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66282</th>\n",
       "      <td>222343</td>\n",
       "      <td>11854</td>\n",
       "      <td>should have called it Gutterball</td>\n",
       "      <td>14</td>\n",
       "      <td>[xxbos, should, have, called, it, xxmaj, gutte...</td>\n",
       "      <td>[2, 156, 49, 775, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66283</th>\n",
       "      <td>222344</td>\n",
       "      <td>11854</td>\n",
       "      <td>have called it Gutterball</td>\n",
       "      <td>65</td>\n",
       "      <td>[xxbos, have, called, it, xxmaj, gutterball, x...</td>\n",
       "      <td>[2, 49, 775, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66284</th>\n",
       "      <td>222345</td>\n",
       "      <td>11854</td>\n",
       "      <td>called it Gutterball</td>\n",
       "      <td>63</td>\n",
       "      <td>[xxbos, called, it, xxmaj, gutterball, xxeos]</td>\n",
       "      <td>[2, 775, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66285</th>\n",
       "      <td>222346</td>\n",
       "      <td>11854</td>\n",
       "      <td>it Gutterball</td>\n",
       "      <td>38</td>\n",
       "      <td>[xxbos, it, xxmaj, gutterball, xxeos]</td>\n",
       "      <td>[2, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66286</th>\n",
       "      <td>222347</td>\n",
       "      <td>11854</td>\n",
       "      <td>Gutterball</td>\n",
       "      <td>34</td>\n",
       "      <td>[xxbos, xxmaj, gutterball, xxeos]</td>\n",
       "      <td>[2, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66287</th>\n",
       "      <td>222348</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario .</td>\n",
       "      <td>32</td>\n",
       "      <td>[xxbos, a, long, -, winded, ,, predictable, sc...</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 9, 390, 2021, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66288</th>\n",
       "      <td>222349</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario</td>\n",
       "      <td>29</td>\n",
       "      <td>[xxbos, a, long, -, winded, ,, predictable, sc...</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 9, 390, 2021, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66289</th>\n",
       "      <td>222350</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded ,</td>\n",
       "      <td>20</td>\n",
       "      <td>[xxbos, a, long, -, winded, ,, xxeos]</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 9, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66290</th>\n",
       "      <td>222351</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded</td>\n",
       "      <td>24</td>\n",
       "      <td>[xxbos, a, long, -, winded, xxeos]</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66291</th>\n",
       "      <td>222352</td>\n",
       "      <td>11855</td>\n",
       "      <td>predictable scenario</td>\n",
       "      <td>21</td>\n",
       "      <td>[xxbos, predictable, scenario, xxeos]</td>\n",
       "      <td>[2, 390, 2021, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId                                  Phrase  \\\n",
       "66282    222343       11854        should have called it Gutterball   \n",
       "66283    222344       11854               have called it Gutterball   \n",
       "66284    222345       11854                    called it Gutterball   \n",
       "66285    222346       11854                           it Gutterball   \n",
       "66286    222347       11854                              Gutterball   \n",
       "66287    222348       11855  A long-winded , predictable scenario .   \n",
       "66288    222349       11855    A long-winded , predictable scenario   \n",
       "66289    222350       11855                         A long-winded ,   \n",
       "66290    222351       11855                           A long-winded   \n",
       "66291    222352       11855                    predictable scenario   \n",
       "\n",
       "       Phrase_length                                   Tokenized_phrase  \\\n",
       "66282             14  [xxbos, should, have, called, it, xxmaj, gutte...   \n",
       "66283             65  [xxbos, have, called, it, xxmaj, gutterball, x...   \n",
       "66284             63      [xxbos, called, it, xxmaj, gutterball, xxeos]   \n",
       "66285             38              [xxbos, it, xxmaj, gutterball, xxeos]   \n",
       "66286             34                  [xxbos, xxmaj, gutterball, xxeos]   \n",
       "66287             32  [xxbos, a, long, -, winded, ,, predictable, sc...   \n",
       "66288             29  [xxbos, a, long, -, winded, ,, predictable, sc...   \n",
       "66289             20              [xxbos, a, long, -, winded, ,, xxeos]   \n",
       "66290             24                 [xxbos, a, long, -, winded, xxeos]   \n",
       "66291             21              [xxbos, predictable, scenario, xxeos]   \n",
       "\n",
       "                                    Indexed_phrase  \n",
       "66282               [2, 156, 49, 775, 20, 7, 0, 3]  \n",
       "66283                    [2, 49, 775, 20, 7, 0, 3]  \n",
       "66284                        [2, 775, 20, 7, 0, 3]  \n",
       "66285                             [2, 20, 7, 0, 3]  \n",
       "66286                                 [2, 7, 0, 3]  \n",
       "66287  [2, 10, 139, 13, 5931, 9, 390, 2021, 15, 3]  \n",
       "66288      [2, 10, 139, 13, 5931, 9, 390, 2021, 3]  \n",
       "66289                 [2, 10, 139, 13, 5931, 9, 3]  \n",
       "66290                    [2, 10, 139, 13, 5931, 3]  \n",
       "66291                            [2, 390, 2021, 3]  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_EXTENSION = '.submit.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(saved_models_path + MODEL_SAVE_FILE_TARGET + CSV_EXTENSION, mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for i in range(len(kaggle_result_list)):\n",
    "        csv_writer.writerow(kaggle_result_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
