{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load config and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import spacy, pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "# Custom impport\n",
    "from common.common_classes import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'prepare-word-embedding-nlp.ipynb', 'tokenization.ipynb', 'test-batching-padding.ipynb', 'train.tsv', 'test-batching-padding-ok.ipynb', 'sampleSubmission.csv', 'save_data', '.ipynb_checkpoints', '__init__.py', 'README.md', '.gitignore', '.git', 'common', 'simple-GRU-implement.ipynb']\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "save_data_path = path + 'save_data/'\n",
    "large_save_data_path = '/notebooks/large-storage/'\n",
    "saved_models_path = '/notebooks/large-storage/saved-models/'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(save_data_path + 'pre-processed-data.pkl', 'rb'))\n",
    "loaded_kaggle_test = pickle.load(open(save_data_path + 'pre-processed-kaggle-test.pkl', 'rb'))\n",
    "loaded_vocab = pickle.load(open(save_data_path + 'genereated-vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, a, series, xxeos]</td>\n",
       "      <td>[2, 10, 341, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, a, xxeos]</td>\n",
       "      <td>[2, 10, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, series, xxeos]</td>\n",
       "      <td>[2, 341, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  Phrase_length  \\\n",
       "0          1            188   \n",
       "1          2             77   \n",
       "2          2              8   \n",
       "3          2              1   \n",
       "4          2              6   \n",
       "\n",
       "                                    Tokenized_phrase  \\\n",
       "0  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "1  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "2                          [xxbos, a, series, xxeos]   \n",
       "3                                  [xxbos, a, xxeos]   \n",
       "4                             [xxbos, series, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "1  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "2                                    [2, 10, 341, 3]  \n",
       "3                                         [2, 10, 3]  \n",
       "4                                        [2, 341, 3]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "MAX_LABEL = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoding and prepraing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(large_save_data_path + 'process-spacy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[BOS].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab.get_vector('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890280, 308)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "PHRASE_ID = data.Field(use_vocab = False)\n",
    "TEXT = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)\n",
    "LABEL = data.LabelField(use_vocab = False, dtype=torch.long)\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "ID_TEST = data.Field(use_vocab = False)\n",
    "PHRASE_ID_TEST = data.Field(use_vocab = False)\n",
    "TEXT_TEST = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "fields = [('id', PHRASE_ID), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "fields_test = [('id', ID_TEST), ('phrase_id', PHRASE_ID_TEST), ('text', TEXT_TEST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_kaggle_test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7fc36d6e3400>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e34a8>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e34e0>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e3518>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e3550>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e3588>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e35c0>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e35f8>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e3630>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e3668>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Train dataset\n",
    "examples = []\n",
    "length = len(loaded_data['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_data['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_data['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_data['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples.append(data.Example.fromlist([ [loaded_data['PhraseId'][i]], embedded, loaded_data['Sentiment'][i]], fields))\n",
    "    \n",
    "examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7fc36d6d0358>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6d0c88>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e00f0>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e0128>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e0160>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e0198>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e01d0>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e0208>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e0240>,\n",
       " <torchtext.data.example.Example at 0x7fc36d6e0278>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Test dataset\n",
    "examples_test = []\n",
    "length = len(loaded_kaggle_test['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_kaggle_test['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples_test.append(data.Example.fromlist([ [i], [loaded_kaggle_test['PhraseId'][i]], embedded ], fields_test))\n",
    "    \n",
    "examples_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "        -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "        -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "        -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "        -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "         3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "         3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "        -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "         3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "        -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "        -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "         3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "        -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "        -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "         1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "        -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "        -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "         6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "        -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "         4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "        -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "        -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "         2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "         2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "         1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "         1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "        -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "        -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "        -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "        -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "        -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "         4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "        -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "        -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "         1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "        -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "        -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "         4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "        -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "         1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "         1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "         7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "        -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "         7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "        -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "        -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "        -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "        -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "         8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "        -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "        -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "         3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "        -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "        -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "         3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "        -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "         4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "        -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "        -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "         8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.7793e-01, -6.2470e-02,  1.9596e-01, -4.3340e-02, -1.4570e-01,\n",
       "         6.7455e-02, -7.9048e-01, -4.7327e-01, -1.2694e-02,  9.4032e-01,\n",
       "         9.7596e-01,  3.6356e-01, -4.2334e-01, -4.7300e-01,  4.3313e-02,\n",
       "        -1.7159e-02,  7.7286e-02,  5.1450e-01, -1.3013e-01, -3.8677e-01,\n",
       "         8.5337e-02, -1.1537e-01, -2.9981e-01,  1.0327e-01,  7.4443e-02,\n",
       "         3.8978e-02, -1.0280e-01, -2.7481e-01, -2.9409e-01, -5.2790e-01,\n",
       "        -5.4200e-01, -4.1610e-01, -7.6345e-01, -2.1057e-01, -6.0277e-01,\n",
       "        -3.7840e-01, -5.1185e-01,  5.4210e-02, -8.2146e-02,  5.3257e-01,\n",
       "         3.3352e-01,  1.6082e-01, -1.6897e-02,  3.3638e-01, -1.3660e-01,\n",
       "         4.2157e-01, -2.5176e-01, -3.2271e-01, -3.9742e-01, -3.2712e-01,\n",
       "         3.5235e-01,  5.4523e-01, -5.8145e-01, -2.5093e-01, -4.2526e-01,\n",
       "         4.7148e-04,  5.4579e-01, -4.9980e-01,  2.3473e-02, -1.0801e-01,\n",
       "        -2.5952e-01, -7.5854e-03, -2.0041e-01, -3.7743e-01, -3.0881e-01,\n",
       "        -7.4719e-02,  3.9462e-03, -3.6002e-01, -7.1041e-02,  2.8597e-01,\n",
       "         1.3777e-01,  1.6269e-01, -1.0977e-01, -3.6319e-01,  8.5886e-02,\n",
       "         2.0098e-01,  1.9972e-01,  3.2699e-01, -1.1095e-01, -2.4142e-01,\n",
       "         1.1838e-01, -2.1962e-01, -6.1894e-01, -1.7774e-01, -1.5444e-01,\n",
       "         7.2033e-02,  7.5290e-01,  1.7973e-01,  3.2366e-01,  6.2210e-01,\n",
       "        -1.1795e-01, -2.4438e-01,  2.0690e-01,  4.0958e-01, -3.5867e-02,\n",
       "        -2.7432e-01,  3.3013e-01,  1.8814e-01,  2.6837e-01, -1.8496e-01,\n",
       "         6.3260e-02, -3.3311e-03, -5.4875e-02,  2.3964e-01, -1.4294e-01,\n",
       "        -1.9737e+00, -1.7595e-01, -7.4150e-02,  3.7287e-01,  1.6554e-01,\n",
       "         2.3770e-01, -1.4556e-01,  4.5459e-01, -8.1550e-02, -3.3170e-01,\n",
       "        -8.1371e-01, -1.7414e-01,  1.3691e-01, -6.0061e-01,  4.0306e-01,\n",
       "        -4.3174e-01, -1.7259e-01,  5.2948e-02, -2.4818e-01,  1.5827e-01,\n",
       "        -1.1220e-01,  3.2860e-02,  5.5025e-01,  2.0649e-01,  1.0403e-01,\n",
       "         4.3973e-01, -1.1043e-01, -2.3429e-01,  4.2473e-01, -2.0461e-01,\n",
       "        -2.3912e-01,  2.7573e-01,  6.4947e-01, -3.4708e-01, -1.4163e-01,\n",
       "        -1.7743e-01,  1.7144e-01, -1.7009e-01, -1.2667e-01, -5.6574e-02,\n",
       "         6.9985e-01,  9.3176e-02, -1.1523e-01,  7.9450e-02,  5.3647e-02,\n",
       "        -4.2493e-01,  7.1772e-01,  2.3705e-01,  2.1213e-01, -1.9776e-02,\n",
       "        -1.9379e-02,  2.9703e-01, -2.7536e-01,  7.3583e-02, -5.1954e-01,\n",
       "        -1.5928e-01, -3.5374e-01,  4.5916e-01,  2.0435e-01,  9.7278e-04,\n",
       "         4.6904e-01, -6.2419e-01,  3.2797e-01, -1.5744e-01, -8.8403e-02,\n",
       "         3.5625e-02, -1.2245e-01,  3.6698e-01, -8.8524e-02, -5.5171e-02,\n",
       "         6.0885e-01,  5.3027e-01, -3.3558e-01,  1.5178e-01, -1.6619e-01,\n",
       "         5.0028e-01, -1.4728e-02, -1.4555e-01,  5.6033e-01,  2.4656e-01,\n",
       "        -3.6279e-01, -7.8195e-04,  9.5365e-02,  5.8955e-02, -4.8528e-01,\n",
       "         5.4453e-01, -1.1293e-02, -1.5423e-01,  3.7746e-01, -4.1018e-01,\n",
       "        -7.5968e-02,  3.2843e-01, -1.0195e-01, -6.5580e-01, -5.7961e-01,\n",
       "        -4.3063e-01,  5.0521e-01,  1.4637e-02,  1.8917e-01,  5.0427e-02,\n",
       "        -1.2479e-01, -1.7724e-01, -1.0494e-01,  3.2128e-01,  5.9077e-02,\n",
       "         2.3804e-01, -6.3107e-02,  3.3216e-02, -2.0066e-01, -2.7485e-01,\n",
       "        -2.8084e-01,  4.9871e-01,  2.0036e-01,  2.9303e-01, -3.4742e-01,\n",
       "         4.9861e-01,  2.3172e-01,  1.4505e-01, -7.2846e-02, -8.4557e-02,\n",
       "        -1.5358e-01,  5.1501e-01, -1.7232e-01,  2.7167e-01, -2.3813e-01,\n",
       "         1.3970e-01,  2.5681e-01,  4.1433e-01,  3.3110e-01, -3.7062e-02,\n",
       "        -6.8761e-02,  6.5520e-02, -3.1337e-01, -3.6793e-02, -1.3847e-01,\n",
       "         2.2251e-01,  3.2041e-01,  3.1190e-01,  2.7103e-01, -4.0910e-01,\n",
       "         1.0762e-01,  1.6541e-01,  3.7305e-01,  2.1920e-01, -4.5466e-01,\n",
       "        -2.6570e-01, -4.3871e-01,  2.7044e-01,  2.0953e-01,  3.1834e-01,\n",
       "        -4.3875e-01,  2.4935e-01,  4.6443e-02, -3.2949e-01,  2.1793e-01,\n",
       "         7.3772e-01,  3.9756e-02, -5.0169e-02, -1.8881e-01,  7.7245e-02,\n",
       "         1.4199e-01,  3.9659e-02,  1.2333e-01, -9.6613e-02, -2.2687e-01,\n",
       "         4.8893e-01, -1.1373e-01,  5.1246e-02,  7.5477e-02, -1.2750e-01,\n",
       "        -1.0697e-01, -4.8071e-01, -2.6223e-01,  4.5893e-01,  3.6267e-01,\n",
       "         2.9100e-01,  1.7118e-02, -2.0185e-01, -7.4050e-02, -2.3640e-01,\n",
       "         2.8046e-01,  1.8657e-01,  8.7665e-02,  3.1472e-01, -7.0119e-02,\n",
       "        -1.8172e-01, -3.9074e-02,  2.8063e-02, -4.9746e-02,  1.0355e-01,\n",
       "        -1.5258e-01,  3.4939e-01, -1.6108e-01, -6.0705e-02, -9.0380e-03,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-0.2712   ,  0.14702  , -0.19126  , -0.24424  ,  0.16476  ,\n",
       "        -0.32537  ,  0.36027  ,  0.29804  , -0.49571  ,  1.2244   ,\n",
       "         0.097696 ,  0.30406  , -0.53059  , -0.25869  ,  0.16519  ,\n",
       "        -0.41392  , -0.2314   ,  0.86309  ,  0.35309  ,  0.41004  ,\n",
       "        -0.35114  ,  0.21699  ,  0.30512  , -0.30977  ,  0.065359 ,\n",
       "         0.13193  ,  0.59324  ,  0.089948 , -0.14227  , -0.31061  ,\n",
       "        -0.08432  ,  0.21609  , -0.57332  , -0.076418 , -0.0041399,\n",
       "        -0.48229  , -0.11635  , -0.0089557, -0.0064539, -0.031811 ,\n",
       "         0.16104  , -0.11247  , -0.042646 , -0.026045 ,  0.097288 ,\n",
       "        -0.030434 , -0.45127  , -0.028487 , -0.14567  , -0.10633  ,\n",
       "        -0.17795  , -0.097141 , -0.15773  , -0.42128  ,  0.25669  ,\n",
       "         0.53326  , -0.027647 ,  0.24529  , -0.46835  ,  0.090866 ,\n",
       "         0.12265  , -0.63001  , -0.25682  ,  0.0060041, -0.3454   ,\n",
       "         0.0072695,  0.17178  , -0.39181  , -0.060693 , -0.031147 ,\n",
       "         0.20888  ,  0.30827  ,  0.035736 , -0.23439  ,  0.28267  ,\n",
       "         0.08706  , -0.057023 , -0.30681  , -0.38813  , -0.039169 ,\n",
       "        -0.059306 , -0.36394  ,  0.26486  ,  0.29665  ,  0.12684  ,\n",
       "        -0.059682 ,  0.18076  ,  1.0117   , -0.020038 , -0.16224  ,\n",
       "        -0.069924 ,  0.29389  ,  0.023328 ,  0.67408  ,  0.13449  ,\n",
       "         0.021691 ,  0.21811  , -0.40404  , -0.13521  ,  0.11014  ,\n",
       "         0.19624  ,  0.05419  ,  0.16292  , -0.17251  ,  0.27033  ,\n",
       "        -1.616    ,  0.60791  ,  0.0033667, -0.29159  ,  0.14195  ,\n",
       "        -0.075748 , -0.19854  ,  0.27378  ,  0.087632 , -0.023092 ,\n",
       "        -0.24036  , -0.20799  , -0.062753 ,  0.040184 ,  0.032635 ,\n",
       "        -0.2035   ,  0.15727  , -0.67543  ,  0.67401  , -0.005282 ,\n",
       "         0.067059 ,  0.20846  , -0.65743  ,  0.17864  , -0.27987  ,\n",
       "        -0.022082 , -0.20144  ,  0.13538  , -0.12531  , -0.39399  ,\n",
       "         0.3964   , -0.11625  , -0.34535  ,  0.25273  ,  0.24191  ,\n",
       "        -1.6655   ,  0.61688  , -0.0035128,  0.37053  , -0.32058  ,\n",
       "        -0.48533  ,  0.1568   , -0.086594 ,  0.13272  , -0.050194 ,\n",
       "         0.12197  ,  0.0621   , -0.052821 ,  0.13192  ,  0.26901  ,\n",
       "         0.26882  , -0.47454  , -0.28357  ,  0.16668  , -0.51742  ,\n",
       "        -0.2906   ,  0.25908  ,  0.71942  ,  0.50009  , -0.092692 ,\n",
       "        -0.36577  , -0.034634 ,  0.056318 , -0.21117  ,  0.37486  ,\n",
       "         0.032773 , -0.14359  ,  0.6884   , -0.026806 ,  0.42897  ,\n",
       "         0.053205 , -0.42011  , -0.25344  , -0.21894  ,  0.14449  ,\n",
       "         0.045221 , -0.19444  ,  0.40097  ,  0.42644  , -0.39018  ,\n",
       "        -0.12735  ,  0.11559  ,  0.019183 , -0.027092 , -0.44417  ,\n",
       "         0.045983 ,  0.30359  ,  0.22169  ,  0.060414 , -0.0059974,\n",
       "        -0.66994  , -0.058462 ,  0.16644  , -0.1518   ,  0.0072898,\n",
       "        -0.21948  ,  0.45436  ,  0.05823  , -0.31103  , -0.23067  ,\n",
       "         0.0078952, -0.30799  ,  0.10996  ,  0.28958  , -0.033313 ,\n",
       "        -0.46981  ,  0.17656  , -0.077724 , -0.44687  , -0.13734  ,\n",
       "         0.44273  ,  0.43806  , -0.32096  ,  0.65241  , -0.29144  ,\n",
       "         0.35948  , -0.39277  ,  0.078458 ,  0.6577   ,  0.45683  ,\n",
       "         0.093801 , -0.25023  ,  0.046448 ,  0.04619  , -0.14565  ,\n",
       "         0.17054  ,  0.072038 , -0.86523  ,  0.20702  ,  0.45185  ,\n",
       "         0.20958  ,  0.043511 , -0.17528  , -0.15821  ,  0.011212 ,\n",
       "         0.41577  , -0.31444  , -0.17748  ,  0.089159 , -0.34662  ,\n",
       "        -0.097137 , -0.28784  , -0.38902  ,  0.21508  ,  0.31001  ,\n",
       "         0.030091 ,  0.30658  ,  0.23107  ,  0.048666 ,  0.13822  ,\n",
       "         0.4905   ,  0.66801  , -0.023158 ,  0.38607  ,  0.1293   ,\n",
       "        -0.027527 , -0.31134  ,  0.31469  ,  0.26038  ,  0.3263   ,\n",
       "         0.087236 , -0.014318 , -0.080271 , -0.14507  ,  0.34931  ,\n",
       "        -0.69845  , -0.25372  , -0.28831  , -0.038035 , -0.59529  ,\n",
       "         0.17554  ,  0.78562  , -0.066821 , -0.062887 ,  0.28903  ,\n",
       "         0.82979  ,  0.33097  ,  0.17223  , -0.27315  , -0.5014   ,\n",
       "        -0.29307  ,  0.43277  ,  0.59082  ,  0.059657 ,  0.19892  ,\n",
       "        -0.39105  , -0.020178 , -0.23956  ,  0.26956  ,  0.15734  ,\n",
       "         0.049013 ,  0.2598   , -0.15101  ,  0.069548 ,  0.058311 ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ], dtype=float32),\n",
       " array([-1.6890e-02,  1.7402e-01, -3.0247e-01, -3.0063e-01,  2.1415e-01,\n",
       "         6.3863e-02,  1.0107e-01, -2.4155e-01, -9.5228e-02,  2.9253e+00,\n",
       "        -5.6759e-03,  1.1752e-01,  1.6121e-01,  2.0813e-02, -8.3593e-02,\n",
       "        -1.4048e-01, -4.0069e-02,  7.5133e-01, -5.6699e-02, -9.6198e-02,\n",
       "        -7.2134e-02, -3.1287e-02,  1.8146e-01, -2.4846e-01, -6.5068e-02,\n",
       "         6.6374e-02, -1.2173e-01, -1.3479e-01,  2.6163e-01, -2.1599e-01,\n",
       "        -2.4221e-01,  9.1074e-02, -4.3504e-02, -1.8047e-01, -1.8158e-01,\n",
       "        -6.6229e-02,  2.6480e-02, -2.5439e-01, -7.8805e-02, -2.1853e-01,\n",
       "        -3.0239e-02,  1.3049e-01,  1.0992e-01,  3.5563e-02,  3.2769e-01,\n",
       "        -2.7750e-02, -1.8852e-01, -1.8579e-01, -8.5064e-02, -2.4057e-02,\n",
       "        -1.4141e-01,  1.2708e-01, -7.9024e-02, -1.3320e-01,  3.7619e-02,\n",
       "        -1.1080e-01, -2.2742e-01, -2.7703e-01, -8.1760e-02,  4.7761e-02,\n",
       "        -1.7936e-01, -3.1907e-01,  1.3931e-01,  3.6374e-01, -2.1399e-01,\n",
       "        -2.7044e-01, -2.0847e-01,  1.1192e-02,  1.6017e-01,  2.4218e-01,\n",
       "         2.5058e-01,  3.2767e-01,  1.3340e-01,  6.4510e-03,  1.7994e-02,\n",
       "         1.1242e-01,  9.0136e-02, -7.2882e-02, -1.6506e-01,  4.8300e-01,\n",
       "         4.6465e-03, -4.8611e-02,  2.7421e-02,  9.7357e-02,  1.0873e-01,\n",
       "        -3.1722e-01,  2.6092e-01, -5.9396e-01,  2.6522e-01,  7.2456e-02,\n",
       "        -1.9246e-01,  3.4932e-02, -1.5662e-01,  3.0779e-01,  3.3744e-01,\n",
       "        -4.8664e-03,  1.5165e-01, -1.9861e-02, -1.4789e-01, -1.7031e-02,\n",
       "        -1.7279e-01, -3.8278e-02, -3.2560e-01, -1.6450e-01,  2.2257e-01,\n",
       "        -1.2442e+00, -1.4105e-01,  7.4932e-02, -1.8879e-01, -1.0341e-01,\n",
       "        -3.6646e-03, -2.1667e-01,  3.5087e-02, -1.9168e-01, -1.0044e-01,\n",
       "        -2.8554e-03,  5.6079e-02, -7.4098e-02, -8.3292e-03, -2.3693e-01,\n",
       "         4.1375e-02,  6.9771e-02,  2.0383e-01, -1.8666e-01, -4.2077e-02,\n",
       "         1.3305e-01, -4.5272e-02, -2.9631e-01,  1.6454e-01, -1.1591e-01,\n",
       "         2.4631e-02, -6.1717e-02, -6.2958e-02,  1.3487e-01,  2.0565e-01,\n",
       "         1.4678e-02,  1.9056e-01, -3.2541e-01,  1.0896e-01, -9.1117e-02,\n",
       "        -1.8126e+00,  3.5567e-01,  5.4440e-01, -3.5205e-02, -2.7339e-02,\n",
       "        -1.0750e-01, -3.0940e-01,  1.2539e-01,  8.7296e-02,  8.2909e-02,\n",
       "        -7.9026e-02, -1.8092e-02,  2.6749e-01,  1.1947e-02, -1.3090e-01,\n",
       "        -3.3304e-01, -2.0343e-01, -2.8020e-01, -1.0974e-01, -2.9613e-01,\n",
       "        -1.3973e-01,  1.8934e-01,  3.5228e-02, -2.5916e-01, -2.6859e-01,\n",
       "        -2.2678e-01,  1.3141e-01,  5.1456e-02,  2.6723e-01, -1.8335e-01,\n",
       "        -7.5100e-02, -1.6394e-02, -9.9408e-02, -1.8685e-01, -1.4718e-01,\n",
       "         6.4933e-02, -1.3858e-01, -1.9941e-01, -8.3017e-02, -1.7141e-01,\n",
       "         1.4692e-02, -2.4256e-01, -2.7010e-01, -8.5571e-02, -1.7614e-01,\n",
       "        -1.7786e-01, -1.3418e-02, -1.4634e-01,  3.0529e-01,  6.0719e-02,\n",
       "         2.8651e-01,  1.4169e-01, -1.9963e-01,  1.3507e-01,  7.0590e-02,\n",
       "        -4.2919e-02, -7.0586e-02, -3.5384e-01,  1.3877e-01,  2.4022e-01,\n",
       "        -3.0725e-02,  4.1167e-02, -2.2029e-01, -1.1796e-01,  1.1195e-01,\n",
       "         2.0739e-01, -1.6588e-01, -1.4708e-01,  9.7583e-02,  2.0831e-01,\n",
       "        -1.2357e-01, -7.9109e-02, -7.7406e-02, -3.4475e-01,  2.9539e-01,\n",
       "         1.4243e-02,  7.5820e-02, -1.9915e-02, -1.6280e-01,  1.9031e-02,\n",
       "         2.5743e-01,  1.2232e-01, -8.2438e-02,  6.1687e-02,  2.1967e-01,\n",
       "         2.2370e-01, -9.4008e-02,  1.5609e-01,  1.6797e-03,  1.8566e-01,\n",
       "        -2.6615e-01, -1.0925e-01,  3.6341e-01,  1.0227e-01,  1.6318e-01,\n",
       "        -1.5906e-01,  2.7914e-02, -9.7719e-02,  2.1670e-02,  1.3730e-01,\n",
       "         3.1447e-01,  1.6585e-02, -3.2731e-01,  4.1633e-01,  1.8380e-01,\n",
       "        -2.2986e-01, -1.2125e-01, -1.5239e-01, -1.3330e-02,  1.5810e-01,\n",
       "         3.2326e-01, -7.6780e-02, -1.2008e-01, -1.1215e-01, -5.0701e-02,\n",
       "         1.1711e-01,  1.4279e-01, -1.2161e-01, -1.5066e-02, -7.6299e-02,\n",
       "         2.2693e-01,  1.7511e-01,  2.3596e-02,  1.4218e-01,  2.1250e-01,\n",
       "         2.1344e-01, -6.8560e-02,  6.3065e-02,  5.5161e-01,  2.6941e-01,\n",
       "         1.7143e-01, -1.9773e-01,  3.0509e-02, -3.7473e-01, -2.3374e-01,\n",
       "         1.9453e-01,  2.3319e-03,  5.2309e-03,  1.6025e-01,  5.0800e-01,\n",
       "         3.0588e-01,  2.6994e-02,  1.1541e-01, -7.9795e-02, -9.9845e-02,\n",
       "        -3.1527e-02,  9.3683e-02,  1.0850e-02,  3.8706e-01, -2.6888e-01,\n",
       "        -4.0783e-01, -1.6802e-03,  1.5798e-02,  3.5886e-02,  1.1664e-01,\n",
       "         2.3909e-02, -5.3473e-02, -3.2640e-01,  1.7957e-01,  1.1095e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.1395e-01, -1.8600e-01, -3.1139e-01,  6.9232e-02, -2.2679e-02,\n",
       "         3.5245e-01, -1.7451e-01, -3.4762e-02,  3.6499e-02,  2.4352e+00,\n",
       "         1.5621e-01, -4.1787e-02, -7.2947e-02, -3.1921e-02,  8.9066e-02,\n",
       "         1.9200e-01,  1.6327e-02,  8.0790e-01, -2.3115e-02, -5.3932e-02,\n",
       "        -2.7845e-01,  2.0944e-02, -2.3572e-01,  8.7822e-02,  1.7885e-01,\n",
       "        -1.2684e-01,  1.1533e-01, -1.9993e-01,  1.7179e-01, -2.9116e-01,\n",
       "        -1.3022e-01,  1.4925e-02, -3.4743e-01, -4.2228e-01, -4.9583e-01,\n",
       "         2.5687e-01, -6.4040e-02,  1.4373e-01, -8.7055e-02,  2.3477e-02,\n",
       "         1.1067e-01,  3.4200e-01,  1.0643e-01, -5.5534e-03,  2.1959e-01,\n",
       "         3.2662e-01,  4.2147e-01, -1.1966e-01,  1.5098e-01,  5.5619e-02,\n",
       "        -3.8991e-01,  1.5516e-01, -8.2267e-02, -2.4025e-01, -3.4519e-01,\n",
       "        -1.9722e-01, -3.1244e-01, -5.8754e-01, -2.0353e-01, -7.6791e-03,\n",
       "        -9.1359e-02, -1.7571e-01, -1.7574e-01,  3.2297e-01, -2.6038e-01,\n",
       "        -2.8268e-01, -6.8955e-02,  2.6774e-02, -1.0006e-01,  3.6755e-01,\n",
       "        -1.4354e-01,  3.6205e-01, -1.1899e-01,  1.9502e-02,  6.3868e-01,\n",
       "        -1.4751e-02, -1.6021e-01,  4.8338e-01,  5.8345e-03,  3.2956e-01,\n",
       "         1.2950e-01, -2.2254e-01, -1.9471e-01, -2.5058e-01,  1.0409e-01,\n",
       "        -1.5040e-01, -3.0280e-01,  2.3811e-01,  2.9955e-01, -1.9751e-02,\n",
       "         1.8583e-03, -3.4478e-01,  1.4361e-01, -1.2800e-03,  4.9823e-01,\n",
       "        -6.4337e-02,  4.9903e-01,  5.1760e-01, -1.5414e-01, -3.1006e-01,\n",
       "        -1.6568e-01,  2.8804e-02, -2.9711e-01,  3.8346e-02,  2.3626e-01,\n",
       "        -1.5317e+00,  4.8050e-03, -4.0435e-01,  2.8585e-02,  1.5562e-01,\n",
       "         8.9044e-02, -8.2613e-01, -3.3854e-01, -2.1842e-01, -4.1949e-01,\n",
       "         1.4582e-01, -7.1290e-02, -1.7822e-01, -4.6116e-01,  8.1208e-02,\n",
       "         1.0228e-01, -2.2000e-02,  1.5704e-01, -4.6219e-01,  4.4219e-01,\n",
       "        -1.2331e-02,  6.0651e-01, -4.6436e-02,  7.1817e-02, -2.8741e-01,\n",
       "         8.0092e-02,  9.5995e-02,  2.1868e-01,  9.2623e-02,  5.9085e-02,\n",
       "        -1.5068e-01,  7.5694e-02, -4.2070e-01,  7.7992e-02,  2.3851e-01,\n",
       "        -1.4185e+00, -6.6634e-02,  1.6350e-01,  1.4849e-01, -1.9227e-01,\n",
       "         1.2907e-02, -3.4447e-01, -4.6486e-01,  1.4224e-01,  1.3265e-01,\n",
       "         8.8162e-02,  1.6292e-01,  1.3023e-01, -3.1362e-01, -3.9454e-01,\n",
       "        -1.4404e-01, -2.1622e-01, -2.0923e-01, -3.4271e-01, -1.6063e-01,\n",
       "         7.1870e-02, -1.9801e-01, -5.0524e-02,  7.4052e-05, -4.3107e-01,\n",
       "        -8.2001e-02, -3.8524e-01, -4.9691e-02,  1.0136e-01, -4.7003e-01,\n",
       "         1.1299e-01,  2.6024e-01, -1.6097e-01,  8.5316e-02, -2.4209e-01,\n",
       "         4.3188e-02, -2.8629e-01, -2.3318e-01,  1.9134e-01, -3.7870e-01,\n",
       "         1.9383e-01, -1.3177e-01, -4.2849e-01, -2.7833e-01, -6.3169e-02,\n",
       "        -2.3149e-01,  3.5011e-02, -2.3458e-01,  5.1698e-01,  8.0902e-02,\n",
       "         5.3812e-01, -1.0787e-01, -2.1465e-01, -3.5011e-01, -1.1482e-01,\n",
       "        -3.7470e-01, -1.4581e-01, -1.5781e-01, -8.8941e-02, -2.5458e-01,\n",
       "        -2.2646e-01, -1.4723e-01,  1.9318e-01,  1.5410e-01,  3.5199e-02,\n",
       "        -1.0698e-01, -4.4188e-02, -4.5023e-01,  2.6755e-01,  3.9788e-02,\n",
       "         1.6353e-01, -4.0591e-02,  1.4291e-01, -2.7215e-01,  2.4453e-01,\n",
       "         1.0331e-02,  2.1538e-01, -1.1237e-01, -3.7174e-02, -1.1926e-01,\n",
       "         2.9896e-01,  5.2738e-02, -8.8533e-02,  4.3619e-02, -1.0675e-01,\n",
       "         1.6294e-01,  2.5775e-01, -3.6536e-02,  9.5472e-02,  2.0618e-01,\n",
       "        -1.4716e-01, -2.3276e-01,  2.8959e-01,  3.0868e-02,  2.7450e-01,\n",
       "        -2.0971e-02,  2.8721e-01, -2.1334e-01,  4.2814e-01,  3.2721e-01,\n",
       "        -1.8220e-01,  2.6856e-01, -2.6014e-01,  2.3042e-01, -3.0705e-01,\n",
       "         2.9832e-01, -4.2529e-01, -3.5242e-01,  2.3699e-01, -5.1793e-03,\n",
       "         1.9576e-01,  1.3232e-01,  1.8231e-01,  9.8129e-02,  2.3562e-01,\n",
       "        -6.8167e-02,  1.2068e-01,  1.9259e-01, -1.3113e-01,  1.4483e-01,\n",
       "         3.3028e-01,  1.0524e-01, -1.6241e-01,  1.8550e-01, -5.3597e-02,\n",
       "         1.2893e-01,  1.0969e-01, -3.3200e-01,  1.4515e-01,  2.8574e-01,\n",
       "        -2.3060e-01, -2.6947e-01, -4.2454e-01, -9.0470e-01, -9.2193e-02,\n",
       "        -1.4784e-01,  5.1422e-02, -2.2871e-01,  1.0521e-01,  4.7294e-01,\n",
       "         2.0239e-01, -1.6172e-02, -2.8370e-02, -1.0857e-01,  2.3019e-01,\n",
       "         1.2928e-01, -3.9928e-01, -5.9975e-02, -1.4594e-01, -5.7901e-02,\n",
       "        -7.0202e-01, -1.8404e-01, -8.4864e-02,  4.5815e-02, -2.0621e-01,\n",
       "         4.5970e-01, -9.3192e-02, -6.1150e-01, -7.5240e-02, -1.2648e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7386e-02,  2.1880e-01,  1.7469e-01,  2.7509e-01, -2.1631e-01,\n",
       "         3.5347e-01, -2.2180e-02, -2.8373e-01,  2.5881e-01,  1.9462e+00,\n",
       "        -4.0591e-02, -1.7405e-01,  1.3791e-01,  5.1707e-01, -1.1379e-01,\n",
       "         4.3605e-01,  4.4465e-01,  1.5117e+00, -3.5965e-01,  4.3858e-02,\n",
       "        -5.1923e-02,  2.9570e-01, -7.3748e-01, -5.4741e-03, -1.7881e-01,\n",
       "        -2.2322e-01,  6.4444e-01, -5.1164e-01,  9.3286e-02, -3.9014e-01,\n",
       "        -2.3993e-01, -3.4526e-01,  6.7683e-03, -4.2794e-01,  1.0783e-01,\n",
       "         4.2653e-01,  3.0891e-01, -1.0818e-01,  1.8644e-01, -2.1826e-01,\n",
       "        -4.1482e-01,  1.0898e-02, -1.7486e-01,  3.9860e-01, -2.0510e-01,\n",
       "         4.8380e-01, -5.0375e-02,  8.0554e-01, -9.0117e-02,  9.7864e-02,\n",
       "         1.5842e-01, -3.6625e-01,  3.8516e-01, -3.7493e-02, -3.4814e-01,\n",
       "         9.3430e-02,  9.1870e-01, -2.3090e-01,  3.5374e-01,  6.5830e-02,\n",
       "         1.1644e-01,  5.0977e-01, -1.5380e-01, -1.8577e-01,  4.4437e-01,\n",
       "         2.7601e-01,  2.2735e-01,  5.3475e-01, -1.8252e-01, -7.4210e-02,\n",
       "         2.1165e-01, -5.0073e-01,  1.2662e-01, -1.2315e-01,  3.8191e-01,\n",
       "        -1.6075e-01,  5.3988e-02, -1.9419e-01,  1.7041e-02, -2.8735e-01,\n",
       "         4.7740e-01, -6.2051e-01, -3.0525e-01, -3.2444e-01, -8.1936e-02,\n",
       "         2.4708e-01,  1.6119e-01,  1.4163e-01, -2.6534e-02,  6.8003e-01,\n",
       "         3.8574e-01,  7.9060e-01,  1.7182e-01,  1.9055e-01, -4.0842e-01,\n",
       "        -7.1388e-01, -2.4055e-01,  3.0572e-01,  3.2042e-01,  9.4508e-02,\n",
       "         1.9716e-01, -3.2531e-01, -4.6567e-02,  4.9304e-03,  2.3895e-01,\n",
       "        -1.5985e+00,  4.4618e-01,  6.7059e-02,  4.2562e-02, -9.4264e-02,\n",
       "         2.6359e-01,  5.9031e-02,  2.1923e-01,  1.1912e-01, -4.8703e-01,\n",
       "        -1.6320e-01,  1.3074e-01,  1.3469e-01, -8.6061e-04,  6.5107e-02,\n",
       "         2.6348e-01, -6.8230e-02, -3.4943e-01,  1.4124e-03, -3.2127e-01,\n",
       "        -3.2119e-01,  2.4275e-01,  1.4489e-01,  5.2155e-01,  1.8236e-02,\n",
       "         3.8936e-01,  1.7392e-01, -1.8137e-01,  2.3953e-01,  7.9878e-02,\n",
       "         2.3554e-01,  3.4491e-01,  1.3673e-01, -2.4935e-02,  1.2661e-01,\n",
       "        -9.4117e-01, -2.7312e-01, -3.9914e-02, -2.1242e-02, -2.4627e-01,\n",
       "         1.8920e-01, -3.7878e-02, -7.5084e-01,  3.1216e-01, -8.1300e-02,\n",
       "        -1.4081e-01,  1.9424e-01,  5.0618e-01,  5.5668e-02, -2.2838e-01,\n",
       "        -7.9835e-02,  3.3727e-01, -3.9557e-01, -3.1844e-01, -6.1797e-01,\n",
       "        -4.3276e-01, -1.0177e-02, -1.6727e-01, -1.4701e-01, -1.3138e-03,\n",
       "        -3.6874e-01,  8.8321e-03,  1.0844e-01,  2.2943e-01, -5.6433e-01,\n",
       "         3.1256e-01, -1.9476e-01, -7.2129e-02, -8.9461e-01, -5.7574e-01,\n",
       "        -9.0824e-02, -6.1415e-01,  1.7660e-01, -2.6355e-01,  6.2693e-01,\n",
       "        -4.5756e-01, -8.6851e-01, -2.4981e-01,  5.5472e-01, -8.5896e-02,\n",
       "         1.1540e-01,  3.1408e-01,  3.9062e-01, -3.4876e-01,  5.4840e-02,\n",
       "        -2.2244e-01,  5.9099e-02,  9.9086e-02, -5.1477e-01, -1.7693e-01,\n",
       "         1.3470e-01, -2.5853e-01, -4.6366e-01,  2.2667e-01,  2.6040e-01,\n",
       "        -3.4442e-01,  2.5386e-01,  3.2574e-02,  3.5130e-01, -6.8617e-01,\n",
       "        -1.3479e-01, -1.5026e-01,  1.6556e-01,  3.2562e-01,  1.3296e-01,\n",
       "         2.2568e-01,  2.6165e-01,  9.7049e-03,  4.1276e-02, -3.9056e-01,\n",
       "        -2.5348e-01,  1.1446e-01, -2.1078e-01,  3.7831e-02, -4.9510e-02,\n",
       "         3.3036e-01,  2.0437e-01, -5.9281e-01,  1.0520e-01, -4.3911e-01,\n",
       "         5.1661e-01, -1.9186e-01,  6.2483e-01,  2.6081e-02, -1.2373e-01,\n",
       "        -2.3343e-01,  7.1194e-01,  3.1010e-01,  1.5132e-01, -2.8571e-01,\n",
       "         2.9020e-02, -6.9205e-02, -2.8972e-01,  1.1664e-03,  9.4657e-03,\n",
       "        -3.1325e-01, -3.6959e-01,  6.3799e-01,  4.5772e-03, -4.4835e-02,\n",
       "        -1.0021e-01,  2.0841e-01, -8.3103e-02,  3.5557e-02,  5.7080e-01,\n",
       "         1.3201e-02,  6.1434e-02,  7.9083e-02, -5.3205e-02,  1.2540e-01,\n",
       "         1.2251e-01,  2.2707e-01, -6.0252e-02, -1.3078e-01,  1.8732e-01,\n",
       "        -2.4053e-01,  3.9825e-01,  9.7576e-02, -6.0194e-01, -9.7147e-02,\n",
       "         5.0851e-01,  9.4998e-02,  2.9620e-01,  6.9577e-01,  2.7756e-03,\n",
       "         6.0900e-02, -2.9739e-01, -2.0041e-01, -3.3111e-01,  3.2232e-01,\n",
       "         2.9970e-01,  7.8138e-02, -2.1120e-01,  2.4587e-01,  7.2880e-01,\n",
       "        -5.8055e-02, -3.1365e-02,  5.0065e-01, -4.5560e-03, -2.1692e-01,\n",
       "        -1.8249e-01, -3.7010e-01,  1.9931e-01,  3.1825e-02, -1.2789e-01,\n",
       "         5.8350e-01,  4.7768e-01,  3.3556e-01,  1.1028e-01,  1.5622e-01,\n",
       "        -7.6022e-02,  2.5118e-01,  5.3027e-02, -7.8588e-02, -4.7783e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7675e-01, -1.1799e-01,  1.7866e-01,  3.2482e-02, -4.9160e-01,\n",
       "         4.4509e-02, -3.8697e-01,  3.4402e-02, -4.0194e-02,  3.1124e+00,\n",
       "        -2.2361e-01,  1.1824e-01,  6.9428e-02, -3.2642e-02,  1.4955e-01,\n",
       "         1.5286e-01,  3.2991e-02,  6.3527e-01, -2.3576e-01,  2.4908e-01,\n",
       "         2.0384e-01,  1.4182e-01,  2.0118e-01, -2.0678e-01, -1.6844e-01,\n",
       "         1.3097e-01,  4.8774e-01, -3.5005e-01, -7.5147e-02, -1.5022e-01,\n",
       "         1.6138e-01, -5.5018e-01, -1.9398e-01, -2.8315e-02,  1.6917e-01,\n",
       "        -4.0531e-01,  6.5695e-02, -1.3529e-01,  1.7965e-01,  7.5198e-02,\n",
       "         1.2148e-01,  6.1736e-01,  3.6658e-02, -5.6777e-01,  3.2880e-01,\n",
       "        -2.4745e-01,  1.3974e-01,  7.4644e-01, -1.9929e-01,  3.6678e-01,\n",
       "         2.9422e-01, -8.5806e-02,  1.2210e-01,  1.6887e-01, -2.1271e-01,\n",
       "         7.3281e-03, -7.6509e-02,  1.7360e-01,  3.0140e-01, -6.2679e-02,\n",
       "        -2.4980e-01, -6.2602e-02, -2.8363e-01,  1.1710e-01,  1.4115e-01,\n",
       "        -5.0121e-01,  1.2329e-01, -1.8731e-01,  3.9721e-01, -2.5896e-02,\n",
       "         2.9596e-01, -3.2547e-02,  1.7333e-01, -1.1771e-01,  1.2088e-01,\n",
       "        -3.5890e-01, -1.3854e-01, -8.9703e-02,  3.1391e-01, -9.1165e-02,\n",
       "        -1.3064e-01, -2.2216e-02,  1.5877e-02,  2.6226e-01, -2.7216e-01,\n",
       "        -1.5718e-01, -8.3953e-01, -1.4901e-01,  1.2313e-01,  3.2840e-01,\n",
       "        -1.1891e-01,  5.5510e-02,  1.8767e-01,  5.3449e-01, -2.0262e-01,\n",
       "         1.6928e-01, -1.9827e-01, -2.4103e-01, -1.8439e-02, -9.7009e-02,\n",
       "        -7.1952e-02, -3.6047e-01, -9.6811e-02, -2.9746e-01, -2.7191e-02,\n",
       "        -1.4366e+00,  4.3149e-01,  1.3098e-01, -7.2661e-02,  1.7736e-01,\n",
       "         1.6811e-01,  1.7400e-02,  1.7484e-01, -2.2591e-01,  4.0063e-01,\n",
       "         1.2184e-01,  6.9323e-02, -1.1818e-01,  3.1484e-01, -1.0699e-01,\n",
       "         4.0806e-01, -1.2300e-01, -3.1155e-01,  2.1126e-01, -5.7120e-02,\n",
       "         8.6237e-02, -4.4460e-01,  2.2277e-04,  2.4001e-02,  3.0837e-01,\n",
       "        -1.6179e-01,  2.5340e-01, -4.2660e-02,  2.5280e-03,  9.9567e-02,\n",
       "        -5.1727e-02,  3.5747e-02, -1.7966e-01,  3.3886e-01, -1.6398e-01,\n",
       "        -7.8192e-01,  6.0500e-01,  2.8252e-01, -1.2994e-01, -1.6583e-02,\n",
       "        -3.4698e-01, -7.2527e-01,  8.0799e-02,  9.4704e-02,  3.6744e-01,\n",
       "        -8.9103e-02,  6.9915e-02,  1.6852e-01, -1.2281e-01, -2.8876e-02,\n",
       "        -7.7054e-02, -1.8588e-01, -1.8323e-01, -1.9980e-01,  4.5056e-02,\n",
       "        -7.6702e-02,  3.8135e-01, -3.8801e-01, -2.1650e-01, -3.0306e-02,\n",
       "         1.6275e-01, -1.0535e-02, -1.6363e-01,  7.2000e-04, -2.2225e-02,\n",
       "        -7.1306e-02, -2.7842e-02,  2.6646e-01, -2.7049e-01, -2.1350e-02,\n",
       "        -4.4271e-02, -1.2112e-01,  1.1946e-01, -1.1000e-02,  3.2661e-01,\n",
       "        -1.8398e-01,  7.4138e-02, -4.2220e-02,  9.7410e-02, -1.4307e-01,\n",
       "         1.0406e-02,  2.1065e-02,  1.9372e-01, -1.3614e-01,  1.1002e-01,\n",
       "         1.5361e-01, -5.0540e-02,  8.6166e-02,  2.4223e-01,  1.4647e-01,\n",
       "        -1.3835e-01, -6.9210e-02, -1.4987e-01,  1.8680e-01,  3.9816e-01,\n",
       "         5.7365e-01,  1.5033e-02,  4.5188e-02,  1.6298e-01,  9.3265e-02,\n",
       "        -7.3806e-02, -3.6836e-01, -1.0767e-01,  4.6960e-01, -1.3725e-01,\n",
       "         1.1711e-01,  2.3232e-01,  2.1569e-02,  8.2387e-03,  2.0852e-01,\n",
       "         1.8448e-01,  2.6981e-01, -7.7155e-02, -1.5074e-01, -1.4044e-01,\n",
       "         1.5819e-01, -3.1127e-01, -2.3062e-01,  1.6080e-01,  1.3612e-01,\n",
       "        -6.8917e-02,  2.7529e-01, -2.0376e-01, -4.3880e-02,  3.3157e-01,\n",
       "         1.8735e-01,  5.5636e-02,  1.2360e-01, -1.1264e-01,  1.3654e-01,\n",
       "         3.6212e-01, -2.0679e-01,  2.2256e-02,  8.0054e-02, -5.1004e-02,\n",
       "         1.5415e-01, -4.8844e-01, -1.7730e-01,  4.1286e-01, -2.8209e-01,\n",
       "        -2.8011e-01, -1.5195e-01, -3.5349e-01,  3.4257e-01,  1.2766e-01,\n",
       "        -4.7474e-02,  3.2728e-01, -6.9187e-01,  1.7493e-01,  8.9532e-02,\n",
       "        -6.1105e-02, -6.9829e-02, -1.5193e-01,  5.7987e-02,  3.0111e-02,\n",
       "        -3.9252e-02,  5.3934e-02,  4.5133e-01,  4.2870e-02, -2.2657e-01,\n",
       "        -3.5830e-01, -5.6488e-02,  1.0440e-01,  4.9360e-01,  2.7961e-01,\n",
       "         5.4662e-01, -1.3974e-01, -2.1185e-01, -4.5541e-01, -6.9248e-02,\n",
       "        -3.5238e-01,  2.2941e-01, -2.4661e-01,  1.6707e-01,  2.9528e-01,\n",
       "        -1.2137e-01, -2.0418e-01,  2.4620e-01,  3.4143e-01, -2.9176e-01,\n",
       "        -5.7014e-01, -2.6190e-01, -3.2669e-01,  3.1928e-01, -3.0172e-01,\n",
       "        -1.3678e-01,  4.1218e-01,  1.1811e-01,  2.8056e-01,  4.2607e-02,\n",
       "        -4.6627e-01,  2.9769e-01, -4.3038e-02, -5.6063e-01,  2.5515e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_test[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "       -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "       -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "       -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "       -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "        3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "        3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "       -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "        3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "       -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "       -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "        3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "       -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "       -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "        1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "       -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "       -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "        6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "       -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "        4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "       -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "       -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "        2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "        2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "        1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "        1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "       -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "       -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "       -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "       -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "       -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "        4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "       -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "       -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "        1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "       -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "       -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "        4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "       -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "        1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "        1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "        7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "       -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "        7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "       -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "       -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "       -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "       -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "        8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "       -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "       -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "        3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "       -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "       -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "        3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "       -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "        4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "       -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "       -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "        8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.Dataset(examples, fields)\n",
    "data_set_test = data.Dataset(examples_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.sort_key = lambda x: len(x.text)\n",
    "data_set_test.sort_key = lambda x: len(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data_set.split([0.8, 0.1, 0.1], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124848"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(train_data.sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set_test.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(data_set_test.sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)\n",
    "\n",
    "kaggle_test_iterator = data.BucketIterator(\n",
    "    data_set_test, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_iterator = iter(train_iterator)\n",
    "batch = next(raw_train_iterator)\n",
    "\n",
    "raw_kaggle_test_iterator = iter(kaggle_test_iterator)\n",
    "batch_test = next(raw_kaggle_test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = batch.text\n",
    "\n",
    "c, d = batch_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 308])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 1, 3, 3, 1, 3, 3, 2, 2, 2, 2, 2, 0, 2, 1, 4, 3, 2, 3, 2,\n",
       "        2, 2, 1, 3, 1, 2, 2, 1, 3, 1, 2, 2, 1, 4, 2, 3, 2, 3, 2, 2, 1, 4, 2, 2,\n",
       "        2, 3, 2, 3, 1, 2, 2, 3, 4, 2, 2, 4, 2, 2, 1, 4], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133918,  57783,  94049,  86671,  84086,  74128, 112683, 126943,  12599,\n",
       "         138419,  47937,  62635, 104584,  86587, 113162,  81036,  62472, 147974,\n",
       "          32209, 133654,  95241, 137072,  66415,  75555,  53039, 130485,  79837,\n",
       "         152257,  39951, 136680, 121126, 102615, 101662, 119728, 108929, 145776,\n",
       "          36689,  66928, 109732, 109843, 130506,  33026, 123268, 155495, 114095,\n",
       "          27453, 122599,  82520,  21535, 104973,  91096,  97182,  84542, 119081,\n",
       "         104435,  65401,  30863, 141808,  39997, 145932,  46149, 125167, 132902,\n",
       "         117864]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data['Sentiment'][117864-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10.], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 308])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.phrase_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId                                                       193462\n",
       "SentenceId                                                      10289\n",
       "Phrase                         you 're not into the Pokemon franchise\n",
       "Phrase_length                                                      72\n",
       "Tokenized_phrase    [xxbos, you, 're, not, into, the, xxmaj, pokem...\n",
       "Indexed_phrase               [2, 33, 157, 41, 61, 8, 7, 2893, 978, 3]\n",
       "Name: 37401, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.iloc[37401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "        -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "         1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "        -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "         6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "        -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "        -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "         8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "        -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "         1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "         5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "        -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "         1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "        -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "         2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "        -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "         3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "        -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "        -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "         1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "        -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "        -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "        -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "         7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "         6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "        -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "        -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "        -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "        -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "        -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "         4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "         3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "        -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "        -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "         7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "        -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "         8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "         8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "         3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "         1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "         1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "        -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "         2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "         4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "        -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "        -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "        -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "         2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "         2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "         1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "        -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "         1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "         3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "         1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "        -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "        -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "         1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "        -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "        -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "         1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[63][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "       -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "        1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "       -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "        6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "       -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "       -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "        8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "       -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "        1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "        5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "       -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "        1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "       -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "        2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "       -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "        3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "       -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "       -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "        1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "       -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "       -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "       -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "        7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "        6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "       -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "       -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "       -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "       -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "       -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "        4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "        3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "       -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "       -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "        7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "       -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "        8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "        8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "        3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "        1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "        1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "       -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "        2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "        4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "       -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "       -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "       -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "        2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "        2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "        1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "       -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "        1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "        3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "        1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "       -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "       -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "        1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "       -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "       -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "        1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector(\"'re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU( embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(text, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        #if self.bidirectional:\n",
    "        #    hidden = [batch size, hid dim * num directions]\n",
    "        #else:\n",
    "        #    hidden = [batch size, hid dim]\n",
    "        \n",
    "        # with RELU\n",
    "        #return self.fc(self.relu(hidden))\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 308\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "# Regularization hyperparameter\n",
    "DROPOUT = 0\n",
    "L2_LAMBDA = 0.001\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "MODEL_SAVE_FILE = 'simple-GRU_origin.pt'\n",
    "model = GRU(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,054,661 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define epoch time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01463 | Train Acc: 61.08%\n",
      "\t Val. Loss: 0.01407 |  Val. Acc: 62.82%\n",
      "Epoch: 02 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01381 | Train Acc: 63.02%\n",
      "\t Val. Loss: 0.01362 |  Val. Acc: 63.98%\n",
      "Epoch: 03 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01348 | Train Acc: 63.87%\n",
      "\t Val. Loss: 0.01348 |  Val. Acc: 64.23%\n",
      "Epoch: 04 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01331 | Train Acc: 64.46%\n",
      "\t Val. Loss: 0.01339 |  Val. Acc: 64.38%\n",
      "Epoch: 05 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01316 | Train Acc: 64.69%\n",
      "\t Val. Loss: 0.01332 |  Val. Acc: 64.33%\n",
      "Epoch: 06 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01307 | Train Acc: 64.96%\n",
      "\t Val. Loss: 0.01349 |  Val. Acc: 63.78%\n",
      "Epoch: 07 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.01298 | Train Acc: 65.33%\n",
      "\t Val. Loss: 0.01320 |  Val. Acc: 64.16%\n",
      "Epoch: 08 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.01291 | Train Acc: 65.39%\n",
      "\t Val. Loss: 0.01305 |  Val. Acc: 65.31%\n",
      "Epoch: 09 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01285 | Train Acc: 65.52%\n",
      "\t Val. Loss: 0.01309 |  Val. Acc: 65.00%\n",
      "Epoch: 10 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.01278 | Train Acc: 65.79%\n",
      "\t Val. Loss: 0.01330 |  Val. Acc: 64.87%\n",
      "Epoch: 11 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01276 | Train Acc: 65.79%\n",
      "\t Val. Loss: 0.01305 |  Val. Acc: 65.17%\n",
      "Epoch: 12 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01271 | Train Acc: 65.84%\n",
      "\t Val. Loss: 0.01297 |  Val. Acc: 65.62%\n",
      "Epoch: 13 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01268 | Train Acc: 65.97%\n",
      "\t Val. Loss: 0.01288 |  Val. Acc: 65.72%\n",
      "Epoch: 14 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.01264 | Train Acc: 66.15%\n",
      "\t Val. Loss: 0.01292 |  Val. Acc: 65.49%\n",
      "Epoch: 15 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01262 | Train Acc: 66.15%\n",
      "\t Val. Loss: 0.01309 |  Val. Acc: 64.75%\n",
      "Epoch: 16 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01258 | Train Acc: 66.35%\n",
      "\t Val. Loss: 0.01305 |  Val. Acc: 65.49%\n",
      "Epoch: 17 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01258 | Train Acc: 66.24%\n",
      "\t Val. Loss: 0.01289 |  Val. Acc: 65.94%\n",
      "Epoch: 18 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.01254 | Train Acc: 66.48%\n",
      "\t Val. Loss: 0.01292 |  Val. Acc: 65.69%\n",
      "Epoch: 19 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01253 | Train Acc: 66.58%\n",
      "\t Val. Loss: 0.01281 |  Val. Acc: 65.83%\n",
      "Epoch: 20 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.01249 | Train Acc: 66.61%\n",
      "\t Val. Loss: 0.01279 |  Val. Acc: 65.91%\n",
      "Epoch: 21 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.01250 | Train Acc: 66.57%\n",
      "\t Val. Loss: 0.01274 |  Val. Acc: 66.24%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ecaec7ca698f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-6b206d8692c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, set_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "# For splotting\n",
    "all_train_losses = []\n",
    "all_valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, len(train_data))\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), saved_models_path + MODEL_SAVE_FILE)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    all_train_losses.append(train_loss)\n",
    "    all_valid_losses.append(valid_loss)\n",
    "    \n",
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# plt.xticks(range(0, 10))\n",
    "plt.plot(all_train_losses)\n",
    "plt.plot(all_valid_losses)\n",
    "plt.axvline(x=best_epoch, color='green')\n",
    "\n",
    "plt.legend(['train loss', 'valid loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 08\n"
     ]
    }
   ],
   "source": [
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01187 | Test Acc: 68.81%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, len(test_data))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.5f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reevaluate on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01172 | Test Acc: 69.20%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "\n",
    "print(f'Test Loss: {valid_loss:.5f} | Test Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Kaggle Dataset and create submittion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model target file\n",
    "MODEL_SAVE_FILE_TARGET = 'simple-GRU-valid-69.59.pt'\n",
    "\n",
    "def predict_kaggle_test(model, iterator):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            predictions = predictions.argmax(1)\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                result.append([batch.id[0][i].item(), [batch.phrase_id[0][i].item(), predictions[i].item()]] )\n",
    "    \n",
    "    result.sort(key = lambda val: val[0])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [156061, 2]],\n",
       " [1, [156062, 2]],\n",
       " [2, [156063, 2]],\n",
       " [3, [156064, 2]],\n",
       " [4, [156065, 2]],\n",
       " [5, [156066, 2]],\n",
       " [6, [156067, 3]],\n",
       " [7, [156068, 2]],\n",
       " [8, [156069, 3]],\n",
       " [9, [156070, 2]]]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE_TARGET))\n",
    "\n",
    "kaggle_result_list = predict_kaggle_test(model, kaggle_test_iterator)\n",
    "\n",
    "kaggle_result_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66282, [222343, 2]],\n",
       " [66283, [222344, 2]],\n",
       " [66284, [222345, 2]],\n",
       " [66285, [222346, 2]],\n",
       " [66286, [222347, 2]],\n",
       " [66287, [222348, 0]],\n",
       " [66288, [222349, 0]],\n",
       " [66289, [222350, 1]],\n",
       " [66290, [222351, 1]],\n",
       " [66291, [222352, 1]]]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_result_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66282</th>\n",
       "      <td>222343</td>\n",
       "      <td>11854</td>\n",
       "      <td>should have called it Gutterball</td>\n",
       "      <td>14</td>\n",
       "      <td>[xxbos, should, have, called, it, xxmaj, gutte...</td>\n",
       "      <td>[2, 156, 49, 775, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66283</th>\n",
       "      <td>222344</td>\n",
       "      <td>11854</td>\n",
       "      <td>have called it Gutterball</td>\n",
       "      <td>65</td>\n",
       "      <td>[xxbos, have, called, it, xxmaj, gutterball, x...</td>\n",
       "      <td>[2, 49, 775, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66284</th>\n",
       "      <td>222345</td>\n",
       "      <td>11854</td>\n",
       "      <td>called it Gutterball</td>\n",
       "      <td>63</td>\n",
       "      <td>[xxbos, called, it, xxmaj, gutterball, xxeos]</td>\n",
       "      <td>[2, 775, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66285</th>\n",
       "      <td>222346</td>\n",
       "      <td>11854</td>\n",
       "      <td>it Gutterball</td>\n",
       "      <td>38</td>\n",
       "      <td>[xxbos, it, xxmaj, gutterball, xxeos]</td>\n",
       "      <td>[2, 20, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66286</th>\n",
       "      <td>222347</td>\n",
       "      <td>11854</td>\n",
       "      <td>Gutterball</td>\n",
       "      <td>34</td>\n",
       "      <td>[xxbos, xxmaj, gutterball, xxeos]</td>\n",
       "      <td>[2, 7, 0, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66287</th>\n",
       "      <td>222348</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario .</td>\n",
       "      <td>32</td>\n",
       "      <td>[xxbos, a, long, -, winded, ,, predictable, sc...</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 9, 390, 2021, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66288</th>\n",
       "      <td>222349</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded , predictable scenario</td>\n",
       "      <td>29</td>\n",
       "      <td>[xxbos, a, long, -, winded, ,, predictable, sc...</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 9, 390, 2021, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66289</th>\n",
       "      <td>222350</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded ,</td>\n",
       "      <td>20</td>\n",
       "      <td>[xxbos, a, long, -, winded, ,, xxeos]</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 9, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66290</th>\n",
       "      <td>222351</td>\n",
       "      <td>11855</td>\n",
       "      <td>A long-winded</td>\n",
       "      <td>24</td>\n",
       "      <td>[xxbos, a, long, -, winded, xxeos]</td>\n",
       "      <td>[2, 10, 139, 13, 5931, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66291</th>\n",
       "      <td>222352</td>\n",
       "      <td>11855</td>\n",
       "      <td>predictable scenario</td>\n",
       "      <td>21</td>\n",
       "      <td>[xxbos, predictable, scenario, xxeos]</td>\n",
       "      <td>[2, 390, 2021, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId                                  Phrase  \\\n",
       "66282    222343       11854        should have called it Gutterball   \n",
       "66283    222344       11854               have called it Gutterball   \n",
       "66284    222345       11854                    called it Gutterball   \n",
       "66285    222346       11854                           it Gutterball   \n",
       "66286    222347       11854                              Gutterball   \n",
       "66287    222348       11855  A long-winded , predictable scenario .   \n",
       "66288    222349       11855    A long-winded , predictable scenario   \n",
       "66289    222350       11855                         A long-winded ,   \n",
       "66290    222351       11855                           A long-winded   \n",
       "66291    222352       11855                    predictable scenario   \n",
       "\n",
       "       Phrase_length                                   Tokenized_phrase  \\\n",
       "66282             14  [xxbos, should, have, called, it, xxmaj, gutte...   \n",
       "66283             65  [xxbos, have, called, it, xxmaj, gutterball, x...   \n",
       "66284             63      [xxbos, called, it, xxmaj, gutterball, xxeos]   \n",
       "66285             38              [xxbos, it, xxmaj, gutterball, xxeos]   \n",
       "66286             34                  [xxbos, xxmaj, gutterball, xxeos]   \n",
       "66287             32  [xxbos, a, long, -, winded, ,, predictable, sc...   \n",
       "66288             29  [xxbos, a, long, -, winded, ,, predictable, sc...   \n",
       "66289             20              [xxbos, a, long, -, winded, ,, xxeos]   \n",
       "66290             24                 [xxbos, a, long, -, winded, xxeos]   \n",
       "66291             21              [xxbos, predictable, scenario, xxeos]   \n",
       "\n",
       "                                    Indexed_phrase  \n",
       "66282               [2, 156, 49, 775, 20, 7, 0, 3]  \n",
       "66283                    [2, 49, 775, 20, 7, 0, 3]  \n",
       "66284                        [2, 775, 20, 7, 0, 3]  \n",
       "66285                             [2, 20, 7, 0, 3]  \n",
       "66286                                 [2, 7, 0, 3]  \n",
       "66287  [2, 10, 139, 13, 5931, 9, 390, 2021, 15, 3]  \n",
       "66288      [2, 10, 139, 13, 5931, 9, 390, 2021, 3]  \n",
       "66289                 [2, 10, 139, 13, 5931, 9, 3]  \n",
       "66290                    [2, 10, 139, 13, 5931, 3]  \n",
       "66291                            [2, 390, 2021, 3]  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_EXTENSION = '.submit.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(saved_models_path + MODEL_SAVE_FILE_TARGET + CSV_EXTENSION, mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    csv_writer.writerow(['PhraseId', 'Sentiment'])\n",
    "    \n",
    "    for i in range(len(kaggle_result_list)):\n",
    "        csv_writer.writerow(kaggle_result_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
