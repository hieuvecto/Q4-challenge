{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer, dtype_to_attr, is_tokenizer_serializable\n",
    "from torchtext import data\n",
    "\n",
    "class TensorField(data.Field):\n",
    "    def __init__(self, sequential=True, use_vocab=True, init_token=None,\n",
    "                 eos_token=None, fix_length=None, dtype=torch.long,\n",
    "                 preprocessing=None, postprocessing=None, lower=False,\n",
    "                 tokenize=None, tokenizer_language='en', include_lengths=False,\n",
    "                 batch_first=False, pad_token=\"<pad>\", unk_token=\"<unk>\",\n",
    "                 pad_first=False, truncate_first=False, stop_words=None,\n",
    "                 is_target=False):\n",
    "        self.sequential = sequential\n",
    "        self.use_vocab = use_vocab\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "        self.fix_length = fix_length\n",
    "        self.dtype = dtype\n",
    "        self.preprocessing = preprocessing\n",
    "        self.postprocessing = postprocessing\n",
    "        self.lower = lower\n",
    "        # store params to construct tokenizer for serialization\n",
    "        # in case the tokenizer isn't picklable (e.g. spacy)\n",
    "        self.tokenizer_args = (tokenize, tokenizer_language)\n",
    "        self.tokenize = get_tokenizer(tokenize, tokenizer_language)\n",
    "        self.include_lengths = include_lengths\n",
    "        self.batch_first = batch_first\n",
    "        self.pad_token = pad_token\n",
    "        self.pad_first = pad_first\n",
    "        self.truncate_first = truncate_first\n",
    "        try:\n",
    "            self.stop_words = set(stop_words) if stop_words is not None else None\n",
    "        except TypeError:\n",
    "            raise ValueError(\"Stop words must be convertible to a set\")\n",
    "        self.is_target = is_target\n",
    "        \n",
    "    def pad(self, minibatch):\n",
    "        \"\"\"Pad a batch of examples using this field.\n",
    "\n",
    "        Pads to self.fix_length if provided, otherwise pads to the length of\n",
    "        the longest example in the batch. Prepends self.init_token and appends\n",
    "        self.eos_token if those attributes are not None. Returns a tuple of the\n",
    "        padded list and a list containing lengths of each example if\n",
    "        `self.include_lengths` is `True` and `self.sequential` is `True`, else just\n",
    "        returns the padded list. If `self.sequential` is `False`, no padding is applied.\n",
    "        \"\"\"\n",
    "        minibatch = list(minibatch)\n",
    "#         if not self.sequential:\n",
    "#             return minibatch\n",
    "        if self.fix_length is None:\n",
    "            max_len = max(len(x) for x in minibatch)\n",
    "        else:\n",
    "            max_len = self.fix_length + (\n",
    "                self.init_token, self.eos_token).count(None) - 2\n",
    "        padded, lengths = [], []\n",
    "\n",
    "        for x in minibatch:\n",
    "            if self.pad_first:\n",
    "                padded.append(\n",
    "                    [self.pad_token] * max(0, max_len - len(x))\n",
    "                    + ([] if self.init_token is None else [self.init_token])\n",
    "                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n",
    "                    + ([] if self.eos_token is None else [self.eos_token]))\n",
    "            else:\n",
    "                padded.append(\n",
    "                    ([] if self.init_token is None else [self.init_token])\n",
    "                    + list(x[-max_len:] if self.truncate_first else x[:max_len])\n",
    "                    + ([] if self.eos_token is None else [self.eos_token])\n",
    "                    + [self.pad_token] * max(0, max_len - len(x)))\n",
    "            lengths.append(len(padded[-1]) - max(0, max_len - len(x)))\n",
    "        if self.include_lengths:\n",
    "            return (padded, lengths)\n",
    "        return padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
