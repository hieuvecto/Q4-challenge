{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load config and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import spacy, pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "# Custom impport\n",
    "from common.common_classes import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'prepare-word-embedding-nlp.ipynb', 'tokenization.ipynb', 'test-batching-padding.ipynb', 'train.tsv', 'test-batching-padding-ok.ipynb', 'LSTM-implement.ipynb', 'sampleSubmission.csv', 'save_data', '.ipynb_checkpoints', '__init__.py', 'README.md', '.gitignore', '.git', 'common', 'simple-GRU-implement.ipynb']\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "save_data_path = path + 'save_data/'\n",
    "large_save_data_path = '/notebooks/large-storage/'\n",
    "saved_models_path = '/notebooks/large-storage/saved-models/'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(save_data_path + 'pre-processed-data.pkl', 'rb'))\n",
    "loaded_kaggle_test = pickle.load(open(save_data_path + 'pre-processed-kaggle-test.pkl', 'rb'))\n",
    "loaded_vocab = pickle.load(open(save_data_path + 'genereated-vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, a, series, xxeos]</td>\n",
       "      <td>[2, 10, 341, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, a, xxeos]</td>\n",
       "      <td>[2, 10, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, series, xxeos]</td>\n",
       "      <td>[2, 341, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  Phrase_length  \\\n",
       "0          1            188   \n",
       "1          2             77   \n",
       "2          2              8   \n",
       "3          2              1   \n",
       "4          2              6   \n",
       "\n",
       "                                    Tokenized_phrase  \\\n",
       "0  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "1  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "2                          [xxbos, a, series, xxeos]   \n",
       "3                                  [xxbos, a, xxeos]   \n",
       "4                             [xxbos, series, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "1  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "2                                    [2, 10, 341, 3]  \n",
       "3                                         [2, 10, 3]  \n",
       "4                                        [2, 341, 3]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "MAX_LABEL = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoding and prepraing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(large_save_data_path + 'process-spacy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[BOS].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab.get_vector('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890280, 308)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "PHRASE_ID = data.Field(use_vocab = False)\n",
    "TEXT = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)\n",
    "LABEL = data.LabelField(use_vocab = False, dtype=torch.long)\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "ID_TEST = data.Field(use_vocab = False)\n",
    "PHRASE_ID_TEST = data.Field(use_vocab = False)\n",
    "TEXT_TEST = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "fields = [('id', PHRASE_ID), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "fields_test = [('id', ID_TEST), ('phrase_id', PHRASE_ID_TEST), ('text', TEXT_TEST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_kaggle_test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7fc014922278>,\n",
       " <torchtext.data.example.Example at 0x7fc014922320>,\n",
       " <torchtext.data.example.Example at 0x7fc014922358>,\n",
       " <torchtext.data.example.Example at 0x7fc014922390>,\n",
       " <torchtext.data.example.Example at 0x7fc0149223c8>,\n",
       " <torchtext.data.example.Example at 0x7fc014922400>,\n",
       " <torchtext.data.example.Example at 0x7fc014922438>,\n",
       " <torchtext.data.example.Example at 0x7fc014922470>,\n",
       " <torchtext.data.example.Example at 0x7fc0149224a8>,\n",
       " <torchtext.data.example.Example at 0x7fc0149224e0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Train dataset\n",
    "examples = []\n",
    "length = len(loaded_data['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_data['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_data['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_data['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples.append(data.Example.fromlist([ [loaded_data['PhraseId'][i]], embedded, loaded_data['Sentiment'][i]], fields))\n",
    "    \n",
    "examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7fc02e671e48>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671e80>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671eb8>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671ef0>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671f28>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671f60>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671f98>,\n",
       " <torchtext.data.example.Example at 0x7fc02e671fd0>,\n",
       " <torchtext.data.example.Example at 0x7fbfbd3ff048>,\n",
       " <torchtext.data.example.Example at 0x7fbfbd3ff080>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Test dataset\n",
    "examples_test = []\n",
    "length = len(loaded_kaggle_test['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_kaggle_test['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples_test.append(data.Example.fromlist([ [i], [loaded_kaggle_test['PhraseId'][i]], embedded ], fields_test))\n",
    "    \n",
    "examples_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "        -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "        -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "        -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "        -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "         3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "         3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "        -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "         3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "        -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "        -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "         3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "        -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "        -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "         1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "        -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "        -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "         6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "        -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "         4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "        -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "        -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "         2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "         2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "         1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "         1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "        -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "        -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "        -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "        -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "        -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "         4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "        -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "        -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "         1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "        -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "        -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "         4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "        -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "         1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "         1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "         7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "        -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "         7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "        -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "        -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "        -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "        -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "         8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "        -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "        -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "         3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "        -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "        -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "         3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "        -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "         4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "        -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "        -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "         8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.7793e-01, -6.2470e-02,  1.9596e-01, -4.3340e-02, -1.4570e-01,\n",
       "         6.7455e-02, -7.9048e-01, -4.7327e-01, -1.2694e-02,  9.4032e-01,\n",
       "         9.7596e-01,  3.6356e-01, -4.2334e-01, -4.7300e-01,  4.3313e-02,\n",
       "        -1.7159e-02,  7.7286e-02,  5.1450e-01, -1.3013e-01, -3.8677e-01,\n",
       "         8.5337e-02, -1.1537e-01, -2.9981e-01,  1.0327e-01,  7.4443e-02,\n",
       "         3.8978e-02, -1.0280e-01, -2.7481e-01, -2.9409e-01, -5.2790e-01,\n",
       "        -5.4200e-01, -4.1610e-01, -7.6345e-01, -2.1057e-01, -6.0277e-01,\n",
       "        -3.7840e-01, -5.1185e-01,  5.4210e-02, -8.2146e-02,  5.3257e-01,\n",
       "         3.3352e-01,  1.6082e-01, -1.6897e-02,  3.3638e-01, -1.3660e-01,\n",
       "         4.2157e-01, -2.5176e-01, -3.2271e-01, -3.9742e-01, -3.2712e-01,\n",
       "         3.5235e-01,  5.4523e-01, -5.8145e-01, -2.5093e-01, -4.2526e-01,\n",
       "         4.7148e-04,  5.4579e-01, -4.9980e-01,  2.3473e-02, -1.0801e-01,\n",
       "        -2.5952e-01, -7.5854e-03, -2.0041e-01, -3.7743e-01, -3.0881e-01,\n",
       "        -7.4719e-02,  3.9462e-03, -3.6002e-01, -7.1041e-02,  2.8597e-01,\n",
       "         1.3777e-01,  1.6269e-01, -1.0977e-01, -3.6319e-01,  8.5886e-02,\n",
       "         2.0098e-01,  1.9972e-01,  3.2699e-01, -1.1095e-01, -2.4142e-01,\n",
       "         1.1838e-01, -2.1962e-01, -6.1894e-01, -1.7774e-01, -1.5444e-01,\n",
       "         7.2033e-02,  7.5290e-01,  1.7973e-01,  3.2366e-01,  6.2210e-01,\n",
       "        -1.1795e-01, -2.4438e-01,  2.0690e-01,  4.0958e-01, -3.5867e-02,\n",
       "        -2.7432e-01,  3.3013e-01,  1.8814e-01,  2.6837e-01, -1.8496e-01,\n",
       "         6.3260e-02, -3.3311e-03, -5.4875e-02,  2.3964e-01, -1.4294e-01,\n",
       "        -1.9737e+00, -1.7595e-01, -7.4150e-02,  3.7287e-01,  1.6554e-01,\n",
       "         2.3770e-01, -1.4556e-01,  4.5459e-01, -8.1550e-02, -3.3170e-01,\n",
       "        -8.1371e-01, -1.7414e-01,  1.3691e-01, -6.0061e-01,  4.0306e-01,\n",
       "        -4.3174e-01, -1.7259e-01,  5.2948e-02, -2.4818e-01,  1.5827e-01,\n",
       "        -1.1220e-01,  3.2860e-02,  5.5025e-01,  2.0649e-01,  1.0403e-01,\n",
       "         4.3973e-01, -1.1043e-01, -2.3429e-01,  4.2473e-01, -2.0461e-01,\n",
       "        -2.3912e-01,  2.7573e-01,  6.4947e-01, -3.4708e-01, -1.4163e-01,\n",
       "        -1.7743e-01,  1.7144e-01, -1.7009e-01, -1.2667e-01, -5.6574e-02,\n",
       "         6.9985e-01,  9.3176e-02, -1.1523e-01,  7.9450e-02,  5.3647e-02,\n",
       "        -4.2493e-01,  7.1772e-01,  2.3705e-01,  2.1213e-01, -1.9776e-02,\n",
       "        -1.9379e-02,  2.9703e-01, -2.7536e-01,  7.3583e-02, -5.1954e-01,\n",
       "        -1.5928e-01, -3.5374e-01,  4.5916e-01,  2.0435e-01,  9.7278e-04,\n",
       "         4.6904e-01, -6.2419e-01,  3.2797e-01, -1.5744e-01, -8.8403e-02,\n",
       "         3.5625e-02, -1.2245e-01,  3.6698e-01, -8.8524e-02, -5.5171e-02,\n",
       "         6.0885e-01,  5.3027e-01, -3.3558e-01,  1.5178e-01, -1.6619e-01,\n",
       "         5.0028e-01, -1.4728e-02, -1.4555e-01,  5.6033e-01,  2.4656e-01,\n",
       "        -3.6279e-01, -7.8195e-04,  9.5365e-02,  5.8955e-02, -4.8528e-01,\n",
       "         5.4453e-01, -1.1293e-02, -1.5423e-01,  3.7746e-01, -4.1018e-01,\n",
       "        -7.5968e-02,  3.2843e-01, -1.0195e-01, -6.5580e-01, -5.7961e-01,\n",
       "        -4.3063e-01,  5.0521e-01,  1.4637e-02,  1.8917e-01,  5.0427e-02,\n",
       "        -1.2479e-01, -1.7724e-01, -1.0494e-01,  3.2128e-01,  5.9077e-02,\n",
       "         2.3804e-01, -6.3107e-02,  3.3216e-02, -2.0066e-01, -2.7485e-01,\n",
       "        -2.8084e-01,  4.9871e-01,  2.0036e-01,  2.9303e-01, -3.4742e-01,\n",
       "         4.9861e-01,  2.3172e-01,  1.4505e-01, -7.2846e-02, -8.4557e-02,\n",
       "        -1.5358e-01,  5.1501e-01, -1.7232e-01,  2.7167e-01, -2.3813e-01,\n",
       "         1.3970e-01,  2.5681e-01,  4.1433e-01,  3.3110e-01, -3.7062e-02,\n",
       "        -6.8761e-02,  6.5520e-02, -3.1337e-01, -3.6793e-02, -1.3847e-01,\n",
       "         2.2251e-01,  3.2041e-01,  3.1190e-01,  2.7103e-01, -4.0910e-01,\n",
       "         1.0762e-01,  1.6541e-01,  3.7305e-01,  2.1920e-01, -4.5466e-01,\n",
       "        -2.6570e-01, -4.3871e-01,  2.7044e-01,  2.0953e-01,  3.1834e-01,\n",
       "        -4.3875e-01,  2.4935e-01,  4.6443e-02, -3.2949e-01,  2.1793e-01,\n",
       "         7.3772e-01,  3.9756e-02, -5.0169e-02, -1.8881e-01,  7.7245e-02,\n",
       "         1.4199e-01,  3.9659e-02,  1.2333e-01, -9.6613e-02, -2.2687e-01,\n",
       "         4.8893e-01, -1.1373e-01,  5.1246e-02,  7.5477e-02, -1.2750e-01,\n",
       "        -1.0697e-01, -4.8071e-01, -2.6223e-01,  4.5893e-01,  3.6267e-01,\n",
       "         2.9100e-01,  1.7118e-02, -2.0185e-01, -7.4050e-02, -2.3640e-01,\n",
       "         2.8046e-01,  1.8657e-01,  8.7665e-02,  3.1472e-01, -7.0119e-02,\n",
       "        -1.8172e-01, -3.9074e-02,  2.8063e-02, -4.9746e-02,  1.0355e-01,\n",
       "        -1.5258e-01,  3.4939e-01, -1.6108e-01, -6.0705e-02, -9.0380e-03,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-0.2712   ,  0.14702  , -0.19126  , -0.24424  ,  0.16476  ,\n",
       "        -0.32537  ,  0.36027  ,  0.29804  , -0.49571  ,  1.2244   ,\n",
       "         0.097696 ,  0.30406  , -0.53059  , -0.25869  ,  0.16519  ,\n",
       "        -0.41392  , -0.2314   ,  0.86309  ,  0.35309  ,  0.41004  ,\n",
       "        -0.35114  ,  0.21699  ,  0.30512  , -0.30977  ,  0.065359 ,\n",
       "         0.13193  ,  0.59324  ,  0.089948 , -0.14227  , -0.31061  ,\n",
       "        -0.08432  ,  0.21609  , -0.57332  , -0.076418 , -0.0041399,\n",
       "        -0.48229  , -0.11635  , -0.0089557, -0.0064539, -0.031811 ,\n",
       "         0.16104  , -0.11247  , -0.042646 , -0.026045 ,  0.097288 ,\n",
       "        -0.030434 , -0.45127  , -0.028487 , -0.14567  , -0.10633  ,\n",
       "        -0.17795  , -0.097141 , -0.15773  , -0.42128  ,  0.25669  ,\n",
       "         0.53326  , -0.027647 ,  0.24529  , -0.46835  ,  0.090866 ,\n",
       "         0.12265  , -0.63001  , -0.25682  ,  0.0060041, -0.3454   ,\n",
       "         0.0072695,  0.17178  , -0.39181  , -0.060693 , -0.031147 ,\n",
       "         0.20888  ,  0.30827  ,  0.035736 , -0.23439  ,  0.28267  ,\n",
       "         0.08706  , -0.057023 , -0.30681  , -0.38813  , -0.039169 ,\n",
       "        -0.059306 , -0.36394  ,  0.26486  ,  0.29665  ,  0.12684  ,\n",
       "        -0.059682 ,  0.18076  ,  1.0117   , -0.020038 , -0.16224  ,\n",
       "        -0.069924 ,  0.29389  ,  0.023328 ,  0.67408  ,  0.13449  ,\n",
       "         0.021691 ,  0.21811  , -0.40404  , -0.13521  ,  0.11014  ,\n",
       "         0.19624  ,  0.05419  ,  0.16292  , -0.17251  ,  0.27033  ,\n",
       "        -1.616    ,  0.60791  ,  0.0033667, -0.29159  ,  0.14195  ,\n",
       "        -0.075748 , -0.19854  ,  0.27378  ,  0.087632 , -0.023092 ,\n",
       "        -0.24036  , -0.20799  , -0.062753 ,  0.040184 ,  0.032635 ,\n",
       "        -0.2035   ,  0.15727  , -0.67543  ,  0.67401  , -0.005282 ,\n",
       "         0.067059 ,  0.20846  , -0.65743  ,  0.17864  , -0.27987  ,\n",
       "        -0.022082 , -0.20144  ,  0.13538  , -0.12531  , -0.39399  ,\n",
       "         0.3964   , -0.11625  , -0.34535  ,  0.25273  ,  0.24191  ,\n",
       "        -1.6655   ,  0.61688  , -0.0035128,  0.37053  , -0.32058  ,\n",
       "        -0.48533  ,  0.1568   , -0.086594 ,  0.13272  , -0.050194 ,\n",
       "         0.12197  ,  0.0621   , -0.052821 ,  0.13192  ,  0.26901  ,\n",
       "         0.26882  , -0.47454  , -0.28357  ,  0.16668  , -0.51742  ,\n",
       "        -0.2906   ,  0.25908  ,  0.71942  ,  0.50009  , -0.092692 ,\n",
       "        -0.36577  , -0.034634 ,  0.056318 , -0.21117  ,  0.37486  ,\n",
       "         0.032773 , -0.14359  ,  0.6884   , -0.026806 ,  0.42897  ,\n",
       "         0.053205 , -0.42011  , -0.25344  , -0.21894  ,  0.14449  ,\n",
       "         0.045221 , -0.19444  ,  0.40097  ,  0.42644  , -0.39018  ,\n",
       "        -0.12735  ,  0.11559  ,  0.019183 , -0.027092 , -0.44417  ,\n",
       "         0.045983 ,  0.30359  ,  0.22169  ,  0.060414 , -0.0059974,\n",
       "        -0.66994  , -0.058462 ,  0.16644  , -0.1518   ,  0.0072898,\n",
       "        -0.21948  ,  0.45436  ,  0.05823  , -0.31103  , -0.23067  ,\n",
       "         0.0078952, -0.30799  ,  0.10996  ,  0.28958  , -0.033313 ,\n",
       "        -0.46981  ,  0.17656  , -0.077724 , -0.44687  , -0.13734  ,\n",
       "         0.44273  ,  0.43806  , -0.32096  ,  0.65241  , -0.29144  ,\n",
       "         0.35948  , -0.39277  ,  0.078458 ,  0.6577   ,  0.45683  ,\n",
       "         0.093801 , -0.25023  ,  0.046448 ,  0.04619  , -0.14565  ,\n",
       "         0.17054  ,  0.072038 , -0.86523  ,  0.20702  ,  0.45185  ,\n",
       "         0.20958  ,  0.043511 , -0.17528  , -0.15821  ,  0.011212 ,\n",
       "         0.41577  , -0.31444  , -0.17748  ,  0.089159 , -0.34662  ,\n",
       "        -0.097137 , -0.28784  , -0.38902  ,  0.21508  ,  0.31001  ,\n",
       "         0.030091 ,  0.30658  ,  0.23107  ,  0.048666 ,  0.13822  ,\n",
       "         0.4905   ,  0.66801  , -0.023158 ,  0.38607  ,  0.1293   ,\n",
       "        -0.027527 , -0.31134  ,  0.31469  ,  0.26038  ,  0.3263   ,\n",
       "         0.087236 , -0.014318 , -0.080271 , -0.14507  ,  0.34931  ,\n",
       "        -0.69845  , -0.25372  , -0.28831  , -0.038035 , -0.59529  ,\n",
       "         0.17554  ,  0.78562  , -0.066821 , -0.062887 ,  0.28903  ,\n",
       "         0.82979  ,  0.33097  ,  0.17223  , -0.27315  , -0.5014   ,\n",
       "        -0.29307  ,  0.43277  ,  0.59082  ,  0.059657 ,  0.19892  ,\n",
       "        -0.39105  , -0.020178 , -0.23956  ,  0.26956  ,  0.15734  ,\n",
       "         0.049013 ,  0.2598   , -0.15101  ,  0.069548 ,  0.058311 ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ], dtype=float32),\n",
       " array([-1.6890e-02,  1.7402e-01, -3.0247e-01, -3.0063e-01,  2.1415e-01,\n",
       "         6.3863e-02,  1.0107e-01, -2.4155e-01, -9.5228e-02,  2.9253e+00,\n",
       "        -5.6759e-03,  1.1752e-01,  1.6121e-01,  2.0813e-02, -8.3593e-02,\n",
       "        -1.4048e-01, -4.0069e-02,  7.5133e-01, -5.6699e-02, -9.6198e-02,\n",
       "        -7.2134e-02, -3.1287e-02,  1.8146e-01, -2.4846e-01, -6.5068e-02,\n",
       "         6.6374e-02, -1.2173e-01, -1.3479e-01,  2.6163e-01, -2.1599e-01,\n",
       "        -2.4221e-01,  9.1074e-02, -4.3504e-02, -1.8047e-01, -1.8158e-01,\n",
       "        -6.6229e-02,  2.6480e-02, -2.5439e-01, -7.8805e-02, -2.1853e-01,\n",
       "        -3.0239e-02,  1.3049e-01,  1.0992e-01,  3.5563e-02,  3.2769e-01,\n",
       "        -2.7750e-02, -1.8852e-01, -1.8579e-01, -8.5064e-02, -2.4057e-02,\n",
       "        -1.4141e-01,  1.2708e-01, -7.9024e-02, -1.3320e-01,  3.7619e-02,\n",
       "        -1.1080e-01, -2.2742e-01, -2.7703e-01, -8.1760e-02,  4.7761e-02,\n",
       "        -1.7936e-01, -3.1907e-01,  1.3931e-01,  3.6374e-01, -2.1399e-01,\n",
       "        -2.7044e-01, -2.0847e-01,  1.1192e-02,  1.6017e-01,  2.4218e-01,\n",
       "         2.5058e-01,  3.2767e-01,  1.3340e-01,  6.4510e-03,  1.7994e-02,\n",
       "         1.1242e-01,  9.0136e-02, -7.2882e-02, -1.6506e-01,  4.8300e-01,\n",
       "         4.6465e-03, -4.8611e-02,  2.7421e-02,  9.7357e-02,  1.0873e-01,\n",
       "        -3.1722e-01,  2.6092e-01, -5.9396e-01,  2.6522e-01,  7.2456e-02,\n",
       "        -1.9246e-01,  3.4932e-02, -1.5662e-01,  3.0779e-01,  3.3744e-01,\n",
       "        -4.8664e-03,  1.5165e-01, -1.9861e-02, -1.4789e-01, -1.7031e-02,\n",
       "        -1.7279e-01, -3.8278e-02, -3.2560e-01, -1.6450e-01,  2.2257e-01,\n",
       "        -1.2442e+00, -1.4105e-01,  7.4932e-02, -1.8879e-01, -1.0341e-01,\n",
       "        -3.6646e-03, -2.1667e-01,  3.5087e-02, -1.9168e-01, -1.0044e-01,\n",
       "        -2.8554e-03,  5.6079e-02, -7.4098e-02, -8.3292e-03, -2.3693e-01,\n",
       "         4.1375e-02,  6.9771e-02,  2.0383e-01, -1.8666e-01, -4.2077e-02,\n",
       "         1.3305e-01, -4.5272e-02, -2.9631e-01,  1.6454e-01, -1.1591e-01,\n",
       "         2.4631e-02, -6.1717e-02, -6.2958e-02,  1.3487e-01,  2.0565e-01,\n",
       "         1.4678e-02,  1.9056e-01, -3.2541e-01,  1.0896e-01, -9.1117e-02,\n",
       "        -1.8126e+00,  3.5567e-01,  5.4440e-01, -3.5205e-02, -2.7339e-02,\n",
       "        -1.0750e-01, -3.0940e-01,  1.2539e-01,  8.7296e-02,  8.2909e-02,\n",
       "        -7.9026e-02, -1.8092e-02,  2.6749e-01,  1.1947e-02, -1.3090e-01,\n",
       "        -3.3304e-01, -2.0343e-01, -2.8020e-01, -1.0974e-01, -2.9613e-01,\n",
       "        -1.3973e-01,  1.8934e-01,  3.5228e-02, -2.5916e-01, -2.6859e-01,\n",
       "        -2.2678e-01,  1.3141e-01,  5.1456e-02,  2.6723e-01, -1.8335e-01,\n",
       "        -7.5100e-02, -1.6394e-02, -9.9408e-02, -1.8685e-01, -1.4718e-01,\n",
       "         6.4933e-02, -1.3858e-01, -1.9941e-01, -8.3017e-02, -1.7141e-01,\n",
       "         1.4692e-02, -2.4256e-01, -2.7010e-01, -8.5571e-02, -1.7614e-01,\n",
       "        -1.7786e-01, -1.3418e-02, -1.4634e-01,  3.0529e-01,  6.0719e-02,\n",
       "         2.8651e-01,  1.4169e-01, -1.9963e-01,  1.3507e-01,  7.0590e-02,\n",
       "        -4.2919e-02, -7.0586e-02, -3.5384e-01,  1.3877e-01,  2.4022e-01,\n",
       "        -3.0725e-02,  4.1167e-02, -2.2029e-01, -1.1796e-01,  1.1195e-01,\n",
       "         2.0739e-01, -1.6588e-01, -1.4708e-01,  9.7583e-02,  2.0831e-01,\n",
       "        -1.2357e-01, -7.9109e-02, -7.7406e-02, -3.4475e-01,  2.9539e-01,\n",
       "         1.4243e-02,  7.5820e-02, -1.9915e-02, -1.6280e-01,  1.9031e-02,\n",
       "         2.5743e-01,  1.2232e-01, -8.2438e-02,  6.1687e-02,  2.1967e-01,\n",
       "         2.2370e-01, -9.4008e-02,  1.5609e-01,  1.6797e-03,  1.8566e-01,\n",
       "        -2.6615e-01, -1.0925e-01,  3.6341e-01,  1.0227e-01,  1.6318e-01,\n",
       "        -1.5906e-01,  2.7914e-02, -9.7719e-02,  2.1670e-02,  1.3730e-01,\n",
       "         3.1447e-01,  1.6585e-02, -3.2731e-01,  4.1633e-01,  1.8380e-01,\n",
       "        -2.2986e-01, -1.2125e-01, -1.5239e-01, -1.3330e-02,  1.5810e-01,\n",
       "         3.2326e-01, -7.6780e-02, -1.2008e-01, -1.1215e-01, -5.0701e-02,\n",
       "         1.1711e-01,  1.4279e-01, -1.2161e-01, -1.5066e-02, -7.6299e-02,\n",
       "         2.2693e-01,  1.7511e-01,  2.3596e-02,  1.4218e-01,  2.1250e-01,\n",
       "         2.1344e-01, -6.8560e-02,  6.3065e-02,  5.5161e-01,  2.6941e-01,\n",
       "         1.7143e-01, -1.9773e-01,  3.0509e-02, -3.7473e-01, -2.3374e-01,\n",
       "         1.9453e-01,  2.3319e-03,  5.2309e-03,  1.6025e-01,  5.0800e-01,\n",
       "         3.0588e-01,  2.6994e-02,  1.1541e-01, -7.9795e-02, -9.9845e-02,\n",
       "        -3.1527e-02,  9.3683e-02,  1.0850e-02,  3.8706e-01, -2.6888e-01,\n",
       "        -4.0783e-01, -1.6802e-03,  1.5798e-02,  3.5886e-02,  1.1664e-01,\n",
       "         2.3909e-02, -5.3473e-02, -3.2640e-01,  1.7957e-01,  1.1095e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.1395e-01, -1.8600e-01, -3.1139e-01,  6.9232e-02, -2.2679e-02,\n",
       "         3.5245e-01, -1.7451e-01, -3.4762e-02,  3.6499e-02,  2.4352e+00,\n",
       "         1.5621e-01, -4.1787e-02, -7.2947e-02, -3.1921e-02,  8.9066e-02,\n",
       "         1.9200e-01,  1.6327e-02,  8.0790e-01, -2.3115e-02, -5.3932e-02,\n",
       "        -2.7845e-01,  2.0944e-02, -2.3572e-01,  8.7822e-02,  1.7885e-01,\n",
       "        -1.2684e-01,  1.1533e-01, -1.9993e-01,  1.7179e-01, -2.9116e-01,\n",
       "        -1.3022e-01,  1.4925e-02, -3.4743e-01, -4.2228e-01, -4.9583e-01,\n",
       "         2.5687e-01, -6.4040e-02,  1.4373e-01, -8.7055e-02,  2.3477e-02,\n",
       "         1.1067e-01,  3.4200e-01,  1.0643e-01, -5.5534e-03,  2.1959e-01,\n",
       "         3.2662e-01,  4.2147e-01, -1.1966e-01,  1.5098e-01,  5.5619e-02,\n",
       "        -3.8991e-01,  1.5516e-01, -8.2267e-02, -2.4025e-01, -3.4519e-01,\n",
       "        -1.9722e-01, -3.1244e-01, -5.8754e-01, -2.0353e-01, -7.6791e-03,\n",
       "        -9.1359e-02, -1.7571e-01, -1.7574e-01,  3.2297e-01, -2.6038e-01,\n",
       "        -2.8268e-01, -6.8955e-02,  2.6774e-02, -1.0006e-01,  3.6755e-01,\n",
       "        -1.4354e-01,  3.6205e-01, -1.1899e-01,  1.9502e-02,  6.3868e-01,\n",
       "        -1.4751e-02, -1.6021e-01,  4.8338e-01,  5.8345e-03,  3.2956e-01,\n",
       "         1.2950e-01, -2.2254e-01, -1.9471e-01, -2.5058e-01,  1.0409e-01,\n",
       "        -1.5040e-01, -3.0280e-01,  2.3811e-01,  2.9955e-01, -1.9751e-02,\n",
       "         1.8583e-03, -3.4478e-01,  1.4361e-01, -1.2800e-03,  4.9823e-01,\n",
       "        -6.4337e-02,  4.9903e-01,  5.1760e-01, -1.5414e-01, -3.1006e-01,\n",
       "        -1.6568e-01,  2.8804e-02, -2.9711e-01,  3.8346e-02,  2.3626e-01,\n",
       "        -1.5317e+00,  4.8050e-03, -4.0435e-01,  2.8585e-02,  1.5562e-01,\n",
       "         8.9044e-02, -8.2613e-01, -3.3854e-01, -2.1842e-01, -4.1949e-01,\n",
       "         1.4582e-01, -7.1290e-02, -1.7822e-01, -4.6116e-01,  8.1208e-02,\n",
       "         1.0228e-01, -2.2000e-02,  1.5704e-01, -4.6219e-01,  4.4219e-01,\n",
       "        -1.2331e-02,  6.0651e-01, -4.6436e-02,  7.1817e-02, -2.8741e-01,\n",
       "         8.0092e-02,  9.5995e-02,  2.1868e-01,  9.2623e-02,  5.9085e-02,\n",
       "        -1.5068e-01,  7.5694e-02, -4.2070e-01,  7.7992e-02,  2.3851e-01,\n",
       "        -1.4185e+00, -6.6634e-02,  1.6350e-01,  1.4849e-01, -1.9227e-01,\n",
       "         1.2907e-02, -3.4447e-01, -4.6486e-01,  1.4224e-01,  1.3265e-01,\n",
       "         8.8162e-02,  1.6292e-01,  1.3023e-01, -3.1362e-01, -3.9454e-01,\n",
       "        -1.4404e-01, -2.1622e-01, -2.0923e-01, -3.4271e-01, -1.6063e-01,\n",
       "         7.1870e-02, -1.9801e-01, -5.0524e-02,  7.4052e-05, -4.3107e-01,\n",
       "        -8.2001e-02, -3.8524e-01, -4.9691e-02,  1.0136e-01, -4.7003e-01,\n",
       "         1.1299e-01,  2.6024e-01, -1.6097e-01,  8.5316e-02, -2.4209e-01,\n",
       "         4.3188e-02, -2.8629e-01, -2.3318e-01,  1.9134e-01, -3.7870e-01,\n",
       "         1.9383e-01, -1.3177e-01, -4.2849e-01, -2.7833e-01, -6.3169e-02,\n",
       "        -2.3149e-01,  3.5011e-02, -2.3458e-01,  5.1698e-01,  8.0902e-02,\n",
       "         5.3812e-01, -1.0787e-01, -2.1465e-01, -3.5011e-01, -1.1482e-01,\n",
       "        -3.7470e-01, -1.4581e-01, -1.5781e-01, -8.8941e-02, -2.5458e-01,\n",
       "        -2.2646e-01, -1.4723e-01,  1.9318e-01,  1.5410e-01,  3.5199e-02,\n",
       "        -1.0698e-01, -4.4188e-02, -4.5023e-01,  2.6755e-01,  3.9788e-02,\n",
       "         1.6353e-01, -4.0591e-02,  1.4291e-01, -2.7215e-01,  2.4453e-01,\n",
       "         1.0331e-02,  2.1538e-01, -1.1237e-01, -3.7174e-02, -1.1926e-01,\n",
       "         2.9896e-01,  5.2738e-02, -8.8533e-02,  4.3619e-02, -1.0675e-01,\n",
       "         1.6294e-01,  2.5775e-01, -3.6536e-02,  9.5472e-02,  2.0618e-01,\n",
       "        -1.4716e-01, -2.3276e-01,  2.8959e-01,  3.0868e-02,  2.7450e-01,\n",
       "        -2.0971e-02,  2.8721e-01, -2.1334e-01,  4.2814e-01,  3.2721e-01,\n",
       "        -1.8220e-01,  2.6856e-01, -2.6014e-01,  2.3042e-01, -3.0705e-01,\n",
       "         2.9832e-01, -4.2529e-01, -3.5242e-01,  2.3699e-01, -5.1793e-03,\n",
       "         1.9576e-01,  1.3232e-01,  1.8231e-01,  9.8129e-02,  2.3562e-01,\n",
       "        -6.8167e-02,  1.2068e-01,  1.9259e-01, -1.3113e-01,  1.4483e-01,\n",
       "         3.3028e-01,  1.0524e-01, -1.6241e-01,  1.8550e-01, -5.3597e-02,\n",
       "         1.2893e-01,  1.0969e-01, -3.3200e-01,  1.4515e-01,  2.8574e-01,\n",
       "        -2.3060e-01, -2.6947e-01, -4.2454e-01, -9.0470e-01, -9.2193e-02,\n",
       "        -1.4784e-01,  5.1422e-02, -2.2871e-01,  1.0521e-01,  4.7294e-01,\n",
       "         2.0239e-01, -1.6172e-02, -2.8370e-02, -1.0857e-01,  2.3019e-01,\n",
       "         1.2928e-01, -3.9928e-01, -5.9975e-02, -1.4594e-01, -5.7901e-02,\n",
       "        -7.0202e-01, -1.8404e-01, -8.4864e-02,  4.5815e-02, -2.0621e-01,\n",
       "         4.5970e-01, -9.3192e-02, -6.1150e-01, -7.5240e-02, -1.2648e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7386e-02,  2.1880e-01,  1.7469e-01,  2.7509e-01, -2.1631e-01,\n",
       "         3.5347e-01, -2.2180e-02, -2.8373e-01,  2.5881e-01,  1.9462e+00,\n",
       "        -4.0591e-02, -1.7405e-01,  1.3791e-01,  5.1707e-01, -1.1379e-01,\n",
       "         4.3605e-01,  4.4465e-01,  1.5117e+00, -3.5965e-01,  4.3858e-02,\n",
       "        -5.1923e-02,  2.9570e-01, -7.3748e-01, -5.4741e-03, -1.7881e-01,\n",
       "        -2.2322e-01,  6.4444e-01, -5.1164e-01,  9.3286e-02, -3.9014e-01,\n",
       "        -2.3993e-01, -3.4526e-01,  6.7683e-03, -4.2794e-01,  1.0783e-01,\n",
       "         4.2653e-01,  3.0891e-01, -1.0818e-01,  1.8644e-01, -2.1826e-01,\n",
       "        -4.1482e-01,  1.0898e-02, -1.7486e-01,  3.9860e-01, -2.0510e-01,\n",
       "         4.8380e-01, -5.0375e-02,  8.0554e-01, -9.0117e-02,  9.7864e-02,\n",
       "         1.5842e-01, -3.6625e-01,  3.8516e-01, -3.7493e-02, -3.4814e-01,\n",
       "         9.3430e-02,  9.1870e-01, -2.3090e-01,  3.5374e-01,  6.5830e-02,\n",
       "         1.1644e-01,  5.0977e-01, -1.5380e-01, -1.8577e-01,  4.4437e-01,\n",
       "         2.7601e-01,  2.2735e-01,  5.3475e-01, -1.8252e-01, -7.4210e-02,\n",
       "         2.1165e-01, -5.0073e-01,  1.2662e-01, -1.2315e-01,  3.8191e-01,\n",
       "        -1.6075e-01,  5.3988e-02, -1.9419e-01,  1.7041e-02, -2.8735e-01,\n",
       "         4.7740e-01, -6.2051e-01, -3.0525e-01, -3.2444e-01, -8.1936e-02,\n",
       "         2.4708e-01,  1.6119e-01,  1.4163e-01, -2.6534e-02,  6.8003e-01,\n",
       "         3.8574e-01,  7.9060e-01,  1.7182e-01,  1.9055e-01, -4.0842e-01,\n",
       "        -7.1388e-01, -2.4055e-01,  3.0572e-01,  3.2042e-01,  9.4508e-02,\n",
       "         1.9716e-01, -3.2531e-01, -4.6567e-02,  4.9304e-03,  2.3895e-01,\n",
       "        -1.5985e+00,  4.4618e-01,  6.7059e-02,  4.2562e-02, -9.4264e-02,\n",
       "         2.6359e-01,  5.9031e-02,  2.1923e-01,  1.1912e-01, -4.8703e-01,\n",
       "        -1.6320e-01,  1.3074e-01,  1.3469e-01, -8.6061e-04,  6.5107e-02,\n",
       "         2.6348e-01, -6.8230e-02, -3.4943e-01,  1.4124e-03, -3.2127e-01,\n",
       "        -3.2119e-01,  2.4275e-01,  1.4489e-01,  5.2155e-01,  1.8236e-02,\n",
       "         3.8936e-01,  1.7392e-01, -1.8137e-01,  2.3953e-01,  7.9878e-02,\n",
       "         2.3554e-01,  3.4491e-01,  1.3673e-01, -2.4935e-02,  1.2661e-01,\n",
       "        -9.4117e-01, -2.7312e-01, -3.9914e-02, -2.1242e-02, -2.4627e-01,\n",
       "         1.8920e-01, -3.7878e-02, -7.5084e-01,  3.1216e-01, -8.1300e-02,\n",
       "        -1.4081e-01,  1.9424e-01,  5.0618e-01,  5.5668e-02, -2.2838e-01,\n",
       "        -7.9835e-02,  3.3727e-01, -3.9557e-01, -3.1844e-01, -6.1797e-01,\n",
       "        -4.3276e-01, -1.0177e-02, -1.6727e-01, -1.4701e-01, -1.3138e-03,\n",
       "        -3.6874e-01,  8.8321e-03,  1.0844e-01,  2.2943e-01, -5.6433e-01,\n",
       "         3.1256e-01, -1.9476e-01, -7.2129e-02, -8.9461e-01, -5.7574e-01,\n",
       "        -9.0824e-02, -6.1415e-01,  1.7660e-01, -2.6355e-01,  6.2693e-01,\n",
       "        -4.5756e-01, -8.6851e-01, -2.4981e-01,  5.5472e-01, -8.5896e-02,\n",
       "         1.1540e-01,  3.1408e-01,  3.9062e-01, -3.4876e-01,  5.4840e-02,\n",
       "        -2.2244e-01,  5.9099e-02,  9.9086e-02, -5.1477e-01, -1.7693e-01,\n",
       "         1.3470e-01, -2.5853e-01, -4.6366e-01,  2.2667e-01,  2.6040e-01,\n",
       "        -3.4442e-01,  2.5386e-01,  3.2574e-02,  3.5130e-01, -6.8617e-01,\n",
       "        -1.3479e-01, -1.5026e-01,  1.6556e-01,  3.2562e-01,  1.3296e-01,\n",
       "         2.2568e-01,  2.6165e-01,  9.7049e-03,  4.1276e-02, -3.9056e-01,\n",
       "        -2.5348e-01,  1.1446e-01, -2.1078e-01,  3.7831e-02, -4.9510e-02,\n",
       "         3.3036e-01,  2.0437e-01, -5.9281e-01,  1.0520e-01, -4.3911e-01,\n",
       "         5.1661e-01, -1.9186e-01,  6.2483e-01,  2.6081e-02, -1.2373e-01,\n",
       "        -2.3343e-01,  7.1194e-01,  3.1010e-01,  1.5132e-01, -2.8571e-01,\n",
       "         2.9020e-02, -6.9205e-02, -2.8972e-01,  1.1664e-03,  9.4657e-03,\n",
       "        -3.1325e-01, -3.6959e-01,  6.3799e-01,  4.5772e-03, -4.4835e-02,\n",
       "        -1.0021e-01,  2.0841e-01, -8.3103e-02,  3.5557e-02,  5.7080e-01,\n",
       "         1.3201e-02,  6.1434e-02,  7.9083e-02, -5.3205e-02,  1.2540e-01,\n",
       "         1.2251e-01,  2.2707e-01, -6.0252e-02, -1.3078e-01,  1.8732e-01,\n",
       "        -2.4053e-01,  3.9825e-01,  9.7576e-02, -6.0194e-01, -9.7147e-02,\n",
       "         5.0851e-01,  9.4998e-02,  2.9620e-01,  6.9577e-01,  2.7756e-03,\n",
       "         6.0900e-02, -2.9739e-01, -2.0041e-01, -3.3111e-01,  3.2232e-01,\n",
       "         2.9970e-01,  7.8138e-02, -2.1120e-01,  2.4587e-01,  7.2880e-01,\n",
       "        -5.8055e-02, -3.1365e-02,  5.0065e-01, -4.5560e-03, -2.1692e-01,\n",
       "        -1.8249e-01, -3.7010e-01,  1.9931e-01,  3.1825e-02, -1.2789e-01,\n",
       "         5.8350e-01,  4.7768e-01,  3.3556e-01,  1.1028e-01,  1.5622e-01,\n",
       "        -7.6022e-02,  2.5118e-01,  5.3027e-02, -7.8588e-02, -4.7783e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7675e-01, -1.1799e-01,  1.7866e-01,  3.2482e-02, -4.9160e-01,\n",
       "         4.4509e-02, -3.8697e-01,  3.4402e-02, -4.0194e-02,  3.1124e+00,\n",
       "        -2.2361e-01,  1.1824e-01,  6.9428e-02, -3.2642e-02,  1.4955e-01,\n",
       "         1.5286e-01,  3.2991e-02,  6.3527e-01, -2.3576e-01,  2.4908e-01,\n",
       "         2.0384e-01,  1.4182e-01,  2.0118e-01, -2.0678e-01, -1.6844e-01,\n",
       "         1.3097e-01,  4.8774e-01, -3.5005e-01, -7.5147e-02, -1.5022e-01,\n",
       "         1.6138e-01, -5.5018e-01, -1.9398e-01, -2.8315e-02,  1.6917e-01,\n",
       "        -4.0531e-01,  6.5695e-02, -1.3529e-01,  1.7965e-01,  7.5198e-02,\n",
       "         1.2148e-01,  6.1736e-01,  3.6658e-02, -5.6777e-01,  3.2880e-01,\n",
       "        -2.4745e-01,  1.3974e-01,  7.4644e-01, -1.9929e-01,  3.6678e-01,\n",
       "         2.9422e-01, -8.5806e-02,  1.2210e-01,  1.6887e-01, -2.1271e-01,\n",
       "         7.3281e-03, -7.6509e-02,  1.7360e-01,  3.0140e-01, -6.2679e-02,\n",
       "        -2.4980e-01, -6.2602e-02, -2.8363e-01,  1.1710e-01,  1.4115e-01,\n",
       "        -5.0121e-01,  1.2329e-01, -1.8731e-01,  3.9721e-01, -2.5896e-02,\n",
       "         2.9596e-01, -3.2547e-02,  1.7333e-01, -1.1771e-01,  1.2088e-01,\n",
       "        -3.5890e-01, -1.3854e-01, -8.9703e-02,  3.1391e-01, -9.1165e-02,\n",
       "        -1.3064e-01, -2.2216e-02,  1.5877e-02,  2.6226e-01, -2.7216e-01,\n",
       "        -1.5718e-01, -8.3953e-01, -1.4901e-01,  1.2313e-01,  3.2840e-01,\n",
       "        -1.1891e-01,  5.5510e-02,  1.8767e-01,  5.3449e-01, -2.0262e-01,\n",
       "         1.6928e-01, -1.9827e-01, -2.4103e-01, -1.8439e-02, -9.7009e-02,\n",
       "        -7.1952e-02, -3.6047e-01, -9.6811e-02, -2.9746e-01, -2.7191e-02,\n",
       "        -1.4366e+00,  4.3149e-01,  1.3098e-01, -7.2661e-02,  1.7736e-01,\n",
       "         1.6811e-01,  1.7400e-02,  1.7484e-01, -2.2591e-01,  4.0063e-01,\n",
       "         1.2184e-01,  6.9323e-02, -1.1818e-01,  3.1484e-01, -1.0699e-01,\n",
       "         4.0806e-01, -1.2300e-01, -3.1155e-01,  2.1126e-01, -5.7120e-02,\n",
       "         8.6237e-02, -4.4460e-01,  2.2277e-04,  2.4001e-02,  3.0837e-01,\n",
       "        -1.6179e-01,  2.5340e-01, -4.2660e-02,  2.5280e-03,  9.9567e-02,\n",
       "        -5.1727e-02,  3.5747e-02, -1.7966e-01,  3.3886e-01, -1.6398e-01,\n",
       "        -7.8192e-01,  6.0500e-01,  2.8252e-01, -1.2994e-01, -1.6583e-02,\n",
       "        -3.4698e-01, -7.2527e-01,  8.0799e-02,  9.4704e-02,  3.6744e-01,\n",
       "        -8.9103e-02,  6.9915e-02,  1.6852e-01, -1.2281e-01, -2.8876e-02,\n",
       "        -7.7054e-02, -1.8588e-01, -1.8323e-01, -1.9980e-01,  4.5056e-02,\n",
       "        -7.6702e-02,  3.8135e-01, -3.8801e-01, -2.1650e-01, -3.0306e-02,\n",
       "         1.6275e-01, -1.0535e-02, -1.6363e-01,  7.2000e-04, -2.2225e-02,\n",
       "        -7.1306e-02, -2.7842e-02,  2.6646e-01, -2.7049e-01, -2.1350e-02,\n",
       "        -4.4271e-02, -1.2112e-01,  1.1946e-01, -1.1000e-02,  3.2661e-01,\n",
       "        -1.8398e-01,  7.4138e-02, -4.2220e-02,  9.7410e-02, -1.4307e-01,\n",
       "         1.0406e-02,  2.1065e-02,  1.9372e-01, -1.3614e-01,  1.1002e-01,\n",
       "         1.5361e-01, -5.0540e-02,  8.6166e-02,  2.4223e-01,  1.4647e-01,\n",
       "        -1.3835e-01, -6.9210e-02, -1.4987e-01,  1.8680e-01,  3.9816e-01,\n",
       "         5.7365e-01,  1.5033e-02,  4.5188e-02,  1.6298e-01,  9.3265e-02,\n",
       "        -7.3806e-02, -3.6836e-01, -1.0767e-01,  4.6960e-01, -1.3725e-01,\n",
       "         1.1711e-01,  2.3232e-01,  2.1569e-02,  8.2387e-03,  2.0852e-01,\n",
       "         1.8448e-01,  2.6981e-01, -7.7155e-02, -1.5074e-01, -1.4044e-01,\n",
       "         1.5819e-01, -3.1127e-01, -2.3062e-01,  1.6080e-01,  1.3612e-01,\n",
       "        -6.8917e-02,  2.7529e-01, -2.0376e-01, -4.3880e-02,  3.3157e-01,\n",
       "         1.8735e-01,  5.5636e-02,  1.2360e-01, -1.1264e-01,  1.3654e-01,\n",
       "         3.6212e-01, -2.0679e-01,  2.2256e-02,  8.0054e-02, -5.1004e-02,\n",
       "         1.5415e-01, -4.8844e-01, -1.7730e-01,  4.1286e-01, -2.8209e-01,\n",
       "        -2.8011e-01, -1.5195e-01, -3.5349e-01,  3.4257e-01,  1.2766e-01,\n",
       "        -4.7474e-02,  3.2728e-01, -6.9187e-01,  1.7493e-01,  8.9532e-02,\n",
       "        -6.1105e-02, -6.9829e-02, -1.5193e-01,  5.7987e-02,  3.0111e-02,\n",
       "        -3.9252e-02,  5.3934e-02,  4.5133e-01,  4.2870e-02, -2.2657e-01,\n",
       "        -3.5830e-01, -5.6488e-02,  1.0440e-01,  4.9360e-01,  2.7961e-01,\n",
       "         5.4662e-01, -1.3974e-01, -2.1185e-01, -4.5541e-01, -6.9248e-02,\n",
       "        -3.5238e-01,  2.2941e-01, -2.4661e-01,  1.6707e-01,  2.9528e-01,\n",
       "        -1.2137e-01, -2.0418e-01,  2.4620e-01,  3.4143e-01, -2.9176e-01,\n",
       "        -5.7014e-01, -2.6190e-01, -3.2669e-01,  3.1928e-01, -3.0172e-01,\n",
       "        -1.3678e-01,  4.1218e-01,  1.1811e-01,  2.8056e-01,  4.2607e-02,\n",
       "        -4.6627e-01,  2.9769e-01, -4.3038e-02, -5.6063e-01,  2.5515e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_test[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "       -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "       -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "       -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "       -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "        3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "        3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "       -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "        3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "       -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "       -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "        3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "       -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "       -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "        1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "       -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "       -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "        6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "       -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "        4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "       -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "       -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "        2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "        2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "        1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "        1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "       -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "       -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "       -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "       -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "       -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "        4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "       -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "       -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "        1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "       -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "       -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "        4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "       -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "        1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "        1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "        7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "       -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "        7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "       -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "       -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "       -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "       -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "        8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "       -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "       -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "        3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "       -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "       -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "        3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "       -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "        4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "       -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "       -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "        8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.Dataset(examples, fields)\n",
    "data_set_test = data.Dataset(examples_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.sort_key = lambda x: len(x.text)\n",
    "data_set_test.sort_key = lambda x: len(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data_set.split([0.8, 0.1, 0.1], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124848"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(train_data.sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set_test.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(data_set_test.sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)\n",
    "\n",
    "kaggle_test_iterator = data.BucketIterator(\n",
    "    data_set_test, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_iterator = iter(train_iterator)\n",
    "batch = next(raw_train_iterator)\n",
    "\n",
    "raw_kaggle_test_iterator = iter(kaggle_test_iterator)\n",
    "batch_test = next(raw_kaggle_test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = batch.text\n",
    "\n",
    "c, d = batch_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 308])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 1, 3, 3, 1, 3, 3, 2, 2, 2, 2, 2, 0, 2, 1, 4, 3, 2, 3, 2,\n",
       "        2, 2, 1, 3, 1, 2, 2, 1, 3, 1, 2, 2, 1, 4, 2, 3, 2, 3, 2, 2, 1, 4, 2, 2,\n",
       "        2, 3, 2, 3, 1, 2, 2, 3, 4, 2, 2, 4, 2, 2, 1, 4], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133918,  57783,  94049,  86671,  84086,  74128, 112683, 126943,  12599,\n",
       "         138419,  47937,  62635, 104584,  86587, 113162,  81036,  62472, 147974,\n",
       "          32209, 133654,  95241, 137072,  66415,  75555,  53039, 130485,  79837,\n",
       "         152257,  39951, 136680, 121126, 102615, 101662, 119728, 108929, 145776,\n",
       "          36689,  66928, 109732, 109843, 130506,  33026, 123268, 155495, 114095,\n",
       "          27453, 122599,  82520,  21535, 104973,  91096,  97182,  84542, 119081,\n",
       "         104435,  65401,  30863, 141808,  39997, 145932,  46149, 125167, 132902,\n",
       "         117864]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data['Sentiment'][117864-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10.], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 308])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.phrase_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId                                                       193462\n",
       "SentenceId                                                      10289\n",
       "Phrase                         you 're not into the Pokemon franchise\n",
       "Phrase_length                                                      72\n",
       "Tokenized_phrase    [xxbos, you, 're, not, into, the, xxmaj, pokem...\n",
       "Indexed_phrase               [2, 33, 157, 41, 61, 8, 7, 2893, 978, 3]\n",
       "Name: 37401, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.iloc[37401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "        -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "         1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "        -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "         6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "        -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "        -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "         8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "        -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "         1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "         5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "        -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "         1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "        -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "         2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "        -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "         3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "        -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "        -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "         1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "        -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "        -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "        -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "         7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "         6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "        -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "        -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "        -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "        -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "        -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "         4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "         3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "        -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "        -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "         7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "        -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "         8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "         8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "         3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "         1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "         1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "        -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "         2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "         4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "        -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "        -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "        -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "         2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "         2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "         1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "        -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "         1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "         3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "         1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "        -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "        -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "         1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "        -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "        -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "         1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[63][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "       -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "        1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "       -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "        6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "       -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "       -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "        8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "       -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "        1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "        5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "       -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "        1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "       -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "        2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "       -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "        3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "       -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "       -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "        1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "       -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "       -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "       -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "        7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "        6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "       -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "       -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "       -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "       -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "       -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "        4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "        3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "       -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "       -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "        7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "       -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "        8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "        8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "        3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "        1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "        1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "       -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "        2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "        4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "       -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "       -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "       -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "        2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "        2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "        1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "       -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "        1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "        3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "        1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "       -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "       -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "        1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "       -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "       -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "        1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector(\"'re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM( embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(text, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        #if self.bidirectional:\n",
    "        #    hidden = [batch size, hid dim * num directions]\n",
    "        #else:\n",
    "        #    hidden = [batch size, hid dim]\n",
    "        \n",
    "        # with RELU\n",
    "        #return self.fc(self.relu(hidden))\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 308\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Regularization hyperparameter\n",
    "DROPOUT = 0.7\n",
    "L2_LAMBDA = 0 #0.0001\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "MODEL_SAVE_FILE = 'LSTM_origin.pt'\n",
    "model = LSTM(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,107,205 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define epoch time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.01488 | Train Acc: 60.89%\n",
      "\t Val. Loss: 0.01401 |  Val. Acc: 62.59%\n",
      "Epoch: 02 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.01343 | Train Acc: 64.17%\n",
      "\t Val. Loss: 0.01294 |  Val. Acc: 65.51%\n",
      "Epoch: 03 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.01274 | Train Acc: 66.15%\n",
      "\t Val. Loss: 0.01253 |  Val. Acc: 66.67%\n",
      "Epoch: 04 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.01216 | Train Acc: 67.71%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 67.19%\n",
      "Epoch: 05 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.01160 | Train Acc: 69.26%\n",
      "\t Val. Loss: 0.01199 |  Val. Acc: 68.35%\n",
      "Epoch: 06 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.01114 | Train Acc: 70.50%\n",
      "\t Val. Loss: 0.01223 |  Val. Acc: 68.07%\n",
      "Epoch: 07 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.01072 | Train Acc: 71.59%\n",
      "\t Val. Loss: 0.01168 |  Val. Acc: 69.18%\n",
      "Epoch: 08 | Epoch Time: 0m 53s\n",
      "\tTrain Loss: 0.01037 | Train Acc: 72.44%\n",
      "\t Val. Loss: 0.01179 |  Val. Acc: 69.15%\n",
      "Epoch: 09 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.01000 | Train Acc: 73.51%\n",
      "\t Val. Loss: 0.01181 |  Val. Acc: 68.92%\n",
      "Epoch: 10 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00966 | Train Acc: 74.33%\n",
      "\t Val. Loss: 0.01206 |  Val. Acc: 69.14%\n",
      "Epoch: 11 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00935 | Train Acc: 75.23%\n",
      "\t Val. Loss: 0.01253 |  Val. Acc: 68.57%\n",
      "Epoch: 12 | Epoch Time: 1m 3s\n",
      "\tTrain Loss: 0.00902 | Train Acc: 76.27%\n",
      "\t Val. Loss: 0.01221 |  Val. Acc: 68.64%\n",
      "Epoch: 13 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00871 | Train Acc: 77.03%\n",
      "\t Val. Loss: 0.01260 |  Val. Acc: 68.73%\n",
      "Epoch: 14 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00838 | Train Acc: 77.93%\n",
      "\t Val. Loss: 0.01287 |  Val. Acc: 68.29%\n",
      "Epoch: 15 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00804 | Train Acc: 78.91%\n",
      "\t Val. Loss: 0.01345 |  Val. Acc: 68.11%\n",
      "Epoch: 16 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00771 | Train Acc: 79.78%\n",
      "\t Val. Loss: 0.01393 |  Val. Acc: 67.79%\n",
      "Epoch: 17 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00738 | Train Acc: 80.79%\n",
      "\t Val. Loss: 0.01443 |  Val. Acc: 67.49%\n",
      "Epoch: 18 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00701 | Train Acc: 81.72%\n",
      "\t Val. Loss: 0.01485 |  Val. Acc: 66.92%\n",
      "Epoch: 19 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00669 | Train Acc: 82.58%\n",
      "\t Val. Loss: 0.01539 |  Val. Acc: 66.61%\n",
      "Epoch: 20 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00637 | Train Acc: 83.61%\n",
      "\t Val. Loss: 0.01631 |  Val. Acc: 67.03%\n",
      "Epoch: 21 | Epoch Time: 0m 52s\n",
      "\tTrain Loss: 0.00606 | Train Acc: 84.59%\n",
      "\t Val. Loss: 0.01677 |  Val. Acc: 66.26%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-bda155f68f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-6b206d8692c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, set_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "# best_valid_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# For splotting\n",
    "all_train_losses = []\n",
    "all_valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, len(train_data))\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "#     if valid_acc > best_valid_acc:\n",
    "        best_valid_loss = valid_loss\n",
    "#         best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), saved_models_path + MODEL_SAVE_FILE)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    all_train_losses.append(train_loss)\n",
    "    all_valid_losses.append(valid_loss)\n",
    "    \n",
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# plt.xticks(range(0, 10))\n",
    "plt.plot(all_train_losses)\n",
    "plt.plot(all_valid_losses)\n",
    "plt.axvline(x=best_epoch, color='green')\n",
    "\n",
    "plt.legend(['train loss', 'valid loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 07\n"
     ]
    }
   ],
   "source": [
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01179 | Test Acc: 68.67%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, len(test_data))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.5f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reevaluate on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01194 | Test Acc: 69.28%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "\n",
    "print(f'Test Loss: {valid_loss:.5f} | Test Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Kaggle Dataset and create submittion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model target file\n",
    "MODEL_SAVE_FILE_TARGET = 'LSTM_origin.pt'\n",
    "\n",
    "def predict_kaggle_test(model, iterator):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            predictions = predictions.argmax(1)\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                result.append([batch.id[0][i].item(), [batch.phrase_id[0][i].item(), predictions[i].item()]] )\n",
    "    \n",
    "    result.sort(key = lambda val: val[0])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [156061, 2]],\n",
       " [1, [156062, 2]],\n",
       " [2, [156063, 2]],\n",
       " [3, [156064, 2]],\n",
       " [4, [156065, 2]],\n",
       " [5, [156066, 2]],\n",
       " [6, [156067, 3]],\n",
       " [7, [156068, 2]],\n",
       " [8, [156069, 3]],\n",
       " [9, [156070, 2]]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE_TARGET))\n",
    "\n",
    "kaggle_result_list = predict_kaggle_test(model, kaggle_test_iterator)\n",
    "\n",
    "kaggle_result_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66282, [222343, 2]],\n",
       " [66283, [222344, 2]],\n",
       " [66284, [222345, 2]],\n",
       " [66285, [222346, 2]],\n",
       " [66286, [222347, 2]],\n",
       " [66287, [222348, 0]],\n",
       " [66288, [222349, 0]],\n",
       " [66289, [222350, 1]],\n",
       " [66290, [222351, 1]],\n",
       " [66291, [222352, 1]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_result_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156066</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but</td>\n",
       "      <td>68</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, xxeos]</td>\n",
       "      <td>[2, 2606, 1723, 30, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>156067</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing</td>\n",
       "      <td>2</td>\n",
       "      <td>[xxbos, intermittently, pleasing, xxeos]</td>\n",
       "      <td>[2, 2606, 1723, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>156068</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently</td>\n",
       "      <td>65</td>\n",
       "      <td>[xxbos, intermittently, xxeos]</td>\n",
       "      <td>[2, 2606, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156069</td>\n",
       "      <td>8545</td>\n",
       "      <td>pleasing</td>\n",
       "      <td>9</td>\n",
       "      <td>[xxbos, pleasing, xxeos]</td>\n",
       "      <td>[2, 1723, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156070</td>\n",
       "      <td>8545</td>\n",
       "      <td>but</td>\n",
       "      <td>55</td>\n",
       "      <td>[xxbos, but, xxeos]</td>\n",
       "      <td>[2, 30, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "5    156066        8545                        intermittently pleasing but   \n",
       "6    156067        8545                            intermittently pleasing   \n",
       "7    156068        8545                                     intermittently   \n",
       "8    156069        8545                                           pleasing   \n",
       "9    156070        8545                                                but   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "5             68      [xxbos, intermittently, pleasing, but, xxeos]   \n",
       "6              2           [xxbos, intermittently, pleasing, xxeos]   \n",
       "7             65                     [xxbos, intermittently, xxeos]   \n",
       "8              9                           [xxbos, pleasing, xxeos]   \n",
       "9             55                                [xxbos, but, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  \n",
       "5                             [2, 2606, 1723, 30, 3]  \n",
       "6                                 [2, 2606, 1723, 3]  \n",
       "7                                       [2, 2606, 3]  \n",
       "8                                       [2, 1723, 3]  \n",
       "9                                         [2, 30, 3]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_EXTENSION = '.submit.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(saved_models_path + MODEL_SAVE_FILE_TARGET + CSV_EXTENSION, mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    csv_writer.writerow(['PhraseId', 'Sentiment'])\n",
    "    \n",
    "    for i in range(len(kaggle_result_list)):\n",
    "        csv_writer.writerow(kaggle_result_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
