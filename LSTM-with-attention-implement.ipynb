{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load config and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import spacy, pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import random\n",
    "import inspect\n",
    "\n",
    "# Custom impport\n",
    "from common.common_classes import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'LSTM-with-attention-implement.ipynb', 'prepare-word-embedding-nlp.ipynb', 'tokenization.ipynb', 'test-batching-padding.ipynb', 'train.tsv', 'test-batching-padding-ok.ipynb', 'LSTM-implement.ipynb', 'sampleSubmission.csv', 'save_data', '.ipynb_checkpoints', '__init__.py', 'README.md', '.gitignore', '.git', 'common', 'simple-GRU-implement.ipynb']\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "save_data_path = path + 'save_data/'\n",
    "large_save_data_path = '/notebooks/large-storage/'\n",
    "saved_models_path = '/notebooks/large-storage/saved-models/'\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pickle.load(open(save_data_path + 'pre-processed-data.pkl', 'rb'))\n",
    "loaded_kaggle_test = pickle.load(open(save_data_path + 'pre-processed-kaggle-test.pkl', 'rb'))\n",
    "loaded_vocab = pickle.load(open(save_data_path + 'genereated-vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, a, series, of, escapades, demonstratin...</td>\n",
       "      <td>[2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, a, series, xxeos]</td>\n",
       "      <td>[2, 10, 341, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, a, xxeos]</td>\n",
       "      <td>[2, 10, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, series, xxeos]</td>\n",
       "      <td>[2, 341, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  Phrase_length  \\\n",
       "0          1            188   \n",
       "1          2             77   \n",
       "2          2              8   \n",
       "3          2              1   \n",
       "4          2              6   \n",
       "\n",
       "                                    Tokenized_phrase  \\\n",
       "0  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "1  [xxbos, a, series, of, escapades, demonstratin...   \n",
       "2                          [xxbos, a, series, xxeos]   \n",
       "3                                  [xxbos, a, xxeos]   \n",
       "4                             [xxbos, series, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "1  [2, 10, 341, 11, 14246, 6044, 8, 6604, 19, 64,...  \n",
       "2                                    [2, 10, 341, 3]  \n",
       "3                                         [2, 10, 3]  \n",
       "4                                        [2, 341, 3]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "MAX_LABEL = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check max length of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "43802\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_index = -1\n",
    "for i in range(len(loaded_data['Tokenized_phrase'])):\n",
    "    if len(loaded_data['Tokenized_phrase'][i]) > max_len:\n",
    "        max_len = len(loaded_data['Tokenized_phrase'][i])\n",
    "        max_index = i\n",
    "        \n",
    "print(max_len)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "35146\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_index = -1\n",
    "for i in range(len(loaded_kaggle_test['Tokenized_phrase'])):\n",
    "    if len(loaded_kaggle_test['Tokenized_phrase'][i]) > max_len:\n",
    "        max_len = len(loaded_kaggle_test['Tokenized_phrase'][i])\n",
    "        max_index = i\n",
    "        \n",
    "print(max_len)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encoding and prepraing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(large_save_data_path + 'process-spacy-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[BOS].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp.vocab.get_vector('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890280, 308)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.vectors.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "PHRASE_ID = data.Field(use_vocab = False)\n",
    "TEXT = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)\n",
    "LABEL = data.LabelField(use_vocab = False, dtype=torch.long)\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "ID_TEST = data.Field(use_vocab = False)\n",
    "PHRASE_ID_TEST = data.Field(use_vocab = False)\n",
    "TEXT_TEST = TensorField(include_lengths = True, use_vocab = False, sequential = False, pad_token = nlp.vocab[PAD].vector, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Kaggle Train dataset\n",
    "fields = [('id', PHRASE_ID), ('text', TEXT), ('label', LABEL)]\n",
    "\n",
    "# For Kaggle Test dataset\n",
    "fields_test = [('id', ID_TEST), ('phrase_id', PHRASE_ID_TEST), ('text', TEXT_TEST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_data['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_kaggle_test['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f27367fbc18>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbe10>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbe48>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbe80>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbeb8>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbef0>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbf28>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbf60>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbf98>,\n",
       " <torchtext.data.example.Example at 0x7f27367fbfd0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Train dataset\n",
    "examples = []\n",
    "length = len(loaded_data['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_data['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_data['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_data['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples.append(data.Example.fromlist([ [loaded_data['PhraseId'][i]], embedded, loaded_data['Sentiment'][i]], fields))\n",
    "    \n",
    "examples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f26df20d828>,\n",
       " <torchtext.data.example.Example at 0x7f26df20d860>,\n",
       " <torchtext.data.example.Example at 0x7f26df20d780>,\n",
       " <torchtext.data.example.Example at 0x7f26df20d8d0>,\n",
       " <torchtext.data.example.Example at 0x7f26df20d908>,\n",
       " <torchtext.data.example.Example at 0x7f26df20d9b0>,\n",
       " <torchtext.data.example.Example at 0x7f26df20d9e8>,\n",
       " <torchtext.data.example.Example at 0x7f26df20da20>,\n",
       " <torchtext.data.example.Example at 0x7f26df20da58>,\n",
       " <torchtext.data.example.Example at 0x7f26df20da90>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Kaggle Test dataset\n",
    "examples_test = []\n",
    "length = len(loaded_kaggle_test['Phrase'])\n",
    "for i in range(length):\n",
    "    embedded = []\n",
    "    for j in range(len(loaded_kaggle_test['Tokenized_phrase'][i])):\n",
    "        if nlp.vocab.has_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]):\n",
    "            embedded.append(nlp.vocab.get_vector(loaded_kaggle_test['Tokenized_phrase'][i][j]))\n",
    "        else:\n",
    "            embedded.append(nlp.vocab.get_vector(UNK))\n",
    "    \n",
    "    examples_test.append(data.Example.fromlist([ [i], [loaded_kaggle_test['PhraseId'][i]], embedded ], fields_test))\n",
    "    \n",
    "examples_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "        -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "        -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "        -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "        -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "         3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "         3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "        -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "         3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "        -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "        -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "         3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "        -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "        -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "         1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "        -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "        -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "         6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "        -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "         4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "        -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "        -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "         2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "         2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "         1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "         1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "        -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "        -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "        -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "        -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "        -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "         4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "        -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "        -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "         1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "        -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "        -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "         4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "        -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "         1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "         1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "         7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "        -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "         7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "        -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "        -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "        -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "        -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "         8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "        -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "        -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "         3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "        -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "        -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "         3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "        -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "         4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "        -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "        -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "         8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([ 4.7793e-01, -6.2470e-02,  1.9596e-01, -4.3340e-02, -1.4570e-01,\n",
       "         6.7455e-02, -7.9048e-01, -4.7327e-01, -1.2694e-02,  9.4032e-01,\n",
       "         9.7596e-01,  3.6356e-01, -4.2334e-01, -4.7300e-01,  4.3313e-02,\n",
       "        -1.7159e-02,  7.7286e-02,  5.1450e-01, -1.3013e-01, -3.8677e-01,\n",
       "         8.5337e-02, -1.1537e-01, -2.9981e-01,  1.0327e-01,  7.4443e-02,\n",
       "         3.8978e-02, -1.0280e-01, -2.7481e-01, -2.9409e-01, -5.2790e-01,\n",
       "        -5.4200e-01, -4.1610e-01, -7.6345e-01, -2.1057e-01, -6.0277e-01,\n",
       "        -3.7840e-01, -5.1185e-01,  5.4210e-02, -8.2146e-02,  5.3257e-01,\n",
       "         3.3352e-01,  1.6082e-01, -1.6897e-02,  3.3638e-01, -1.3660e-01,\n",
       "         4.2157e-01, -2.5176e-01, -3.2271e-01, -3.9742e-01, -3.2712e-01,\n",
       "         3.5235e-01,  5.4523e-01, -5.8145e-01, -2.5093e-01, -4.2526e-01,\n",
       "         4.7148e-04,  5.4579e-01, -4.9980e-01,  2.3473e-02, -1.0801e-01,\n",
       "        -2.5952e-01, -7.5854e-03, -2.0041e-01, -3.7743e-01, -3.0881e-01,\n",
       "        -7.4719e-02,  3.9462e-03, -3.6002e-01, -7.1041e-02,  2.8597e-01,\n",
       "         1.3777e-01,  1.6269e-01, -1.0977e-01, -3.6319e-01,  8.5886e-02,\n",
       "         2.0098e-01,  1.9972e-01,  3.2699e-01, -1.1095e-01, -2.4142e-01,\n",
       "         1.1838e-01, -2.1962e-01, -6.1894e-01, -1.7774e-01, -1.5444e-01,\n",
       "         7.2033e-02,  7.5290e-01,  1.7973e-01,  3.2366e-01,  6.2210e-01,\n",
       "        -1.1795e-01, -2.4438e-01,  2.0690e-01,  4.0958e-01, -3.5867e-02,\n",
       "        -2.7432e-01,  3.3013e-01,  1.8814e-01,  2.6837e-01, -1.8496e-01,\n",
       "         6.3260e-02, -3.3311e-03, -5.4875e-02,  2.3964e-01, -1.4294e-01,\n",
       "        -1.9737e+00, -1.7595e-01, -7.4150e-02,  3.7287e-01,  1.6554e-01,\n",
       "         2.3770e-01, -1.4556e-01,  4.5459e-01, -8.1550e-02, -3.3170e-01,\n",
       "        -8.1371e-01, -1.7414e-01,  1.3691e-01, -6.0061e-01,  4.0306e-01,\n",
       "        -4.3174e-01, -1.7259e-01,  5.2948e-02, -2.4818e-01,  1.5827e-01,\n",
       "        -1.1220e-01,  3.2860e-02,  5.5025e-01,  2.0649e-01,  1.0403e-01,\n",
       "         4.3973e-01, -1.1043e-01, -2.3429e-01,  4.2473e-01, -2.0461e-01,\n",
       "        -2.3912e-01,  2.7573e-01,  6.4947e-01, -3.4708e-01, -1.4163e-01,\n",
       "        -1.7743e-01,  1.7144e-01, -1.7009e-01, -1.2667e-01, -5.6574e-02,\n",
       "         6.9985e-01,  9.3176e-02, -1.1523e-01,  7.9450e-02,  5.3647e-02,\n",
       "        -4.2493e-01,  7.1772e-01,  2.3705e-01,  2.1213e-01, -1.9776e-02,\n",
       "        -1.9379e-02,  2.9703e-01, -2.7536e-01,  7.3583e-02, -5.1954e-01,\n",
       "        -1.5928e-01, -3.5374e-01,  4.5916e-01,  2.0435e-01,  9.7278e-04,\n",
       "         4.6904e-01, -6.2419e-01,  3.2797e-01, -1.5744e-01, -8.8403e-02,\n",
       "         3.5625e-02, -1.2245e-01,  3.6698e-01, -8.8524e-02, -5.5171e-02,\n",
       "         6.0885e-01,  5.3027e-01, -3.3558e-01,  1.5178e-01, -1.6619e-01,\n",
       "         5.0028e-01, -1.4728e-02, -1.4555e-01,  5.6033e-01,  2.4656e-01,\n",
       "        -3.6279e-01, -7.8195e-04,  9.5365e-02,  5.8955e-02, -4.8528e-01,\n",
       "         5.4453e-01, -1.1293e-02, -1.5423e-01,  3.7746e-01, -4.1018e-01,\n",
       "        -7.5968e-02,  3.2843e-01, -1.0195e-01, -6.5580e-01, -5.7961e-01,\n",
       "        -4.3063e-01,  5.0521e-01,  1.4637e-02,  1.8917e-01,  5.0427e-02,\n",
       "        -1.2479e-01, -1.7724e-01, -1.0494e-01,  3.2128e-01,  5.9077e-02,\n",
       "         2.3804e-01, -6.3107e-02,  3.3216e-02, -2.0066e-01, -2.7485e-01,\n",
       "        -2.8084e-01,  4.9871e-01,  2.0036e-01,  2.9303e-01, -3.4742e-01,\n",
       "         4.9861e-01,  2.3172e-01,  1.4505e-01, -7.2846e-02, -8.4557e-02,\n",
       "        -1.5358e-01,  5.1501e-01, -1.7232e-01,  2.7167e-01, -2.3813e-01,\n",
       "         1.3970e-01,  2.5681e-01,  4.1433e-01,  3.3110e-01, -3.7062e-02,\n",
       "        -6.8761e-02,  6.5520e-02, -3.1337e-01, -3.6793e-02, -1.3847e-01,\n",
       "         2.2251e-01,  3.2041e-01,  3.1190e-01,  2.7103e-01, -4.0910e-01,\n",
       "         1.0762e-01,  1.6541e-01,  3.7305e-01,  2.1920e-01, -4.5466e-01,\n",
       "        -2.6570e-01, -4.3871e-01,  2.7044e-01,  2.0953e-01,  3.1834e-01,\n",
       "        -4.3875e-01,  2.4935e-01,  4.6443e-02, -3.2949e-01,  2.1793e-01,\n",
       "         7.3772e-01,  3.9756e-02, -5.0169e-02, -1.8881e-01,  7.7245e-02,\n",
       "         1.4199e-01,  3.9659e-02,  1.2333e-01, -9.6613e-02, -2.2687e-01,\n",
       "         4.8893e-01, -1.1373e-01,  5.1246e-02,  7.5477e-02, -1.2750e-01,\n",
       "        -1.0697e-01, -4.8071e-01, -2.6223e-01,  4.5893e-01,  3.6267e-01,\n",
       "         2.9100e-01,  1.7118e-02, -2.0185e-01, -7.4050e-02, -2.3640e-01,\n",
       "         2.8046e-01,  1.8657e-01,  8.7665e-02,  3.1472e-01, -7.0119e-02,\n",
       "        -1.8172e-01, -3.9074e-02,  2.8063e-02, -4.9746e-02,  1.0355e-01,\n",
       "        -1.5258e-01,  3.4939e-01, -1.6108e-01, -6.0705e-02, -9.0380e-03,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-0.2712   ,  0.14702  , -0.19126  , -0.24424  ,  0.16476  ,\n",
       "        -0.32537  ,  0.36027  ,  0.29804  , -0.49571  ,  1.2244   ,\n",
       "         0.097696 ,  0.30406  , -0.53059  , -0.25869  ,  0.16519  ,\n",
       "        -0.41392  , -0.2314   ,  0.86309  ,  0.35309  ,  0.41004  ,\n",
       "        -0.35114  ,  0.21699  ,  0.30512  , -0.30977  ,  0.065359 ,\n",
       "         0.13193  ,  0.59324  ,  0.089948 , -0.14227  , -0.31061  ,\n",
       "        -0.08432  ,  0.21609  , -0.57332  , -0.076418 , -0.0041399,\n",
       "        -0.48229  , -0.11635  , -0.0089557, -0.0064539, -0.031811 ,\n",
       "         0.16104  , -0.11247  , -0.042646 , -0.026045 ,  0.097288 ,\n",
       "        -0.030434 , -0.45127  , -0.028487 , -0.14567  , -0.10633  ,\n",
       "        -0.17795  , -0.097141 , -0.15773  , -0.42128  ,  0.25669  ,\n",
       "         0.53326  , -0.027647 ,  0.24529  , -0.46835  ,  0.090866 ,\n",
       "         0.12265  , -0.63001  , -0.25682  ,  0.0060041, -0.3454   ,\n",
       "         0.0072695,  0.17178  , -0.39181  , -0.060693 , -0.031147 ,\n",
       "         0.20888  ,  0.30827  ,  0.035736 , -0.23439  ,  0.28267  ,\n",
       "         0.08706  , -0.057023 , -0.30681  , -0.38813  , -0.039169 ,\n",
       "        -0.059306 , -0.36394  ,  0.26486  ,  0.29665  ,  0.12684  ,\n",
       "        -0.059682 ,  0.18076  ,  1.0117   , -0.020038 , -0.16224  ,\n",
       "        -0.069924 ,  0.29389  ,  0.023328 ,  0.67408  ,  0.13449  ,\n",
       "         0.021691 ,  0.21811  , -0.40404  , -0.13521  ,  0.11014  ,\n",
       "         0.19624  ,  0.05419  ,  0.16292  , -0.17251  ,  0.27033  ,\n",
       "        -1.616    ,  0.60791  ,  0.0033667, -0.29159  ,  0.14195  ,\n",
       "        -0.075748 , -0.19854  ,  0.27378  ,  0.087632 , -0.023092 ,\n",
       "        -0.24036  , -0.20799  , -0.062753 ,  0.040184 ,  0.032635 ,\n",
       "        -0.2035   ,  0.15727  , -0.67543  ,  0.67401  , -0.005282 ,\n",
       "         0.067059 ,  0.20846  , -0.65743  ,  0.17864  , -0.27987  ,\n",
       "        -0.022082 , -0.20144  ,  0.13538  , -0.12531  , -0.39399  ,\n",
       "         0.3964   , -0.11625  , -0.34535  ,  0.25273  ,  0.24191  ,\n",
       "        -1.6655   ,  0.61688  , -0.0035128,  0.37053  , -0.32058  ,\n",
       "        -0.48533  ,  0.1568   , -0.086594 ,  0.13272  , -0.050194 ,\n",
       "         0.12197  ,  0.0621   , -0.052821 ,  0.13192  ,  0.26901  ,\n",
       "         0.26882  , -0.47454  , -0.28357  ,  0.16668  , -0.51742  ,\n",
       "        -0.2906   ,  0.25908  ,  0.71942  ,  0.50009  , -0.092692 ,\n",
       "        -0.36577  , -0.034634 ,  0.056318 , -0.21117  ,  0.37486  ,\n",
       "         0.032773 , -0.14359  ,  0.6884   , -0.026806 ,  0.42897  ,\n",
       "         0.053205 , -0.42011  , -0.25344  , -0.21894  ,  0.14449  ,\n",
       "         0.045221 , -0.19444  ,  0.40097  ,  0.42644  , -0.39018  ,\n",
       "        -0.12735  ,  0.11559  ,  0.019183 , -0.027092 , -0.44417  ,\n",
       "         0.045983 ,  0.30359  ,  0.22169  ,  0.060414 , -0.0059974,\n",
       "        -0.66994  , -0.058462 ,  0.16644  , -0.1518   ,  0.0072898,\n",
       "        -0.21948  ,  0.45436  ,  0.05823  , -0.31103  , -0.23067  ,\n",
       "         0.0078952, -0.30799  ,  0.10996  ,  0.28958  , -0.033313 ,\n",
       "        -0.46981  ,  0.17656  , -0.077724 , -0.44687  , -0.13734  ,\n",
       "         0.44273  ,  0.43806  , -0.32096  ,  0.65241  , -0.29144  ,\n",
       "         0.35948  , -0.39277  ,  0.078458 ,  0.6577   ,  0.45683  ,\n",
       "         0.093801 , -0.25023  ,  0.046448 ,  0.04619  , -0.14565  ,\n",
       "         0.17054  ,  0.072038 , -0.86523  ,  0.20702  ,  0.45185  ,\n",
       "         0.20958  ,  0.043511 , -0.17528  , -0.15821  ,  0.011212 ,\n",
       "         0.41577  , -0.31444  , -0.17748  ,  0.089159 , -0.34662  ,\n",
       "        -0.097137 , -0.28784  , -0.38902  ,  0.21508  ,  0.31001  ,\n",
       "         0.030091 ,  0.30658  ,  0.23107  ,  0.048666 ,  0.13822  ,\n",
       "         0.4905   ,  0.66801  , -0.023158 ,  0.38607  ,  0.1293   ,\n",
       "        -0.027527 , -0.31134  ,  0.31469  ,  0.26038  ,  0.3263   ,\n",
       "         0.087236 , -0.014318 , -0.080271 , -0.14507  ,  0.34931  ,\n",
       "        -0.69845  , -0.25372  , -0.28831  , -0.038035 , -0.59529  ,\n",
       "         0.17554  ,  0.78562  , -0.066821 , -0.062887 ,  0.28903  ,\n",
       "         0.82979  ,  0.33097  ,  0.17223  , -0.27315  , -0.5014   ,\n",
       "        -0.29307  ,  0.43277  ,  0.59082  ,  0.059657 ,  0.19892  ,\n",
       "        -0.39105  , -0.020178 , -0.23956  ,  0.26956  ,  0.15734  ,\n",
       "         0.049013 ,  0.2598   , -0.15101  ,  0.069548 ,  0.058311 ,\n",
       "         0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ,  0.       ], dtype=float32),\n",
       " array([-1.6890e-02,  1.7402e-01, -3.0247e-01, -3.0063e-01,  2.1415e-01,\n",
       "         6.3863e-02,  1.0107e-01, -2.4155e-01, -9.5228e-02,  2.9253e+00,\n",
       "        -5.6759e-03,  1.1752e-01,  1.6121e-01,  2.0813e-02, -8.3593e-02,\n",
       "        -1.4048e-01, -4.0069e-02,  7.5133e-01, -5.6699e-02, -9.6198e-02,\n",
       "        -7.2134e-02, -3.1287e-02,  1.8146e-01, -2.4846e-01, -6.5068e-02,\n",
       "         6.6374e-02, -1.2173e-01, -1.3479e-01,  2.6163e-01, -2.1599e-01,\n",
       "        -2.4221e-01,  9.1074e-02, -4.3504e-02, -1.8047e-01, -1.8158e-01,\n",
       "        -6.6229e-02,  2.6480e-02, -2.5439e-01, -7.8805e-02, -2.1853e-01,\n",
       "        -3.0239e-02,  1.3049e-01,  1.0992e-01,  3.5563e-02,  3.2769e-01,\n",
       "        -2.7750e-02, -1.8852e-01, -1.8579e-01, -8.5064e-02, -2.4057e-02,\n",
       "        -1.4141e-01,  1.2708e-01, -7.9024e-02, -1.3320e-01,  3.7619e-02,\n",
       "        -1.1080e-01, -2.2742e-01, -2.7703e-01, -8.1760e-02,  4.7761e-02,\n",
       "        -1.7936e-01, -3.1907e-01,  1.3931e-01,  3.6374e-01, -2.1399e-01,\n",
       "        -2.7044e-01, -2.0847e-01,  1.1192e-02,  1.6017e-01,  2.4218e-01,\n",
       "         2.5058e-01,  3.2767e-01,  1.3340e-01,  6.4510e-03,  1.7994e-02,\n",
       "         1.1242e-01,  9.0136e-02, -7.2882e-02, -1.6506e-01,  4.8300e-01,\n",
       "         4.6465e-03, -4.8611e-02,  2.7421e-02,  9.7357e-02,  1.0873e-01,\n",
       "        -3.1722e-01,  2.6092e-01, -5.9396e-01,  2.6522e-01,  7.2456e-02,\n",
       "        -1.9246e-01,  3.4932e-02, -1.5662e-01,  3.0779e-01,  3.3744e-01,\n",
       "        -4.8664e-03,  1.5165e-01, -1.9861e-02, -1.4789e-01, -1.7031e-02,\n",
       "        -1.7279e-01, -3.8278e-02, -3.2560e-01, -1.6450e-01,  2.2257e-01,\n",
       "        -1.2442e+00, -1.4105e-01,  7.4932e-02, -1.8879e-01, -1.0341e-01,\n",
       "        -3.6646e-03, -2.1667e-01,  3.5087e-02, -1.9168e-01, -1.0044e-01,\n",
       "        -2.8554e-03,  5.6079e-02, -7.4098e-02, -8.3292e-03, -2.3693e-01,\n",
       "         4.1375e-02,  6.9771e-02,  2.0383e-01, -1.8666e-01, -4.2077e-02,\n",
       "         1.3305e-01, -4.5272e-02, -2.9631e-01,  1.6454e-01, -1.1591e-01,\n",
       "         2.4631e-02, -6.1717e-02, -6.2958e-02,  1.3487e-01,  2.0565e-01,\n",
       "         1.4678e-02,  1.9056e-01, -3.2541e-01,  1.0896e-01, -9.1117e-02,\n",
       "        -1.8126e+00,  3.5567e-01,  5.4440e-01, -3.5205e-02, -2.7339e-02,\n",
       "        -1.0750e-01, -3.0940e-01,  1.2539e-01,  8.7296e-02,  8.2909e-02,\n",
       "        -7.9026e-02, -1.8092e-02,  2.6749e-01,  1.1947e-02, -1.3090e-01,\n",
       "        -3.3304e-01, -2.0343e-01, -2.8020e-01, -1.0974e-01, -2.9613e-01,\n",
       "        -1.3973e-01,  1.8934e-01,  3.5228e-02, -2.5916e-01, -2.6859e-01,\n",
       "        -2.2678e-01,  1.3141e-01,  5.1456e-02,  2.6723e-01, -1.8335e-01,\n",
       "        -7.5100e-02, -1.6394e-02, -9.9408e-02, -1.8685e-01, -1.4718e-01,\n",
       "         6.4933e-02, -1.3858e-01, -1.9941e-01, -8.3017e-02, -1.7141e-01,\n",
       "         1.4692e-02, -2.4256e-01, -2.7010e-01, -8.5571e-02, -1.7614e-01,\n",
       "        -1.7786e-01, -1.3418e-02, -1.4634e-01,  3.0529e-01,  6.0719e-02,\n",
       "         2.8651e-01,  1.4169e-01, -1.9963e-01,  1.3507e-01,  7.0590e-02,\n",
       "        -4.2919e-02, -7.0586e-02, -3.5384e-01,  1.3877e-01,  2.4022e-01,\n",
       "        -3.0725e-02,  4.1167e-02, -2.2029e-01, -1.1796e-01,  1.1195e-01,\n",
       "         2.0739e-01, -1.6588e-01, -1.4708e-01,  9.7583e-02,  2.0831e-01,\n",
       "        -1.2357e-01, -7.9109e-02, -7.7406e-02, -3.4475e-01,  2.9539e-01,\n",
       "         1.4243e-02,  7.5820e-02, -1.9915e-02, -1.6280e-01,  1.9031e-02,\n",
       "         2.5743e-01,  1.2232e-01, -8.2438e-02,  6.1687e-02,  2.1967e-01,\n",
       "         2.2370e-01, -9.4008e-02,  1.5609e-01,  1.6797e-03,  1.8566e-01,\n",
       "        -2.6615e-01, -1.0925e-01,  3.6341e-01,  1.0227e-01,  1.6318e-01,\n",
       "        -1.5906e-01,  2.7914e-02, -9.7719e-02,  2.1670e-02,  1.3730e-01,\n",
       "         3.1447e-01,  1.6585e-02, -3.2731e-01,  4.1633e-01,  1.8380e-01,\n",
       "        -2.2986e-01, -1.2125e-01, -1.5239e-01, -1.3330e-02,  1.5810e-01,\n",
       "         3.2326e-01, -7.6780e-02, -1.2008e-01, -1.1215e-01, -5.0701e-02,\n",
       "         1.1711e-01,  1.4279e-01, -1.2161e-01, -1.5066e-02, -7.6299e-02,\n",
       "         2.2693e-01,  1.7511e-01,  2.3596e-02,  1.4218e-01,  2.1250e-01,\n",
       "         2.1344e-01, -6.8560e-02,  6.3065e-02,  5.5161e-01,  2.6941e-01,\n",
       "         1.7143e-01, -1.9773e-01,  3.0509e-02, -3.7473e-01, -2.3374e-01,\n",
       "         1.9453e-01,  2.3319e-03,  5.2309e-03,  1.6025e-01,  5.0800e-01,\n",
       "         3.0588e-01,  2.6994e-02,  1.1541e-01, -7.9795e-02, -9.9845e-02,\n",
       "        -3.1527e-02,  9.3683e-02,  1.0850e-02,  3.8706e-01, -2.6888e-01,\n",
       "        -4.0783e-01, -1.6802e-03,  1.5798e-02,  3.5886e-02,  1.1664e-01,\n",
       "         2.3909e-02, -5.3473e-02, -3.2640e-01,  1.7957e-01,  1.1095e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.1395e-01, -1.8600e-01, -3.1139e-01,  6.9232e-02, -2.2679e-02,\n",
       "         3.5245e-01, -1.7451e-01, -3.4762e-02,  3.6499e-02,  2.4352e+00,\n",
       "         1.5621e-01, -4.1787e-02, -7.2947e-02, -3.1921e-02,  8.9066e-02,\n",
       "         1.9200e-01,  1.6327e-02,  8.0790e-01, -2.3115e-02, -5.3932e-02,\n",
       "        -2.7845e-01,  2.0944e-02, -2.3572e-01,  8.7822e-02,  1.7885e-01,\n",
       "        -1.2684e-01,  1.1533e-01, -1.9993e-01,  1.7179e-01, -2.9116e-01,\n",
       "        -1.3022e-01,  1.4925e-02, -3.4743e-01, -4.2228e-01, -4.9583e-01,\n",
       "         2.5687e-01, -6.4040e-02,  1.4373e-01, -8.7055e-02,  2.3477e-02,\n",
       "         1.1067e-01,  3.4200e-01,  1.0643e-01, -5.5534e-03,  2.1959e-01,\n",
       "         3.2662e-01,  4.2147e-01, -1.1966e-01,  1.5098e-01,  5.5619e-02,\n",
       "        -3.8991e-01,  1.5516e-01, -8.2267e-02, -2.4025e-01, -3.4519e-01,\n",
       "        -1.9722e-01, -3.1244e-01, -5.8754e-01, -2.0353e-01, -7.6791e-03,\n",
       "        -9.1359e-02, -1.7571e-01, -1.7574e-01,  3.2297e-01, -2.6038e-01,\n",
       "        -2.8268e-01, -6.8955e-02,  2.6774e-02, -1.0006e-01,  3.6755e-01,\n",
       "        -1.4354e-01,  3.6205e-01, -1.1899e-01,  1.9502e-02,  6.3868e-01,\n",
       "        -1.4751e-02, -1.6021e-01,  4.8338e-01,  5.8345e-03,  3.2956e-01,\n",
       "         1.2950e-01, -2.2254e-01, -1.9471e-01, -2.5058e-01,  1.0409e-01,\n",
       "        -1.5040e-01, -3.0280e-01,  2.3811e-01,  2.9955e-01, -1.9751e-02,\n",
       "         1.8583e-03, -3.4478e-01,  1.4361e-01, -1.2800e-03,  4.9823e-01,\n",
       "        -6.4337e-02,  4.9903e-01,  5.1760e-01, -1.5414e-01, -3.1006e-01,\n",
       "        -1.6568e-01,  2.8804e-02, -2.9711e-01,  3.8346e-02,  2.3626e-01,\n",
       "        -1.5317e+00,  4.8050e-03, -4.0435e-01,  2.8585e-02,  1.5562e-01,\n",
       "         8.9044e-02, -8.2613e-01, -3.3854e-01, -2.1842e-01, -4.1949e-01,\n",
       "         1.4582e-01, -7.1290e-02, -1.7822e-01, -4.6116e-01,  8.1208e-02,\n",
       "         1.0228e-01, -2.2000e-02,  1.5704e-01, -4.6219e-01,  4.4219e-01,\n",
       "        -1.2331e-02,  6.0651e-01, -4.6436e-02,  7.1817e-02, -2.8741e-01,\n",
       "         8.0092e-02,  9.5995e-02,  2.1868e-01,  9.2623e-02,  5.9085e-02,\n",
       "        -1.5068e-01,  7.5694e-02, -4.2070e-01,  7.7992e-02,  2.3851e-01,\n",
       "        -1.4185e+00, -6.6634e-02,  1.6350e-01,  1.4849e-01, -1.9227e-01,\n",
       "         1.2907e-02, -3.4447e-01, -4.6486e-01,  1.4224e-01,  1.3265e-01,\n",
       "         8.8162e-02,  1.6292e-01,  1.3023e-01, -3.1362e-01, -3.9454e-01,\n",
       "        -1.4404e-01, -2.1622e-01, -2.0923e-01, -3.4271e-01, -1.6063e-01,\n",
       "         7.1870e-02, -1.9801e-01, -5.0524e-02,  7.4052e-05, -4.3107e-01,\n",
       "        -8.2001e-02, -3.8524e-01, -4.9691e-02,  1.0136e-01, -4.7003e-01,\n",
       "         1.1299e-01,  2.6024e-01, -1.6097e-01,  8.5316e-02, -2.4209e-01,\n",
       "         4.3188e-02, -2.8629e-01, -2.3318e-01,  1.9134e-01, -3.7870e-01,\n",
       "         1.9383e-01, -1.3177e-01, -4.2849e-01, -2.7833e-01, -6.3169e-02,\n",
       "        -2.3149e-01,  3.5011e-02, -2.3458e-01,  5.1698e-01,  8.0902e-02,\n",
       "         5.3812e-01, -1.0787e-01, -2.1465e-01, -3.5011e-01, -1.1482e-01,\n",
       "        -3.7470e-01, -1.4581e-01, -1.5781e-01, -8.8941e-02, -2.5458e-01,\n",
       "        -2.2646e-01, -1.4723e-01,  1.9318e-01,  1.5410e-01,  3.5199e-02,\n",
       "        -1.0698e-01, -4.4188e-02, -4.5023e-01,  2.6755e-01,  3.9788e-02,\n",
       "         1.6353e-01, -4.0591e-02,  1.4291e-01, -2.7215e-01,  2.4453e-01,\n",
       "         1.0331e-02,  2.1538e-01, -1.1237e-01, -3.7174e-02, -1.1926e-01,\n",
       "         2.9896e-01,  5.2738e-02, -8.8533e-02,  4.3619e-02, -1.0675e-01,\n",
       "         1.6294e-01,  2.5775e-01, -3.6536e-02,  9.5472e-02,  2.0618e-01,\n",
       "        -1.4716e-01, -2.3276e-01,  2.8959e-01,  3.0868e-02,  2.7450e-01,\n",
       "        -2.0971e-02,  2.8721e-01, -2.1334e-01,  4.2814e-01,  3.2721e-01,\n",
       "        -1.8220e-01,  2.6856e-01, -2.6014e-01,  2.3042e-01, -3.0705e-01,\n",
       "         2.9832e-01, -4.2529e-01, -3.5242e-01,  2.3699e-01, -5.1793e-03,\n",
       "         1.9576e-01,  1.3232e-01,  1.8231e-01,  9.8129e-02,  2.3562e-01,\n",
       "        -6.8167e-02,  1.2068e-01,  1.9259e-01, -1.3113e-01,  1.4483e-01,\n",
       "         3.3028e-01,  1.0524e-01, -1.6241e-01,  1.8550e-01, -5.3597e-02,\n",
       "         1.2893e-01,  1.0969e-01, -3.3200e-01,  1.4515e-01,  2.8574e-01,\n",
       "        -2.3060e-01, -2.6947e-01, -4.2454e-01, -9.0470e-01, -9.2193e-02,\n",
       "        -1.4784e-01,  5.1422e-02, -2.2871e-01,  1.0521e-01,  4.7294e-01,\n",
       "         2.0239e-01, -1.6172e-02, -2.8370e-02, -1.0857e-01,  2.3019e-01,\n",
       "         1.2928e-01, -3.9928e-01, -5.9975e-02, -1.4594e-01, -5.7901e-02,\n",
       "        -7.0202e-01, -1.8404e-01, -8.4864e-02,  4.5815e-02, -2.0621e-01,\n",
       "         4.5970e-01, -9.3192e-02, -6.1150e-01, -7.5240e-02, -1.2648e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7386e-02,  2.1880e-01,  1.7469e-01,  2.7509e-01, -2.1631e-01,\n",
       "         3.5347e-01, -2.2180e-02, -2.8373e-01,  2.5881e-01,  1.9462e+00,\n",
       "        -4.0591e-02, -1.7405e-01,  1.3791e-01,  5.1707e-01, -1.1379e-01,\n",
       "         4.3605e-01,  4.4465e-01,  1.5117e+00, -3.5965e-01,  4.3858e-02,\n",
       "        -5.1923e-02,  2.9570e-01, -7.3748e-01, -5.4741e-03, -1.7881e-01,\n",
       "        -2.2322e-01,  6.4444e-01, -5.1164e-01,  9.3286e-02, -3.9014e-01,\n",
       "        -2.3993e-01, -3.4526e-01,  6.7683e-03, -4.2794e-01,  1.0783e-01,\n",
       "         4.2653e-01,  3.0891e-01, -1.0818e-01,  1.8644e-01, -2.1826e-01,\n",
       "        -4.1482e-01,  1.0898e-02, -1.7486e-01,  3.9860e-01, -2.0510e-01,\n",
       "         4.8380e-01, -5.0375e-02,  8.0554e-01, -9.0117e-02,  9.7864e-02,\n",
       "         1.5842e-01, -3.6625e-01,  3.8516e-01, -3.7493e-02, -3.4814e-01,\n",
       "         9.3430e-02,  9.1870e-01, -2.3090e-01,  3.5374e-01,  6.5830e-02,\n",
       "         1.1644e-01,  5.0977e-01, -1.5380e-01, -1.8577e-01,  4.4437e-01,\n",
       "         2.7601e-01,  2.2735e-01,  5.3475e-01, -1.8252e-01, -7.4210e-02,\n",
       "         2.1165e-01, -5.0073e-01,  1.2662e-01, -1.2315e-01,  3.8191e-01,\n",
       "        -1.6075e-01,  5.3988e-02, -1.9419e-01,  1.7041e-02, -2.8735e-01,\n",
       "         4.7740e-01, -6.2051e-01, -3.0525e-01, -3.2444e-01, -8.1936e-02,\n",
       "         2.4708e-01,  1.6119e-01,  1.4163e-01, -2.6534e-02,  6.8003e-01,\n",
       "         3.8574e-01,  7.9060e-01,  1.7182e-01,  1.9055e-01, -4.0842e-01,\n",
       "        -7.1388e-01, -2.4055e-01,  3.0572e-01,  3.2042e-01,  9.4508e-02,\n",
       "         1.9716e-01, -3.2531e-01, -4.6567e-02,  4.9304e-03,  2.3895e-01,\n",
       "        -1.5985e+00,  4.4618e-01,  6.7059e-02,  4.2562e-02, -9.4264e-02,\n",
       "         2.6359e-01,  5.9031e-02,  2.1923e-01,  1.1912e-01, -4.8703e-01,\n",
       "        -1.6320e-01,  1.3074e-01,  1.3469e-01, -8.6061e-04,  6.5107e-02,\n",
       "         2.6348e-01, -6.8230e-02, -3.4943e-01,  1.4124e-03, -3.2127e-01,\n",
       "        -3.2119e-01,  2.4275e-01,  1.4489e-01,  5.2155e-01,  1.8236e-02,\n",
       "         3.8936e-01,  1.7392e-01, -1.8137e-01,  2.3953e-01,  7.9878e-02,\n",
       "         2.3554e-01,  3.4491e-01,  1.3673e-01, -2.4935e-02,  1.2661e-01,\n",
       "        -9.4117e-01, -2.7312e-01, -3.9914e-02, -2.1242e-02, -2.4627e-01,\n",
       "         1.8920e-01, -3.7878e-02, -7.5084e-01,  3.1216e-01, -8.1300e-02,\n",
       "        -1.4081e-01,  1.9424e-01,  5.0618e-01,  5.5668e-02, -2.2838e-01,\n",
       "        -7.9835e-02,  3.3727e-01, -3.9557e-01, -3.1844e-01, -6.1797e-01,\n",
       "        -4.3276e-01, -1.0177e-02, -1.6727e-01, -1.4701e-01, -1.3138e-03,\n",
       "        -3.6874e-01,  8.8321e-03,  1.0844e-01,  2.2943e-01, -5.6433e-01,\n",
       "         3.1256e-01, -1.9476e-01, -7.2129e-02, -8.9461e-01, -5.7574e-01,\n",
       "        -9.0824e-02, -6.1415e-01,  1.7660e-01, -2.6355e-01,  6.2693e-01,\n",
       "        -4.5756e-01, -8.6851e-01, -2.4981e-01,  5.5472e-01, -8.5896e-02,\n",
       "         1.1540e-01,  3.1408e-01,  3.9062e-01, -3.4876e-01,  5.4840e-02,\n",
       "        -2.2244e-01,  5.9099e-02,  9.9086e-02, -5.1477e-01, -1.7693e-01,\n",
       "         1.3470e-01, -2.5853e-01, -4.6366e-01,  2.2667e-01,  2.6040e-01,\n",
       "        -3.4442e-01,  2.5386e-01,  3.2574e-02,  3.5130e-01, -6.8617e-01,\n",
       "        -1.3479e-01, -1.5026e-01,  1.6556e-01,  3.2562e-01,  1.3296e-01,\n",
       "         2.2568e-01,  2.6165e-01,  9.7049e-03,  4.1276e-02, -3.9056e-01,\n",
       "        -2.5348e-01,  1.1446e-01, -2.1078e-01,  3.7831e-02, -4.9510e-02,\n",
       "         3.3036e-01,  2.0437e-01, -5.9281e-01,  1.0520e-01, -4.3911e-01,\n",
       "         5.1661e-01, -1.9186e-01,  6.2483e-01,  2.6081e-02, -1.2373e-01,\n",
       "        -2.3343e-01,  7.1194e-01,  3.1010e-01,  1.5132e-01, -2.8571e-01,\n",
       "         2.9020e-02, -6.9205e-02, -2.8972e-01,  1.1664e-03,  9.4657e-03,\n",
       "        -3.1325e-01, -3.6959e-01,  6.3799e-01,  4.5772e-03, -4.4835e-02,\n",
       "        -1.0021e-01,  2.0841e-01, -8.3103e-02,  3.5557e-02,  5.7080e-01,\n",
       "         1.3201e-02,  6.1434e-02,  7.9083e-02, -5.3205e-02,  1.2540e-01,\n",
       "         1.2251e-01,  2.2707e-01, -6.0252e-02, -1.3078e-01,  1.8732e-01,\n",
       "        -2.4053e-01,  3.9825e-01,  9.7576e-02, -6.0194e-01, -9.7147e-02,\n",
       "         5.0851e-01,  9.4998e-02,  2.9620e-01,  6.9577e-01,  2.7756e-03,\n",
       "         6.0900e-02, -2.9739e-01, -2.0041e-01, -3.3111e-01,  3.2232e-01,\n",
       "         2.9970e-01,  7.8138e-02, -2.1120e-01,  2.4587e-01,  7.2880e-01,\n",
       "        -5.8055e-02, -3.1365e-02,  5.0065e-01, -4.5560e-03, -2.1692e-01,\n",
       "        -1.8249e-01, -3.7010e-01,  1.9931e-01,  3.1825e-02, -1.2789e-01,\n",
       "         5.8350e-01,  4.7768e-01,  3.3556e-01,  1.1028e-01,  1.5622e-01,\n",
       "        -7.6022e-02,  2.5118e-01,  5.3027e-02, -7.8588e-02, -4.7783e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([-3.7675e-01, -1.1799e-01,  1.7866e-01,  3.2482e-02, -4.9160e-01,\n",
       "         4.4509e-02, -3.8697e-01,  3.4402e-02, -4.0194e-02,  3.1124e+00,\n",
       "        -2.2361e-01,  1.1824e-01,  6.9428e-02, -3.2642e-02,  1.4955e-01,\n",
       "         1.5286e-01,  3.2991e-02,  6.3527e-01, -2.3576e-01,  2.4908e-01,\n",
       "         2.0384e-01,  1.4182e-01,  2.0118e-01, -2.0678e-01, -1.6844e-01,\n",
       "         1.3097e-01,  4.8774e-01, -3.5005e-01, -7.5147e-02, -1.5022e-01,\n",
       "         1.6138e-01, -5.5018e-01, -1.9398e-01, -2.8315e-02,  1.6917e-01,\n",
       "        -4.0531e-01,  6.5695e-02, -1.3529e-01,  1.7965e-01,  7.5198e-02,\n",
       "         1.2148e-01,  6.1736e-01,  3.6658e-02, -5.6777e-01,  3.2880e-01,\n",
       "        -2.4745e-01,  1.3974e-01,  7.4644e-01, -1.9929e-01,  3.6678e-01,\n",
       "         2.9422e-01, -8.5806e-02,  1.2210e-01,  1.6887e-01, -2.1271e-01,\n",
       "         7.3281e-03, -7.6509e-02,  1.7360e-01,  3.0140e-01, -6.2679e-02,\n",
       "        -2.4980e-01, -6.2602e-02, -2.8363e-01,  1.1710e-01,  1.4115e-01,\n",
       "        -5.0121e-01,  1.2329e-01, -1.8731e-01,  3.9721e-01, -2.5896e-02,\n",
       "         2.9596e-01, -3.2547e-02,  1.7333e-01, -1.1771e-01,  1.2088e-01,\n",
       "        -3.5890e-01, -1.3854e-01, -8.9703e-02,  3.1391e-01, -9.1165e-02,\n",
       "        -1.3064e-01, -2.2216e-02,  1.5877e-02,  2.6226e-01, -2.7216e-01,\n",
       "        -1.5718e-01, -8.3953e-01, -1.4901e-01,  1.2313e-01,  3.2840e-01,\n",
       "        -1.1891e-01,  5.5510e-02,  1.8767e-01,  5.3449e-01, -2.0262e-01,\n",
       "         1.6928e-01, -1.9827e-01, -2.4103e-01, -1.8439e-02, -9.7009e-02,\n",
       "        -7.1952e-02, -3.6047e-01, -9.6811e-02, -2.9746e-01, -2.7191e-02,\n",
       "        -1.4366e+00,  4.3149e-01,  1.3098e-01, -7.2661e-02,  1.7736e-01,\n",
       "         1.6811e-01,  1.7400e-02,  1.7484e-01, -2.2591e-01,  4.0063e-01,\n",
       "         1.2184e-01,  6.9323e-02, -1.1818e-01,  3.1484e-01, -1.0699e-01,\n",
       "         4.0806e-01, -1.2300e-01, -3.1155e-01,  2.1126e-01, -5.7120e-02,\n",
       "         8.6237e-02, -4.4460e-01,  2.2277e-04,  2.4001e-02,  3.0837e-01,\n",
       "        -1.6179e-01,  2.5340e-01, -4.2660e-02,  2.5280e-03,  9.9567e-02,\n",
       "        -5.1727e-02,  3.5747e-02, -1.7966e-01,  3.3886e-01, -1.6398e-01,\n",
       "        -7.8192e-01,  6.0500e-01,  2.8252e-01, -1.2994e-01, -1.6583e-02,\n",
       "        -3.4698e-01, -7.2527e-01,  8.0799e-02,  9.4704e-02,  3.6744e-01,\n",
       "        -8.9103e-02,  6.9915e-02,  1.6852e-01, -1.2281e-01, -2.8876e-02,\n",
       "        -7.7054e-02, -1.8588e-01, -1.8323e-01, -1.9980e-01,  4.5056e-02,\n",
       "        -7.6702e-02,  3.8135e-01, -3.8801e-01, -2.1650e-01, -3.0306e-02,\n",
       "         1.6275e-01, -1.0535e-02, -1.6363e-01,  7.2000e-04, -2.2225e-02,\n",
       "        -7.1306e-02, -2.7842e-02,  2.6646e-01, -2.7049e-01, -2.1350e-02,\n",
       "        -4.4271e-02, -1.2112e-01,  1.1946e-01, -1.1000e-02,  3.2661e-01,\n",
       "        -1.8398e-01,  7.4138e-02, -4.2220e-02,  9.7410e-02, -1.4307e-01,\n",
       "         1.0406e-02,  2.1065e-02,  1.9372e-01, -1.3614e-01,  1.1002e-01,\n",
       "         1.5361e-01, -5.0540e-02,  8.6166e-02,  2.4223e-01,  1.4647e-01,\n",
       "        -1.3835e-01, -6.9210e-02, -1.4987e-01,  1.8680e-01,  3.9816e-01,\n",
       "         5.7365e-01,  1.5033e-02,  4.5188e-02,  1.6298e-01,  9.3265e-02,\n",
       "        -7.3806e-02, -3.6836e-01, -1.0767e-01,  4.6960e-01, -1.3725e-01,\n",
       "         1.1711e-01,  2.3232e-01,  2.1569e-02,  8.2387e-03,  2.0852e-01,\n",
       "         1.8448e-01,  2.6981e-01, -7.7155e-02, -1.5074e-01, -1.4044e-01,\n",
       "         1.5819e-01, -3.1127e-01, -2.3062e-01,  1.6080e-01,  1.3612e-01,\n",
       "        -6.8917e-02,  2.7529e-01, -2.0376e-01, -4.3880e-02,  3.3157e-01,\n",
       "         1.8735e-01,  5.5636e-02,  1.2360e-01, -1.1264e-01,  1.3654e-01,\n",
       "         3.6212e-01, -2.0679e-01,  2.2256e-02,  8.0054e-02, -5.1004e-02,\n",
       "         1.5415e-01, -4.8844e-01, -1.7730e-01,  4.1286e-01, -2.8209e-01,\n",
       "        -2.8011e-01, -1.5195e-01, -3.5349e-01,  3.4257e-01,  1.2766e-01,\n",
       "        -4.7474e-02,  3.2728e-01, -6.9187e-01,  1.7493e-01,  8.9532e-02,\n",
       "        -6.1105e-02, -6.9829e-02, -1.5193e-01,  5.7987e-02,  3.0111e-02,\n",
       "        -3.9252e-02,  5.3934e-02,  4.5133e-01,  4.2870e-02, -2.2657e-01,\n",
       "        -3.5830e-01, -5.6488e-02,  1.0440e-01,  4.9360e-01,  2.7961e-01,\n",
       "         5.4662e-01, -1.3974e-01, -2.1185e-01, -4.5541e-01, -6.9248e-02,\n",
       "        -3.5238e-01,  2.2941e-01, -2.4661e-01,  1.6707e-01,  2.9528e-01,\n",
       "        -1.2137e-01, -2.0418e-01,  2.4620e-01,  3.4143e-01, -2.9176e-01,\n",
       "        -5.7014e-01, -2.6190e-01, -3.2669e-01,  3.1928e-01, -3.0172e-01,\n",
       "        -1.3678e-01,  4.1218e-01,  1.1811e-01,  2.8056e-01,  4.2607e-02,\n",
       "        -4.6627e-01,  2.9769e-01, -4.3038e-02, -5.6063e-01,  2.5515e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0.,\n",
       "        0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_test[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3798e-02,  2.4779e-02, -2.0937e-01,  4.9745e-01,  3.6019e-01,\n",
       "       -3.7503e-01, -5.2078e-02, -6.0555e-01,  3.6744e-02,  2.2085e+00,\n",
       "       -2.3389e-01, -6.8360e-02, -2.2355e-01, -5.3989e-02, -1.5198e-01,\n",
       "       -1.7319e-01,  5.3355e-02,  1.6485e+00, -4.7991e-02, -8.5311e-02,\n",
       "       -1.5712e-01, -6.4425e-01, -3.9819e-01,  2.7800e-01,  1.5364e-01,\n",
       "        3.1678e-02,  5.5414e-02,  1.5939e-02,  3.1851e-01, -5.8979e-02,\n",
       "        3.8584e-02,  1.0770e-01,  1.0410e-01, -7.7346e-02,  3.7396e-01,\n",
       "       -2.1482e-01,  3.8320e-01, -2.7737e-01, -1.8352e-01, -8.3838e-01,\n",
       "        3.4124e-01,  5.8164e-01,  1.8543e-01, -3.1028e-01,  1.7666e-01,\n",
       "       -6.9421e-02, -3.4422e-01, -1.3665e-01, -1.0823e-01,  2.3637e-01,\n",
       "       -3.2923e-01,  6.1348e-01,  1.9720e-01,  8.7123e-02,  1.0785e-01,\n",
       "        3.0730e-01,  1.3757e-01,  3.0809e-01,  2.4331e-01, -2.9422e-01,\n",
       "       -9.8214e-03,  5.5675e-01, -4.8880e-02,  9.9468e-02,  3.0543e-01,\n",
       "       -3.7597e-01, -1.9525e-01,  4.6246e-02, -3.6675e-02,  3.4023e-01,\n",
       "        1.4905e-01,  9.7800e-02, -2.6664e-01,  5.6834e-02, -4.3201e-02,\n",
       "       -2.3338e-01,  1.3111e-01, -3.5742e-01, -3.6070e-01,  3.0997e-01,\n",
       "       -1.9727e-01, -1.4320e-01, -1.6747e-01,  4.2435e-04, -1.5120e-01,\n",
       "        6.7562e-02, -3.8644e-01,  2.5349e-02,  2.4918e-01, -2.3955e-01,\n",
       "       -1.5615e-01,  4.9868e-01,  8.2758e-03, -1.9120e-01, -1.4906e-01,\n",
       "        4.8757e-01, -1.5281e-02,  1.0196e-02,  3.7642e-01, -1.9460e-02,\n",
       "       -2.7835e-01,  1.6355e-01, -2.4127e-01, -2.1405e-01, -2.1562e-01,\n",
       "       -7.9697e-01,  3.4321e-01,  9.3209e-02,  7.3977e-02, -2.7147e-01,\n",
       "        2.0539e-01,  1.5061e-01,  2.0734e-02,  1.1267e-01,  2.8714e-02,\n",
       "        2.9670e-01, -2.1267e-01,  4.3214e-01,  1.2788e-01,  2.9249e-01,\n",
       "        1.9056e-01, -2.9113e-01, -1.1382e-01, -3.8242e-02, -2.0290e-01,\n",
       "        1.8301e-01, -1.6661e-01, -2.7116e-01,  1.2685e-03,  7.1704e-02,\n",
       "       -1.8583e-01,  8.9850e-02, -3.9895e-02,  3.9479e-01,  5.3211e-03,\n",
       "       -6.1548e-04, -2.7082e-01, -8.9782e-02, -2.8790e-01, -1.4865e-01,\n",
       "       -1.3746e+00,  1.6515e-01,  2.0598e-01,  1.5252e-01,  3.4723e-02,\n",
       "       -3.8531e-01, -9.4574e-02, -1.9871e-01,  5.0239e-01, -2.8702e-01,\n",
       "       -8.8727e-02,  5.6881e-02,  1.3634e-01,  1.9034e-01, -1.9353e-01,\n",
       "        4.0506e-01, -1.9317e-01,  2.2908e-01,  1.0055e-01, -2.6895e-01,\n",
       "       -3.4727e-02, -8.4010e-02,  5.7806e-02,  1.1076e-02, -4.3349e-02,\n",
       "       -2.6917e-01, -1.9333e-01,  2.2181e-01,  2.6123e-01, -1.1761e-01,\n",
       "        1.0092e-01, -1.5078e-01,  4.7153e-01,  1.1253e-01, -2.6749e-01,\n",
       "       -3.8785e-02, -3.6520e-02, -8.9248e-02, -2.4427e-01, -4.1381e-02,\n",
       "       -2.1785e-02, -3.5738e-01, -6.3409e-02, -5.3983e-01, -1.0112e-02,\n",
       "        4.1238e-04, -9.7049e-02,  4.2628e-01, -2.1349e-01, -4.1055e-01,\n",
       "       -2.4940e-01, -3.3571e-02, -4.9540e-01,  1.5557e-01,  1.9882e-01,\n",
       "        1.0498e-01, -2.4372e-01,  1.1429e-01, -3.9279e-02, -3.6258e-01,\n",
       "        1.0318e-01,  1.2900e-01, -4.1785e-01, -4.1607e-02,  3.3522e-01,\n",
       "        7.3186e-02,  1.3362e-01,  1.0812e-02,  5.2645e-02,  1.8801e-01,\n",
       "       -3.0185e-01,  2.0333e-01, -3.2258e-01, -2.4673e-01,  2.1124e-01,\n",
       "        7.9132e-01, -4.1539e-01,  3.6220e-01,  9.9852e-02, -3.5378e-02,\n",
       "       -4.1900e-02, -1.3851e-01, -6.3255e-02,  1.3635e-01,  9.0863e-02,\n",
       "       -3.9940e-01,  9.9062e-02,  3.2210e-01, -1.2256e-01, -8.5906e-02,\n",
       "       -1.0218e-01,  2.6350e-01, -1.8689e-01, -1.8560e-01, -4.3923e-01,\n",
       "       -3.2500e-01, -1.9910e-01,  1.7831e-01, -2.7283e-01,  3.3473e-01,\n",
       "        8.2382e-02,  1.2825e-01,  3.9275e-01, -3.4929e-02,  1.6148e-01,\n",
       "       -2.6713e-02,  4.0129e-01, -3.9503e-01, -6.4823e-02, -8.9820e-02,\n",
       "       -6.6592e-02, -3.4537e-01,  4.6283e-02,  3.6837e-01, -2.4573e-02,\n",
       "        3.2213e-01,  3.0641e-01, -2.8112e-01,  6.6449e-03,  8.7743e-02,\n",
       "       -3.4170e-02,  6.0373e-01,  4.2120e-01, -7.3349e-02,  2.6682e-01,\n",
       "       -1.5860e-01,  2.3765e-01, -6.2604e-03,  1.5236e-01, -2.3409e-01,\n",
       "        3.1634e-01, -8.7860e-02, -1.5747e-01, -2.4955e-01, -1.8766e-01,\n",
       "       -9.6743e-02, -2.7994e-01, -2.4334e-01,  3.2643e-01,  2.9906e-01,\n",
       "        4.2763e-01,  2.2266e-01, -1.7464e-01, -1.9916e-02, -3.1206e-01,\n",
       "       -3.4009e-01, -1.4993e-01, -2.8818e-01,  1.4750e-01, -4.0503e-02,\n",
       "       -1.0347e-01,  3.3634e-03,  2.1760e-01, -2.0409e-01,  9.2415e-02,\n",
       "        8.0421e-02, -6.1246e-02, -3.0099e-01, -1.4584e-01,  2.8188e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[3].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156060"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data.Dataset(examples, fields)\n",
    "data_set_test = data.Dataset(examples_test, fields_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.sort_key = lambda x: len(x.text)\n",
    "data_set_test.sort_key = lambda x: len(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = data_set.split([0.8, 0.1, 0.1], random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sort_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124848"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(train_data.sort_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_set_test.sort_key = lambda x: len(x.text)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsource(data_set_test.sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)\n",
    "\n",
    "kaggle_test_iterator = data.BucketIterator(\n",
    "    data_set_test, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_iterator = iter(train_iterator)\n",
    "batch = next(raw_train_iterator)\n",
    "\n",
    "raw_kaggle_test_iterator = iter(kaggle_test_iterator)\n",
    "batch_test = next(raw_kaggle_test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = batch.text\n",
    "\n",
    "c, d = batch_test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9., 9.,\n",
       "        9., 9., 9., 9., 9., 9., 9., 9., 9., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 9, 308])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 1, 3, 3, 1, 3, 3, 2, 2, 2, 2, 2, 0, 2, 1, 4, 3, 2, 3, 2,\n",
       "        2, 2, 1, 3, 1, 2, 2, 1, 3, 1, 2, 2, 1, 4, 2, 3, 2, 3, 2, 2, 1, 4, 2, 2,\n",
       "        2, 3, 2, 3, 1, 2, 2, 3, 4, 2, 2, 4, 2, 2, 1, 4], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[133918,  57783,  94049,  86671,  84086,  74128, 112683, 126943,  12599,\n",
       "         138419,  47937,  62635, 104584,  86587, 113162,  81036,  62472, 147974,\n",
       "          32209, 133654,  95241, 137072,  66415,  75555,  53039, 130485,  79837,\n",
       "         152257,  39951, 136680, 121126, 102615, 101662, 119728, 108929, 145776,\n",
       "          36689,  66928, 109732, 109843, 130506,  33026, 123268, 155495, 114095,\n",
       "          27453, 122599,  82520,  21535, 104973,  91096,  97182,  84542, 119081,\n",
       "         104435,  65401,  30863, 141808,  39997, 145932,  46149, 125167, 132902,\n",
       "         117864]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data['Sentiment'][117864-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10.], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10, 308])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test.phrase_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhraseId                                                       193462\n",
       "SentenceId                                                      10289\n",
       "Phrase                         you 're not into the Pokemon franchise\n",
       "Phrase_length                                                      72\n",
       "Tokenized_phrase    [xxbos, you, 're, not, into, the, xxmaj, pokem...\n",
       "Indexed_phrase               [2, 33, 157, 41, 61, 8, 7, 2893, 978, 3]\n",
       "Name: 37401, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.iloc[37401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "        -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "         1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "        -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "         6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "        -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "        -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "         8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "        -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "         1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "         5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "        -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "         1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "        -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "         2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "        -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "         3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "        -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "        -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "         1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "        -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "        -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "        -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "         7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "         6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "        -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "        -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "        -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "        -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "        -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "         4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "         3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "        -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "        -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "         7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "        -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "         8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "         8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "         3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "         1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "         1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "        -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "         2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "         4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "        -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "        -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "        -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "         2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "         2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "         1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "        -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "         1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "         3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "         1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "        -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "        -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "         1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "        -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "        -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "         1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         0.0000e+00,  0.0000e+00,  0.0000e+00], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[63][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2598e-01,  2.3681e-01, -3.0048e-01, -3.4822e-02,  1.4940e-01,\n",
       "       -3.6147e-02, -2.9160e-02,  4.2446e-02, -1.4922e-02,  2.4803e+00,\n",
       "        1.3138e-01,  1.8270e-01,  1.9732e-01, -1.1765e-01, -1.9910e-01,\n",
       "       -9.9976e-02, -1.3003e-02,  8.6874e-01, -2.5387e-01, -4.7935e-02,\n",
       "        6.2166e-02, -6.8214e-02,  9.0818e-02,  1.8553e-01, -4.6234e-01,\n",
       "       -7.4295e-02, -1.1136e-01, -1.7341e-01,  6.2637e-01, -5.4815e-01,\n",
       "       -2.3186e-01,  2.1671e-01,  2.3657e-01,  3.4814e-03,  2.1973e-01,\n",
       "        8.1694e-02,  7.2784e-02,  3.2418e-01,  1.8485e-01, -7.8116e-02,\n",
       "       -1.0398e-01,  2.5184e-01, -3.2611e-01,  2.1050e-01,  1.7217e-01,\n",
       "        1.4633e-01, -2.4610e-01, -1.2313e-01,  1.8549e-02, -1.7679e-01,\n",
       "        5.4608e-02,  1.9721e-02, -1.4351e-01, -9.8868e-02,  1.9156e-01,\n",
       "       -1.8175e-01, -9.6741e-03, -2.8860e-01,  3.2727e-01,  8.7877e-02,\n",
       "        1.5791e-01, -5.9451e-01, -4.0692e-01,  1.0203e-01,  2.0662e-01,\n",
       "       -3.0393e-01, -1.7494e-01,  1.6815e-01,  1.3331e-01,  2.0567e-01,\n",
       "        2.1249e-02,  2.3856e-01,  5.4913e-01, -1.0505e-01,  1.0088e-01,\n",
       "       -3.5271e-02,  4.2108e-01, -4.6818e-02,  7.6190e-02,  5.4104e-01,\n",
       "        3.5311e-02, -8.5459e-02, -2.3098e-01,  6.2153e-02, -1.4791e-01,\n",
       "       -2.1184e-01, -1.7303e-01, -3.4038e-01,  5.1728e-01,  5.9601e-02,\n",
       "       -1.2829e-01,  1.7726e-01, -4.4329e-03,  1.3955e-01, -1.7913e-01,\n",
       "        1.3499e-01, -3.0629e-01, -4.0483e-01,  9.3147e-02, -6.9782e-02,\n",
       "       -1.1435e-01,  1.5556e-01, -1.6525e-01, -7.2852e-03,  4.5260e-01,\n",
       "       -4.5582e-01,  2.3156e-01, -3.3502e-01,  3.8883e-02,  3.6845e-01,\n",
       "       -1.0226e-02, -3.2910e-01,  7.2277e-01, -1.9368e-01,  2.6302e-01,\n",
       "        7.8934e-02,  7.9095e-02,  2.2046e-01, -4.1357e-01, -2.6871e-02,\n",
       "        6.6428e-02, -2.2310e-01,  2.1052e-02, -8.7812e-02,  5.7772e-01,\n",
       "       -6.2890e-02,  1.1224e-01, -3.0373e-01, -4.3458e-01, -2.4094e-01,\n",
       "       -1.0639e-01, -1.0959e-01, -1.4303e-01, -1.0129e-01,  2.0117e-01,\n",
       "       -1.6260e-01, -2.6334e-01, -2.8480e-01, -1.3761e-01,  3.0538e-02,\n",
       "       -2.1268e+00,  2.2864e-01, -2.1886e-01, -9.4784e-02, -7.0648e-02,\n",
       "       -3.2755e-01,  7.3180e-02,  2.5432e-01,  1.7743e-02, -2.0381e-01,\n",
       "        4.8345e-02, -8.5388e-02, -5.1551e-02,  2.6829e-02, -1.7335e-02,\n",
       "        3.7336e-01, -6.6955e-02, -2.0040e-01, -5.5134e-02, -1.4291e-01,\n",
       "       -4.1980e-01,  1.5889e-01, -3.4629e-01, -1.6996e-01, -1.0990e-01,\n",
       "       -2.7387e-01,  5.0658e-01, -4.6021e-01,  1.2509e-01, -9.1655e-02,\n",
       "        7.9102e-02,  7.0087e-02,  2.0379e-01, -2.0391e-02, -7.6817e-02,\n",
       "       -1.1453e-01, -2.3451e-01,  7.9403e-02,  9.4492e-02,  1.0165e-01,\n",
       "        8.3505e-02, -8.7970e-02, -3.1083e-01, -1.7076e-01,  6.6609e-02,\n",
       "        8.9503e-02,  2.5501e-01, -1.3593e-01,  2.6518e-02,  8.6873e-02,\n",
       "        3.4049e-01,  1.0440e-02,  1.5305e-01, -2.9087e-01,  6.7273e-02,\n",
       "        1.9965e-01,  1.0600e-01,  1.0882e-01,  1.9005e-02,  4.4465e-01,\n",
       "        1.6081e-01, -2.3920e-01,  1.7224e-01,  2.8367e-02,  8.5946e-02,\n",
       "       -4.9735e-02, -8.4397e-03, -1.7956e-02,  5.3168e-01, -4.8747e-01,\n",
       "        2.0351e-01, -4.0947e-01,  1.1752e-01, -4.2937e-01,  1.7858e-01,\n",
       "        4.3189e-01,  9.7437e-03,  2.7766e-01, -5.4580e-01, -2.6416e-01,\n",
       "       -1.6082e-01,  2.7527e-01, -1.2740e-01,  2.3949e-02,  3.8710e-01,\n",
       "       -4.7191e-01, -4.5332e-02,  5.3680e-02, -1.1380e-01, -4.7495e-01,\n",
       "       -1.6382e-01, -7.8423e-02,  1.8142e-01, -1.3703e-01, -6.0409e-02,\n",
       "        2.2847e-01,  1.5158e-01, -8.3614e-01, -2.2553e-01,  3.8352e-01,\n",
       "        2.4135e-01,  3.9261e-02,  2.2913e-01,  2.7824e-01, -3.6930e-01,\n",
       "        1.2437e-01, -3.0092e-01, -1.0804e-01, -3.1174e-02,  4.8490e-01,\n",
       "       -2.9842e-01, -1.5944e-01,  5.2066e-02,  5.9537e-02, -1.3802e-01,\n",
       "        1.5527e-01, -2.4373e-01, -2.0169e-01,  2.1848e-01,  5.5785e-01,\n",
       "        3.0584e-02,  6.3418e-01,  7.1647e-02, -1.0504e-01,  2.1907e-01,\n",
       "        1.6623e-01,  1.2458e-01,  4.6116e-01,  5.4494e-01,  1.0317e-01,\n",
       "       -4.3065e-01, -2.4416e-01, -6.9782e-01, -4.7639e-01, -6.9007e-02,\n",
       "       -2.1339e-01,  1.8076e-01, -5.7100e-02,  9.4077e-02,  1.0082e-01,\n",
       "        1.5056e-01, -1.2696e-01, -1.2386e-01, -2.8849e-01,  4.4003e-02,\n",
       "       -2.5213e-01, -1.6345e-03, -7.0346e-01,  4.5023e-02,  3.0148e-01,\n",
       "       -2.6656e-01, -1.2663e-02, -2.8847e-01, -2.3672e-01,  1.0115e-01,\n",
       "        1.3502e-01, -3.4870e-01, -1.8094e-01,  1.6793e-01, -1.9305e-01,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "        0.0000e+00,  0.0000e+00,  0.0000e+00], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.get_vector(\"'re\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, context_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM( embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if bidirectional:\n",
    "            ## Word-level hierarchical attention:\n",
    "            self.ui = nn.Linear(2*hidden_dim, context_dim)\n",
    "            self.uw = nn.Parameter(torch.randn(context_dim))\n",
    "            \n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            ## Word-level hierarchical attention:\n",
    "            self.ui = nn.Linear(hidden_dim, context_dim)\n",
    "            self.uw = nn.Parameter(torch.randn(context_d))\n",
    "            \n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(text, text_lengths, batch_first=True)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        #output = [batch size, senq len, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            ## Word-level hierarchical attention:\n",
    "            u_it = torch.tanh(self.ui(output)) # Batch size X senq len X context dim\n",
    "            weights = torch.softmax(u_it.matmul(self.uw), dim=1).unsqueeze(1)\n",
    "            \n",
    "            hidden = torch.sum(weights.matmul(output), dim=1) # Batch size X Hidden dim*2\n",
    "\n",
    "            hidden = self.dropout(hidden)\n",
    "        else:\n",
    "            u_it = torch.tanh(self.ui(output)) # Batch size X senq len X context dim\n",
    "            weights = torch.softmax(u_it.matmul(self.uw), dim=1).unsqueeze(1)\n",
    "            \n",
    "            hidden = torch.sum(weights.matmul(output), dim=1) # Batch size X Hidden dim\n",
    "\n",
    "            hidden = self.dropout(hidden)\n",
    "        \n",
    "        #if self.bidirectional:\n",
    "        #    hidden = [batch size, hid dim * num directions]\n",
    "        #else:\n",
    "        #    hidden = [batch size, hid dim]\n",
    "        \n",
    "        # with RELU\n",
    "        #return self.fc(self.relu(hidden))\n",
    "        \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter and init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 308\n",
    "HIDDEN_DIM = 256\n",
    "CONTEXT_DIM = 70\n",
    "OUTPUT_DIM = 5\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "# Regularization hyperparameter\n",
    "DROPOUT = 0.5\n",
    "L2_LAMBDA = 0 #0.00001\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "MODEL_SAVE_FILE = 'LSTM_with_attention_origin.pt'\n",
    "model = LSTMWithAttention(EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            CONTEXT_DIM,\n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,774,673 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        predictions = model(text, text_lengths)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, set_length):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (predictions.argmax(1) == batch.label).sum().item()\n",
    "        \n",
    "    return epoch_loss / set_length, epoch_acc / set_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define epoch time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.01435 | Train Acc: 61.97%\n",
      "\t Val. Loss: 0.01348 |  Val. Acc: 64.32%\n",
      "Epoch: 02 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.01299 | Train Acc: 65.19%\n",
      "\t Val. Loss: 0.01261 |  Val. Acc: 66.66%\n",
      "Epoch: 03 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.01228 | Train Acc: 67.23%\n",
      "\t Val. Loss: 0.01224 |  Val. Acc: 67.47%\n",
      "Epoch: 04 | Epoch Time: 1m 0s\n",
      "\tTrain Loss: 0.01166 | Train Acc: 68.97%\n",
      "\t Val. Loss: 0.01188 |  Val. Acc: 68.44%\n",
      "Epoch: 05 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.01111 | Train Acc: 70.38%\n",
      "\t Val. Loss: 0.01171 |  Val. Acc: 69.18%\n",
      "Epoch: 06 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.01066 | Train Acc: 71.63%\n",
      "\t Val. Loss: 0.01182 |  Val. Acc: 69.28%\n",
      "Epoch: 07 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.01024 | Train Acc: 72.66%\n",
      "\t Val. Loss: 0.01163 |  Val. Acc: 69.61%\n",
      "Epoch: 08 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00987 | Train Acc: 73.52%\n",
      "\t Val. Loss: 0.01169 |  Val. Acc: 69.90%\n",
      "Epoch: 09 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00949 | Train Acc: 74.51%\n",
      "\t Val. Loss: 0.01170 |  Val. Acc: 69.45%\n",
      "Epoch: 10 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00916 | Train Acc: 75.56%\n",
      "\t Val. Loss: 0.01206 |  Val. Acc: 69.73%\n",
      "Epoch: 11 | Epoch Time: 1m 0s\n",
      "\tTrain Loss: 0.00875 | Train Acc: 76.67%\n",
      "\t Val. Loss: 0.01264 |  Val. Acc: 68.88%\n",
      "Epoch: 12 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00839 | Train Acc: 77.48%\n",
      "\t Val. Loss: 0.01235 |  Val. Acc: 69.06%\n",
      "Epoch: 13 | Epoch Time: 1m 0s\n",
      "\tTrain Loss: 0.00803 | Train Acc: 78.48%\n",
      "\t Val. Loss: 0.01310 |  Val. Acc: 68.47%\n",
      "Epoch: 14 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00764 | Train Acc: 79.49%\n",
      "\t Val. Loss: 0.01331 |  Val. Acc: 68.08%\n",
      "Epoch: 15 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.00727 | Train Acc: 80.35%\n",
      "\t Val. Loss: 0.01376 |  Val. Acc: 67.85%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-15332ba96c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-6b206d8692c3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, set_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mepoch_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# best_valid_loss = float('inf')\n",
    "best_valid_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# For splotting\n",
    "all_train_losses = []\n",
    "all_valid_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, len(train_data))\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "#     if valid_loss < best_valid_loss:\n",
    "    if valid_acc > best_valid_acc:\n",
    "#         best_valid_loss = valid_loss\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), saved_models_path + MODEL_SAVE_FILE)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.5f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.5f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    all_train_losses.append(train_loss)\n",
    "    all_valid_losses.append(valid_loss)\n",
    "    \n",
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# plt.xticks(range(0, 10))\n",
    "plt.plot(all_train_losses)\n",
    "plt.plot(all_valid_losses)\n",
    "plt.axvline(x=best_epoch, color='green')\n",
    "\n",
    "plt.legend(['train loss', 'valid loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15606"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch: 08\n"
     ]
    }
   ],
   "source": [
    "print(f'Best epoch: {best_epoch+1:02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01184 | Test Acc: 69.00%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, len(test_data))\n",
    "\n",
    "print(f'Test Loss: {test_loss:.5f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reevaluate on valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.01169 | Test Acc: 69.90%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE))\n",
    "\n",
    "valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, len(valid_data))\n",
    "\n",
    "print(f'Test Loss: {valid_loss:.5f} | Test Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Kaggle Dataset and create submittion file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model target file\n",
    "MODEL_SAVE_FILE_TARGET = 'LSTM_with_attention_train-73.52-valid-69.90.pt'\n",
    "\n",
    "def predict_kaggle_test(model, iterator):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text, text_lengths)\n",
    "            \n",
    "            predictions = predictions.argmax(1)\n",
    "            \n",
    "            for i in range(len(batch)):\n",
    "                result.append([batch.id[0][i].item(), [batch.phrase_id[0][i].item(), predictions[i].item()]] )\n",
    "    \n",
    "    result.sort(key = lambda val: val[0])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [156061, 2]],\n",
       " [1, [156062, 2]],\n",
       " [2, [156063, 2]],\n",
       " [3, [156064, 2]],\n",
       " [4, [156065, 2]],\n",
       " [5, [156066, 3]],\n",
       " [6, [156067, 3]],\n",
       " [7, [156068, 2]],\n",
       " [8, [156069, 3]],\n",
       " [9, [156070, 2]]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(saved_models_path + MODEL_SAVE_FILE_TARGET))\n",
    "\n",
    "kaggle_result_list = predict_kaggle_test(model, kaggle_test_iterator)\n",
    "\n",
    "kaggle_result_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[66282, [222343, 1]],\n",
       " [66283, [222344, 2]],\n",
       " [66284, [222345, 2]],\n",
       " [66285, [222346, 2]],\n",
       " [66286, [222347, 2]],\n",
       " [66287, [222348, 0]],\n",
       " [66288, [222349, 1]],\n",
       " [66289, [222350, 1]],\n",
       " [66290, [222351, 1]],\n",
       " [66291, [222352, 1]]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_result_list[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66292"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Phrase_length</th>\n",
       "      <th>Tokenized_phrase</th>\n",
       "      <th>Indexed_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>188</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "      <td>77</td>\n",
       "      <td>[xxbos, xxmaj, an, intermittently, pleasing, b...</td>\n",
       "      <td>[2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "      <td>8</td>\n",
       "      <td>[xxbos, xxmaj, an, xxeos]</td>\n",
       "      <td>[2, 7, 26, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "      <td>1</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 409, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "      <td>6</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, mostly,...</td>\n",
       "      <td>[2, 2606, 1723, 30, 632, 1041, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>156066</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but</td>\n",
       "      <td>68</td>\n",
       "      <td>[xxbos, intermittently, pleasing, but, xxeos]</td>\n",
       "      <td>[2, 2606, 1723, 30, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>156067</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing</td>\n",
       "      <td>2</td>\n",
       "      <td>[xxbos, intermittently, pleasing, xxeos]</td>\n",
       "      <td>[2, 2606, 1723, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>156068</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently</td>\n",
       "      <td>65</td>\n",
       "      <td>[xxbos, intermittently, xxeos]</td>\n",
       "      <td>[2, 2606, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>156069</td>\n",
       "      <td>8545</td>\n",
       "      <td>pleasing</td>\n",
       "      <td>9</td>\n",
       "      <td>[xxbos, pleasing, xxeos]</td>\n",
       "      <td>[2, 1723, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>156070</td>\n",
       "      <td>8545</td>\n",
       "      <td>but</td>\n",
       "      <td>55</td>\n",
       "      <td>[xxbos, but, xxeos]</td>\n",
       "      <td>[2, 30, 3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...   \n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...   \n",
       "2    156063        8545                                                 An   \n",
       "3    156064        8545  intermittently pleasing but mostly routine effort   \n",
       "4    156065        8545         intermittently pleasing but mostly routine   \n",
       "5    156066        8545                        intermittently pleasing but   \n",
       "6    156067        8545                            intermittently pleasing   \n",
       "7    156068        8545                                     intermittently   \n",
       "8    156069        8545                                           pleasing   \n",
       "9    156070        8545                                                but   \n",
       "\n",
       "   Phrase_length                                   Tokenized_phrase  \\\n",
       "0            188  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "1             77  [xxbos, xxmaj, an, intermittently, pleasing, b...   \n",
       "2              8                          [xxbos, xxmaj, an, xxeos]   \n",
       "3              1  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "4              6  [xxbos, intermittently, pleasing, but, mostly,...   \n",
       "5             68      [xxbos, intermittently, pleasing, but, xxeos]   \n",
       "6              2           [xxbos, intermittently, pleasing, xxeos]   \n",
       "7             65                     [xxbos, intermittently, xxeos]   \n",
       "8              9                           [xxbos, pleasing, xxeos]   \n",
       "9             55                                [xxbos, but, xxeos]   \n",
       "\n",
       "                                      Indexed_phrase  \n",
       "0  [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 15, 3]  \n",
       "1      [2, 7, 26, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "2                                      [2, 7, 26, 3]  \n",
       "3             [2, 2606, 1723, 30, 632, 1041, 409, 3]  \n",
       "4                  [2, 2606, 1723, 30, 632, 1041, 3]  \n",
       "5                             [2, 2606, 1723, 30, 3]  \n",
       "6                                 [2, 2606, 1723, 3]  \n",
       "7                                       [2, 2606, 3]  \n",
       "8                                       [2, 1723, 3]  \n",
       "9                                         [2, 30, 3]  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kaggle_test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_EXTENSION = '.submit.csv'\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(saved_models_path + MODEL_SAVE_FILE_TARGET + CSV_EXTENSION, mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    csv_writer.writerow(['PhraseId', 'Sentiment'])\n",
    "    \n",
    "    for i in range(len(kaggle_result_list)):\n",
    "        csv_writer.writerow(kaggle_result_list[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
